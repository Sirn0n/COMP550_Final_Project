{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWI55rzF51IT",
        "outputId": "95a7772f-3346-4ee8-b4d9-a3b75cb5560d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8UqXwN1dsZf"
      },
      "source": [
        "# Modules and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PggLJ9xf7iTz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import BertTokenizer, TFBertModel, BertModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "import warnings\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import ast\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7C4WkPA0Xwq",
        "outputId": "8ffd867c-683f-4a89-bdc7-1302d02352b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "cuda\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    torch.set_default_tensor_type(torch.cuda.FloatTensor)  # Default to CUDA\n",
        "else:\n",
        "    torch.set_default_tensor_type(torch.FloatTensor)  # Default to CPU\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(torch.cuda.is_available())\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ez5ic-HGdwy1"
      },
      "source": [
        "Uncomment to get the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzgs9m7tKuLX",
        "outputId": "4928e504-2081-4572-dd23-c45a2b4f0b09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'sarcasm'...\n",
            "remote: Enumerating objects: 56, done.\u001b[K\n",
            "remote: Counting objects: 100% (56/56), done.\u001b[K\n",
            "remote: Compressing objects: 100% (54/54), done.\u001b[K\n",
            "remote: Total 56 (delta 20), reused 10 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (56/56), 2.89 MiB | 18.05 MiB/s, done.\n",
            "Resolving deltas: 100% (20/20), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/EducationalTestingService/sarcasm.git\n",
        "!mkdir -p /content/drive/MyDrive/FALL2024/comp550/final_project\n",
        "!cp -r sarcasm /content/drive/MyDrive/FALL2024/comp550/final_project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rCQiWhedzSg"
      },
      "source": [
        "# Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "m4Th6uCDd2AF"
      },
      "outputs": [],
      "source": [
        "def disconnect_runtime():\n",
        "    \"\"\"\n",
        "    Disconnects the Google Colab runtime programmatically.\n",
        "    \"\"\"\n",
        "    from google.colab import runtime\n",
        "    print(\"Disconnecting the runtime...\")\n",
        "    runtime.unassign()\n",
        "\n",
        "\n",
        "\n",
        "def load_jsonl_data(file_path):\n",
        "    data = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "\n",
        "def list_shape(lst):\n",
        "    \"\"\"\n",
        "    Recursively determines the shape of a nested list.\n",
        "\n",
        "    Args:\n",
        "        lst (list): The input nested list.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple representing the shape of the list.\n",
        "    \"\"\"\n",
        "    if isinstance(lst, list):\n",
        "        if len(lst) == 0:\n",
        "            return (0,)  # Empty list\n",
        "        return (len(lst), *list_shape(lst[0]))\n",
        "    else:\n",
        "        return ()  # End of recursion for non-list elements\n",
        "\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    contexts = [sample['context'] for sample in batch]\n",
        "    responses = [sample['response'] for sample in batch]\n",
        "    syntactic_features = torch.stack([sample['syntactic_features'] for sample in batch])\n",
        "    labels = torch.stack([sample['label'] for sample in batch])\n",
        "\n",
        "    # Process contexts and responses as needed\n",
        "    # For example, flattening or stacking them appropriately\n",
        "    return {\n",
        "        'context': contexts,\n",
        "        'response': responses,\n",
        "        'syntactic_features': syntactic_features,\n",
        "        'label': labels\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhhZbKJzyMGO"
      },
      "source": [
        "# Data Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU0HpG03yrlD"
      },
      "source": [
        "## Path Constants"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrU31ljQGrPs"
      },
      "source": [
        "## JSON files format (only use once to convert it to dataframe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sPMz3FROyWwO"
      },
      "outputs": [],
      "source": [
        "REDDIT_TRAINING_DATA_PATH_JSON = \"/content/drive/MyDrive/FALL2024/comp550/final_project/sarcasm/reddit/sarcasm_detection_shared_task_reddit_training.jsonl\"\n",
        "REDDIT_TESTING_DATA_PATH_JSON = \"/content/drive/MyDrive/FALL2024/comp550/final_project/sarcasm/reddit/sarcasm_detection_shared_task_reddit_testing.jsonl\"\n",
        "TWITTER_TRAINING_DATA_PATH_JSON = \"/content/drive/MyDrive/FALL2024/comp550/final_project/sarcasm/twitter/sarcasm_detection_shared_task_twitter_training.jsonl\"\n",
        "TWITTER_TESTING_DATA_PATH_JSON= \"/content/drive/MyDrive/FALL2024/comp550/final_project/sarcasm/twitter/sarcasm_detection_shared_task_twitter_testing.jsonl\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eyg6urIfGvg1"
      },
      "source": [
        "## Data frame format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zCljfY67Gxe5"
      },
      "outputs": [],
      "source": [
        "REDDIT_TRAINING_PATH_DATAFRAME = \"/content/drive/MyDrive/FALL2024/comp550/final_project/sarcasm/reddit/train_val_split/training_set.csv\"\n",
        "REDDIT_VALIDATION_PATH_DATAFRAME = \"/content/drive/MyDrive/FALL2024/comp550/final_project/sarcasm/reddit/train_val_split/validation_set.csv\"\n",
        "REDDIT_TESTING_PATH_DATAFRAME = \"/content/drive/MyDrive/FALL2024/comp550/final_project/sarcasm/reddit/sarcasm_detection_reddit_testing.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ntf1CKr-70Uw"
      },
      "source": [
        "# Data Processing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WC6p-8r3uufr"
      },
      "outputs": [],
      "source": [
        "def is_pad_tensor(tensor):\n",
        "    correct_start = tensor[0][0] == 101\n",
        "    correct_middle = tensor[0][1] == 0\n",
        "    correct_end = tensor[0][2] == 102\n",
        "    return correct_start and correct_middle and correct_end"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7cyt02-sR65"
      },
      "source": [
        "## Padding\n",
        "Given that each context has a different number of utterences, it means that the context list differes in length depending on the data point. So we need to pad those to make it uniform\n",
        "\n",
        "`No need to run this cell each time. run it once to convert the dataset to df and pad it. The code save it in the directory so no need to rerun agin`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxL5UPSgsifO",
        "outputId": "66638454-e6ce-4420-ad22-b9e110892c23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Maximum number of utterences:  13\n",
            "Padded dataset saved to /content/drive/MyDrive/FALL2024/comp550/final_project/sarcasm/reddit/sarcasm_detection_reddit_testing.csv\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "dataset = pd.DataFrame(load_jsonl_data(REDDIT_TESTING_DATA_PATH_JSON))\n",
        "\n",
        "# Determine the maximum length of the lists in the \"context\" column\n",
        "dataset['context_list_length'] = dataset['context'].apply(len)\n",
        "max_length = dataset['context_list_length'].max()\n",
        "\n",
        "print(\"Maximum number of utterences: \", max_length)\n",
        "\n",
        "# Function to pad or truncate each context list to the maximum length\n",
        "def pad_context(context, max_length):\n",
        "    return context + ['[PAD]'] * (max_length - len(context))\n",
        "\n",
        "# Apply padding\n",
        "dataset['context'] = dataset['context'].apply(lambda x: pad_context(x, max_length))\n",
        "\n",
        "# Drop the intermediate length column\n",
        "dataset = dataset.drop(columns=['context_list_length'])\n",
        "\n",
        "# Save the updated dataset\n",
        "output_file_path = '/content/drive/MyDrive/FALL2024/comp550/final_project/sarcasm/reddit/sarcasm_detection_reddit_testing.csv'  # Replace with your desired output file name\n",
        "dataset.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(f\"Padded dataset saved to {output_file_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIL740zUcxcN"
      },
      "source": [
        "# Syntactic structure preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFbItJhRc-rP",
        "outputId": "e4b9e57b-9902-4646-e4c0-66ce215a766e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Syntactic data with features saved to /content/drive/MyDrive/FALL2024/comp550/final_project/sarcasm/reddit/syntactic_training.json\n",
            "Syntactic data with features saved to /content/drive/MyDrive/FALL2024/comp550/final_project/sarcasm/reddit/syntactic_validation.json\n",
            "Syntactic data with features saved to /content/drive/MyDrive/FALL2024/comp550/final_project/sarcasm/reddit/syntactic_testing.json\n"
          ]
        }
      ],
      "source": [
        "\n",
        "SYNTACTIC_TRAINING_OUTPUT = \"/content/drive/MyDrive/FALL2024/comp550/final_project/sarcasm/reddit/syntactic_training.json\"\n",
        "SYNTACTIC_VALIDATION_OUTPUT = \"/content/drive/MyDrive/FALL2024/comp550/final_project/sarcasm/reddit/syntactic_validation.json\"\n",
        "SYNTACTIC_TESTING_OUTPUT = \"/content/drive/MyDrive/FALL2024/comp550/final_project/sarcasm/reddit/syntactic_testing.json\"\n",
        "\n",
        "def extract_syntactic_structure(sentence):\n",
        "    \"\"\"\n",
        "    Extract syntactic structure using SpaCy.\n",
        "    \"\"\"\n",
        "    doc = nlp(sentence)\n",
        "    return [\n",
        "        {\n",
        "            \"word\": token.text,\n",
        "            \"lemma\": token.lemma_,\n",
        "            \"pos\": token.pos_,\n",
        "            \"dep\": token.dep_,\n",
        "            \"head\": token.head.text\n",
        "        }\n",
        "        for token in doc\n",
        "    ]\n",
        "\n",
        "\n",
        "\n",
        "def extract_features(syntactic_structure):\n",
        "    \"\"\"\n",
        "    Compute features from syntactic structure.\n",
        "    \"\"\"\n",
        "    features = {\n",
        "        \"neg_count\": 0,\n",
        "        \"is_question\": 0,\n",
        "        \"is_exclamation\": 0,\n",
        "        \"modifier_count\": 0\n",
        "    }\n",
        "    for token in syntactic_structure:\n",
        "        if token[\"dep\"] == \"neg\":\n",
        "            features[\"neg_count\"] += 1\n",
        "        if token[\"pos\"] == \"PUNCT\" and token[\"word\"] == \"?\":\n",
        "            features[\"is_question\"] = 1\n",
        "        if token[\"pos\"] == \"PUNCT\" and token[\"word\"] == \"!\":\n",
        "            features[\"is_exclamation\"] = 1\n",
        "        if token[\"dep\"] in [\"advmod\", \"amod\"]:\n",
        "            features[\"modifier_count\"] += 1\n",
        "    return features\n",
        "\n",
        "\n",
        "def preprocess_dataset(data_path, output_path):\n",
        "    \"\"\"\n",
        "    Process dataset to extract syntactic features and save them to a JSON file.\n",
        "    \"\"\"\n",
        "    data = pd.read_csv(data_path)\n",
        "    syntactic_data = []\n",
        "    for _, row in data.iterrows():\n",
        "        response = row[\"response\"]  # Assuming \"response\" column exists\n",
        "        label = row[\"label\"]        # Assuming \"label\" column exists\n",
        "        syntactic_structure = extract_syntactic_structure(response)\n",
        "        features = extract_features(syntactic_structure)\n",
        "        syntactic_data.append({\n",
        "            \"response\": response,\n",
        "            \"features\": features,\n",
        "            \"label\": label\n",
        "        })\n",
        "\n",
        "    # Save to JSON file\n",
        "    with open(output_path, \"w\") as f:\n",
        "        json.dump(syntactic_data, f, indent=4)\n",
        "    print(f\"Syntactic data with features saved to {output_path}\")\n",
        "\n",
        "# Generate syntactic data with features\n",
        "preprocess_dataset(REDDIT_TRAINING_PATH_DATAFRAME, SYNTACTIC_TRAINING_OUTPUT )\n",
        "preprocess_dataset(REDDIT_VALIDATION_PATH_DATAFRAME, SYNTACTIC_VALIDATION_OUTPUT )\n",
        "preprocess_dataset(REDDIT_TESTING_PATH_DATAFRAME, SYNTACTIC_TESTING_OUTPUT )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEeczvzkJVFT"
      },
      "source": [
        "# Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "mquw6WAUJWtT"
      },
      "outputs": [],
      "source": [
        "class RedditDataset(Dataset):\n",
        "    def __init__(self, path_to_dataset_df, syntactic_structures_path, device, a_sample_of = None, tokenizer = \"bert-base-cased\" , max_sentence_length=100):\n",
        "        # Load data from json file to dataframe\n",
        "        if path_to_dataset_df.endswith('.jsonl'):\n",
        "            print(\"Recevied a json file\")\n",
        "            self.data = pd.DataFrame(load_jsonl_data(path_to_dataset_df))\n",
        "        else:\n",
        "            print(\"Recevied a csv file\")\n",
        "            self.data = pd.read_csv(path_to_dataset_df)\n",
        "            self.data['context']= self.data['context'].apply(ast.literal_eval)\n",
        "\n",
        "\n",
        "        # Load syntactic structures\n",
        "        with open(syntactic_structures_path, 'r') as file:\n",
        "            self.syntactic_structures = json.load(file)\n",
        "\n",
        "        # Ensure alignment\n",
        "        assert len(self.data) == len(self.syntactic_structures)\n",
        "\n",
        "        # Added this feature to use it if we want to check if the code trains well on a subset\n",
        "        if a_sample_of is not None:\n",
        "            balanced_subset, _ = train_test_split(self.data,\n",
        "                                      stratify=self.data['label'],\n",
        "                                      train_size=a_sample_of/self.data.shape[0],\n",
        "                                      random_state=42)\n",
        "\n",
        "            self.data = balanced_subset\n",
        "\n",
        "        # Map string labels to floats\n",
        "        self.data['label'] = self.data['label'].map({'SARCASM': 1, 'NOT_SARCASM': 0}).astype(float)\n",
        "\n",
        "        self.device = device\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(tokenizer)\n",
        "        self.max_sentence_length = max_sentence_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx] # Here we obtain the row that contain the context, response and label. All readable, nothing tokenized yet\n",
        "\n",
        "        syntactic_features = torch.tensor(list(self.syntactic_structures[idx][\"features\"].values()), dtype=torch.float32).to(self.device)\n",
        "\n",
        "        context = row['context'] # Untokenized context\n",
        "\n",
        "        context_tokens = []\n",
        "        for utterence in context: # Here we are going to tokenize each utterencein the context\n",
        "            tokenized_utterence = self.tokenizer(\n",
        "                utterence,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                max_length=self.max_sentence_length\n",
        "            ).to(self.device)\n",
        "\n",
        "            context_tokens.append(tokenized_utterence)\n",
        "\n",
        "\n",
        "        response = row['response']\n",
        "        response_tokens = self.tokenizer(\n",
        "            response,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_sentence_length\n",
        "        ).to(self.device)\n",
        "\n",
        "        label = torch.tensor(float(row['label']), dtype=torch.float32).to(self.device)\n",
        "        return {'context': context_tokens, 'response': response_tokens,\n",
        "                 'syntactic_features': syntactic_features,'label': label}\n",
        "\n",
        "    def get_df(self):\n",
        "        return self.data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHmKz07S7O2O"
      },
      "source": [
        "# Model Architecture as per the article\n",
        "\n",
        "I will build the model architure exactly as shown in the article"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfty7KOLGouL"
      },
      "source": [
        "## Layer 1: Sentence Encoding Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "LSgtADX37R2x"
      },
      "outputs": [],
      "source": [
        "# Input: batches of Context (utterences), and response\n",
        "\n",
        "# output: two outputs:\n",
        "    # batch of Context Encoded (batch_size, m, 100, 768)\n",
        "    # batch of Reponse Encoded list of size (batch_size, 100, 2*lstm_hiddien_size)\n",
        "class SentenceEncodingLayer(nn.Module):\n",
        "\n",
        "    # The lstm_hidden_size is a parameter that we can adjust\n",
        "    def __init__(self, bert_model_name=\"bert-base-cased\", lstm_hidden_size=300):\n",
        "\n",
        "        super(SentenceEncodingLayer, self).__init__()\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Initialize BERT tokenizer and model for context and response\n",
        "        self.context_encoder = BertModel.from_pretrained(bert_model_name).to(self.device)\n",
        "        self.response_encoder = BertModel.from_pretrained(bert_model_name).to(self.device)\n",
        "\n",
        "        # BiLSTM layer for response encoding\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=768,  # BERT output dimension\n",
        "            hidden_size=lstm_hidden_size,\n",
        "            bidirectional=True,\n",
        "            batch_first=True\n",
        "        ).to(self.device)\n",
        "\n",
        "\n",
        "    def encode_context(self, context_tokens_batch): # Outputs are stacked to form a tensor of shape (m, max_sentence_length, 768).\n",
        "        batched_context_embeddings = []\n",
        "\n",
        "\n",
        "        for tokenized_context in context_tokens_batch:\n",
        "\n",
        "            utterance_embeddings = []\n",
        "\n",
        "            for utt in tokenized_context:\n",
        "               # if is_pad_tensor(utt['input_ids']):\n",
        "               #     continue\n",
        "                utt_embedding = self.context_encoder(**utt).last_hidden_state\n",
        "                utterance_embeddings.append(utt_embedding.squeeze(0))\n",
        "\n",
        "\n",
        "            stacked_utterance_embeddings = torch.stack(utterance_embeddings)\n",
        "            batched_context_embeddings.append(stacked_utterance_embeddings)\n",
        "\n",
        "        stacked_batched_context_embeddings = torch.stack(batched_context_embeddings)\n",
        "        return stacked_batched_context_embeddings\n",
        "\n",
        "    def encode_response(self, response_tokens_batch):\n",
        "        # The input is a batch of respones\n",
        "        \"\"\"\n",
        "        Encodes the response sentence using the response BERT encoder and a BiLSTM.\n",
        "        A BiLSTM processes the embeddings to capture sequential dependencies,\n",
        "        outputting a representation of shape (batch_size, max_sentence_length, 2*lstm_hidden_size).\n",
        "        \"\"\"\n",
        "        batched_response_embeddings = []\n",
        "        for tokenized_response in response_tokens_batch:\n",
        "            response_embedding = self.response_encoder(**tokenized_response).last_hidden_state  # Shape: (1, max_sentence_length, 768)\n",
        "            lstm_out, _ = self.lstm(response_embedding)  # Shape: (1, max_sentence_length, 2*lstm_hidden_size)\n",
        "            batched_response_embeddings.append(lstm_out)\n",
        "        # Stack embeddings for all responses in the batch\n",
        "\n",
        "        lstm_out = lstm_out = torch.cat(batched_response_embeddings, dim=0) #torch.stack(batched_response_embeddings)\n",
        "\n",
        "        return lstm_out\n",
        "\n",
        "    def forward(self, context_tokens_batch, response_tokens_batch):\n",
        "        \"\"\"\n",
        "        Encodes the context and response and returns their representations.\n",
        "        \"\"\"\n",
        "        # Encode the context (list of utterances)\n",
        "        context_representation = self.encode_context(context_tokens_batch)\n",
        "\n",
        "        # Encode the response\n",
        "        response_representation = self.encode_response(response_tokens_batch)\n",
        "        return context_representation, response_representation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dj3bHfHeGsZf"
      },
      "source": [
        "## Layer 2: Context Summarization\n",
        "\n",
        "This layer only operates on the \"Encoded_Context\" according to the article only deal with the \"encoded_context\" which is outputted from the previous layer. The each encoded context has a dimension of [m, 100, 768] and in total we have len(dataset) contexts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Du2lzjalJMWA"
      },
      "outputs": [],
      "source": [
        "# This thing only deals with the Context (It does not care about the response)\n",
        "# The goal is to reduce the dimension of the context\n",
        "\n",
        "\n",
        "# input shape: [batch_size num_uttenrences, 100, 768]\n",
        "\n",
        "# output: [batch_size, num_uttenrences - k_row + 1, 100 - k_col +1, 128 ]\n",
        "\n",
        "class ContextSummarizationLayer(nn.Module):\n",
        "    # The dsum, k_rows, k_col are variables that need to be optimized\n",
        "    def __init__(self, d_bert= 768, dsum = 128, k_row = 2, k_col =2, padding = 0, stride =1):\n",
        "        super(ContextSummarizationLayer, self).__init__()\n",
        "        self.k_row = k_row\n",
        "        self.k_col = k_col\n",
        "\n",
        "        # Conv2D expects an input as follows: (N, C_in, H, W)\n",
        "        self.conv2D = nn.Conv2d(in_channels=d_bert,  # Context tensor is treated as a single \"channel\"\n",
        "                                out_channels=dsum, # Number of features maps to produce, which means that each utterence will be mapped to d_sum features\n",
        "                                kernel_size=(k_row, k_col),\n",
        "                                stride = stride,     # I think this is the default value, but just putting it to ensure the understanding flow of the article\n",
        "                                padding = padding  # Does the article mentions padding? I think that's a different aspect to look after\n",
        "                                )\n",
        "\n",
        "    def forward(self, context_representation_batch):  # Input shape: (Batch_size ,m, d_sen, d_bert) which is (Batch_size , num_utterences, 100, 768)\n",
        "        # Permute from (Batch_size , m, d_sen, d_bert), to (Batch_size , d_bert, m, d_sen) to match Conv2D requirements.\n",
        "        context_representation_batch = context_representation_batch.permute(0, 3, 1, 2).to(self.conv2D.weight.device)\n",
        "        # Apply 2D convolution the input shape here is (d_bert, m, d_sen)\n",
        "        summarized_context_batch = self.conv2D(context_representation_batch) # Shape: Output shape: (Batch_size , dsum, m-k_row+1, d_sen-k_col+1).\n",
        "\n",
        "        # Permute the dimensions to make the output compatible with what the article says: (Batch_size , dsum, m-k_row+1, d_sen-k_col+1) -> (Batch_size , m-k_row+1, d_sen-k_col+1, dsum)\n",
        "        summarized_context_batch = summarized_context_batch.permute(0, 2, 3, 1)\n",
        "\n",
        "        return summarized_context_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q25MB-kqTzj"
      },
      "source": [
        "## Layer 3: Context Encoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "kJx5zMAaq8Dh"
      },
      "outputs": [],
      "source": [
        "# input shape: (batch_size, m-k_row+1, d_sen-k_col+1, dsum)\n",
        "# output_shape: (batch_size, m-k_row+1 , 2*lstm_hidden_size)\n",
        "class ContextEncoderLayer(nn.Module):\n",
        "    # The article says that the number of layers is 1\n",
        "    def __init__(self, size_of_input, lstm_hidden_size=300, number_of_layers=1):\n",
        "        super(ContextEncoderLayer, self).__init__()\n",
        "        self.input_size = size_of_input\n",
        "        self.lstm_hidden_size = lstm_hidden_size\n",
        "        self.bilstm = nn.LSTM(\n",
        "            input_size=self.input_size, # This should be the dimension of the input for each utterance\n",
        "            hidden_size=self.lstm_hidden_size,\n",
        "            num_layers=number_of_layers,\n",
        "            bidirectional=True,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "    def forward(self, summarized_context_batch):\n",
        "\n",
        "        # Input shape of the summarized context is: (batch_size, m-k_row+1, d_sen-k_col+1, dsum)\n",
        "        batch_size, reduced_num_utterences, reduce_dsen, dsum = summarized_context_batch.size()\n",
        "\n",
        "        # The reshape operation flattens dsen - kcol + 1 and dsum into a single dimension input preserving m- krows + 1 as the sequence length.\n",
        "        summarized_context_batch = summarized_context_batch.reshape(batch_size, reduced_num_utterences, -1)  # Shape: (batch_size, m - k_row + 1, (d_sen - k_col + 1) * d_sum)\n",
        "        summarized_context_batch = summarized_context_batch.to(self.bilstm.weight_hh_l0.device) # This is just to make sure we are on the correct device, maybe need to remove it\n",
        "        lstm_output, _ = self.bilstm(summarized_context_batch)\n",
        "\n",
        "        # Output shape: (batch_size ,m - k_row + 1, hidden_dim * 2)\n",
        "        return lstm_output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWLnfdgLEFFF"
      },
      "source": [
        "## Layer 4: CNN Layer\n",
        "\n",
        "This takes both the \"context\" outputted from the bilstm ContextEncoderLayer, and the response that we had in the first place.\n",
        "\n",
        "For the context, its dimensions is <M, d_lstm> where M is the reduced number of utterences after convolution, and d_lstm is the number of hidden units times two because it is biredctional\n",
        "\n",
        "For the response, its dimension is <d_sen, d_lstm>\n",
        "\n",
        "The article specifies combining the context representation (o_{con}) and the response representation (o_{res}). `so we need to handle this concatination BEFORE entering this layer, this layer takes as an input the concatinated stuff`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Hx_S-8MJLb31"
      },
      "outputs": [],
      "source": [
        "class CNNLayer(nn.Module):\n",
        "    def __init__(self, input_channels, output_channels, kernel_sizes):\n",
        "        super(CNNLayer, self).__init__()\n",
        "\n",
        "\n",
        "        # Conv2D expects an input as follows: (N, C_in, H, W)\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv2d(\n",
        "                in_channels=input_channels,\n",
        "                out_channels=output_channels,\n",
        "                kernel_size=kernel_size,\n",
        "                stride=1\n",
        "            ) for kernel_size in kernel_sizes\n",
        "        ])\n",
        "\n",
        "        self.maxpool = nn.AdaptiveMaxPool2d((1, 1))\n",
        "\n",
        "    def forward(self, combined_representation_batch):\n",
        "        # combined representation: concatination(response,context)\n",
        "\n",
        "        feature_maps = []\n",
        "\n",
        "        for conv in self.convs:\n",
        "\n",
        "            conv_output = torch.relu(conv(combined_representation_batch))\n",
        "\n",
        "\n",
        "            pooled_output = self.maxpool(conv_output)\n",
        "\n",
        "            feature_maps.append(pooled_output)\n",
        "\n",
        "        # Initialize an empty list to store flattened feature maps\n",
        "        flattened_maps = []\n",
        "\n",
        "        # Loop through the feature maps and flatten them\n",
        "        for feature_map in feature_maps:\n",
        "            # Get the batch size and number of features\n",
        "            batch_size = feature_map.size(0)  # Number of samples in the batch\n",
        "            num_features = feature_map.size(1)  # Number of features per sample\n",
        "\n",
        "            # Flatten\n",
        "            flattened_map = feature_map.view(batch_size, num_features)\n",
        "\n",
        "            # Append the flattened map to the list\n",
        "            flattened_maps.append(flattened_map)\n",
        "\n",
        "        # Concatenate all flattened feature maps along the feature dimension\n",
        "        # This combines features from all convolutional layers into one vector\n",
        "        unified_feature_vector = torch.cat(flattened_maps, dim=1)\n",
        "\n",
        "        # Return the final feature vector\n",
        "        return unified_feature_vector\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3iSeZYawbK_"
      },
      "source": [
        "## Layer 5: Fully Connected Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "UNV3pg_Ewd3I"
      },
      "outputs": [],
      "source": [
        "import torch.nn.init as init\n",
        "\n",
        "class FullyConnectedLayer(nn.Module):\n",
        "    def __init__(self, input_features):\n",
        "        super(FullyConnectedLayer, self).__init__()\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(input_features, 1)  # Output size is 1 (binary classification)\n",
        "\n",
        "        # Explicit initialization\n",
        "        init.xavier_uniform_(self.fc.weight)  # Xavier initialization for weights\n",
        "        init.zeros_(self.fc.bias)             # Initialize bias to 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through the fully connected layer and sigmoid activation\n",
        "        x = self.fc(x) # No sigmoid here; use BCEWithLogitsLoss\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NPWkJJH0u8E"
      },
      "source": [
        "## Full Architecutre (All Layers Combined Together)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Sj2e2Efr8RLL"
      },
      "outputs": [],
      "source": [
        "class HierarchicalBERT(nn.Module):\n",
        "    # Define class attributes (constants shared by all instances of the class)\n",
        "    GLOBAL_D_BERT = 768\n",
        "    GLOBAL_STRIDE = 1\n",
        "    GLOBAL_PADDING = 0\n",
        "    GLOBAL_K_ROW = 2\n",
        "    GLOBAL_K_COL = 2\n",
        "\n",
        "    def __init__(self, bert_model_name=\"bert-base-cased\", lstm_hidden_size=300, max_sentence_length=100,\n",
        "                 output_channels=128, kernel_sizes=[(2, 2), (2, 3), (2, 5)], syntactic_feature_dim=4, num_classes=1):\n",
        "        super(HierarchicalBERT, self).__init__()\n",
        "\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.sentence_encoder = SentenceEncodingLayer(\n",
        "            bert_model_name=bert_model_name,\n",
        "            lstm_hidden_size=lstm_hidden_size,\n",
        "        ).to(device)\n",
        "\n",
        "        self.context_summarizer = ContextSummarizationLayer(\n",
        "            d_bert=self.GLOBAL_D_BERT,\n",
        "            dsum=output_channels,\n",
        "            k_row=self.GLOBAL_K_ROW,\n",
        "            k_col=self.GLOBAL_K_COL\n",
        "        ).to(device)\n",
        "\n",
        "        self.context_encoder = ContextEncoderLayer(\n",
        "            size_of_input=(output_channels * (max_sentence_length - self.GLOBAL_K_ROW + 1)),\n",
        "            lstm_hidden_size=lstm_hidden_size\n",
        "        ).to(device)\n",
        "\n",
        "        self.syntactic_feature_fc = nn.Linear(syntactic_feature_dim, lstm_hidden_size * 2).to(device)\n",
        "\n",
        "        self.cnn_layer = CNNLayer(\n",
        "            input_channels=1,\n",
        "            output_channels=output_channels,\n",
        "            kernel_sizes=kernel_sizes\n",
        "        ).to(device)\n",
        "\n",
        "        # Adjust Fully Connected Layer input size\n",
        "        flattened_cnn_size = sum([\n",
        "            (max_sentence_length - k[0] + 1) * (self.GLOBAL_D_BERT - k[1] + 1)\n",
        "            for k in kernel_sizes\n",
        "        ]) * output_channels\n",
        "        fc_input_size = 384 + 384 + 600  # Two CNN outputs + syntactic features\n",
        "        self.fc_layer = FullyConnectedLayer(\n",
        "            input_features=fc_input_size\n",
        "        ).to(device)\n",
        "\n",
        "    def forward(self, context, response, syntactic_features, p=0):\n",
        "        \"\"\"\n",
        "        Forward pass through the entire model.\n",
        "        Args:\n",
        "            context (List[str]): List of utterances (context).\n",
        "            response (str): Response to classify as sarcastic or not.\n",
        "        Returns:\n",
        "            Tensor: Probability of sarcasm for the response.\n",
        "        \"\"\"\n",
        "        if p == 1: print(\"Received input\")\n",
        "        if p == 1: print(f\"Number of utterances: {len(context)}\")\n",
        "        if p == 1: print(f\"Response: {response}\")\n",
        "\n",
        "        # 1. Sentence Encoding\n",
        "        context_representation, response_representation = self.sentence_encoder(context, response)\n",
        "        if p == 1: print(\"After SentenceEncodingLayer, Here is the input to the context Summarization Layer\")\n",
        "        if p == 1: print(f\"Context Representation Shape: {context_representation.shape}\")\n",
        "        if p == 1: print(f\"Response Representation Shape: {response_representation.shape}\")\n",
        "\n",
        "        # 2. Context Summarization\n",
        "        summarized_context = self.context_summarizer(context_representation)\n",
        "        if p == 1: print(f\"Summarized Context Representation Shape: {summarized_context.shape}\")\n",
        "\n",
        "        # 3. Context Encoding\n",
        "        encoded_context = self.context_encoder(summarized_context)\n",
        "        if p == 1: print(f\"Encoded Context Representation Shape: {encoded_context.shape}\")\n",
        "\n",
        "        # 4. Pass Context and Response Through CNN\n",
        "        encoded_context = encoded_context.unsqueeze(1)  # Add channel dims for CNN\n",
        "        response_representation = response_representation.unsqueeze(1)  # Add channel dims for CNN\n",
        "\n",
        "        cnn_context_output = self.cnn_layer(encoded_context)\n",
        "        cnn_response_output = self.cnn_layer(response_representation)\n",
        "\n",
        "        cnn_context_flat = cnn_context_output.view(cnn_context_output.size(0), -1)\n",
        "        cnn_response_flat = cnn_response_output.view(cnn_response_output.size(0), -1)\n",
        "        syntactic_feature_embedding = self.syntactic_feature_fc(syntactic_features)\n",
        "\n",
        "        print(f\"CNN Context Output Shape: {cnn_context_output.shape}\")\n",
        "        print(f\"CNN Response Output Shape: {cnn_response_output.shape}\")\n",
        "        print(f\"Syntactic Feature Embedding Shape: {syntactic_feature_embedding.shape}\")\n",
        "        print(f\"Flattened CNN Context Shape: {cnn_context_flat.shape}\")\n",
        "        print(f\"Flattened CNN Response Shape: {cnn_response_flat.shape}\")\n",
        "        #print(f\"Final Representation Shape (before FC layer): {final_representation.shape}\")\n",
        "        #print(f\"Expected Input Features for FC Layer: {self.fc_layer.in_features}\")\n",
        "\n",
        "\n",
        "\n",
        "        final_representation = torch.cat((cnn_context_flat, cnn_response_flat, syntactic_feature_embedding), dim=1)\n",
        "\n",
        "        if p == 1: print(f\"Final Representation Shape: {final_representation.shape}\")\n",
        "\n",
        "        # 7. Fully Connected Layer\n",
        "        logits = self.fc_layer(final_representation)\n",
        "        if p == 1: print(f\"Final Output: {logits}\")\n",
        "        if p == 1: print(\"================= \\n\")\n",
        "\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PwEZdAd0q78"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "h5EpUU0SdviM"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, data_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    training_epoch_loss = 0\n",
        "    correct_training_predictions = 0\n",
        "    total_training_predictions = 0\n",
        "    progress_bar = tqdm(data_loader, leave=True, desc=\"Training Progress\")\n",
        "    for batch in progress_bar:\n",
        "        # Move batch data to the device\n",
        "        context = batch['context']\n",
        "        response = batch['response']\n",
        "        syntactic_features = batch['syntactic_features']\n",
        "        labels = batch['label'].unsqueeze(1).to(device).float()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(context, response, syntactic_features)  # Remove p=0\n",
        "        loss = criterion(logits, labels)\n",
        "        predictions = (torch.sigmoid(logits) > 0.5).long()\n",
        "\n",
        "        # Backward pass and optimizer step\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update metrics\n",
        "        training_epoch_loss += loss.item()\n",
        "        total_training_predictions += labels.size(0)\n",
        "        correct_training_predictions += (predictions == labels.long()).sum().item()\n",
        "        batch_accuracy = (predictions == labels.long()).sum().item() / labels.size(0)\n",
        "\n",
        "    epoch_loss = training_epoch_loss / len(data_loader)\n",
        "    epoch_accuracy = correct_training_predictions / total_training_predictions\n",
        "\n",
        "    return epoch_loss, epoch_accuracy\n",
        "\n",
        "\n",
        "def validation(model, val_loader, criterion, device):\n",
        "    \"\"\"\n",
        "    Validate the model on the validation dataset and return loss and accuracy.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    epoch_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    progress_bar = tqdm(val_loader, leave=True, desc=\"Validation Progress\")\n",
        "    with torch.no_grad():\n",
        "        for batch in progress_bar:\n",
        "            # Move data to the device\n",
        "            context = batch['context']\n",
        "            response = batch['response']\n",
        "            syntactic_features = batch['syntactic_features']\n",
        "            labels = batch['label'].unsqueeze(1).to(device).float()\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(context, response, syntactic_features)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            # Compute predictions\n",
        "            predictions = (torch.sigmoid(logits) > 0.5).long()\n",
        "\n",
        "            # Update metrics\n",
        "            epoch_loss += loss.item()\n",
        "            total_predictions += labels.size(0)\n",
        "            correct_predictions += (predictions == labels.long()).sum().item()\n",
        "            #batch_accuracy = (predictions == labels.long()).sum().item() / labels.size(0)\n",
        "\n",
        "\n",
        "\n",
        "    # Calculate average loss and accuracy\n",
        "    avg_loss = epoch_loss / len(val_loader)\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "\n",
        "    return avg_loss, accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff2ieLVsAzeJ"
      },
      "source": [
        "## Dataset and model initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144,
          "referenced_widgets": [
            "d2dbc7f69bca4ed8aa0fab11d6c23264",
            "8e90fb6ebe984fe9ad8f455063ab8049",
            "13f2e1cb405748b3ad2106ef1a2eeb4c",
            "e73c11b4cff94f6a91bda2bfc163ffa9",
            "8b0103837fc64e3b85cde418e64139eb",
            "85b14e9a686e4d3a859255a5f6abe98c",
            "27107c15410f4d1aaf5024808ae7b10b",
            "07eaede52e414518859b7ef8e3decdf9",
            "814fcc4b662c4b4b92686cf5c42c4002",
            "24eb164519124866bec36b49adb61d31",
            "5173af63a09f4f42bdadf01c5cfe68dc",
            "6e3ec84fbb8c4139943a6bff7d2f334d",
            "2104d2ae2a9d4effae258fd8a3d59e85",
            "d5ad2cfb2dfd4ae5a2e9a36be08872e2",
            "01c1390329b543fe94327f9d8365a7b6",
            "47d9c86c963946c985b7fad02c352071",
            "002c24e70d3d42dc8225ac57c03de12a",
            "7f46abc0f4ae40b6b19e94eca97549f2",
            "b31d627e784f45df815c9abfeb28a827",
            "b848c6cdf0004e198521e812ce943fd8",
            "2555660ccaf346e5bacb1a75b9c1b1a2",
            "657dbb637136492c9253f078fb7def4b",
            "a92286d8461d449db35a076b391a39c9",
            "4a3414073c4a42d98edad65d57e64e1c",
            "4fa6bd0598f84cf4ab51c193cd5060b0",
            "9925ba69ee5a404d9c4277639661c157",
            "aa84539315ce44bf9f0ef96f8415f8a9",
            "9bfe7282584641b9bc54abac95e6793e",
            "77471b6cb886453d840addeb3f31a726",
            "145560b4b5624c2596ae4545c8b53229",
            "904e8ec70b254b2ca83f2e2d9ff36be7",
            "cf1f93140177405e96d5ed5fad9145da",
            "f679b9049a6f42bab43cc0fe71b76215",
            "6fcc712426df4335ad1965932422f542",
            "b6f47643b84c40589ba1c55687bee93b",
            "2255e4aed13a46d8807d024e42b3d81d",
            "abaca4e1ee7f40fba3ca5fabe39324d8",
            "b6e527bcc8fb421281e64a8e85062ccb",
            "c37a136098704fbcb5d986704e83506e",
            "3e7d6c8ff12340f781c3dddcfd092d42",
            "69ff5149b99846e7bde0d944b64e2224",
            "5ff6d61576a44b93878d53c7d2a2aa7a",
            "8b5feec9d48c47149ac18c053e87d86d",
            "39782951d4cb4bac94d4a1455c6bf586"
          ]
        },
        "collapsed": true,
        "id": "WFQM-tlBgpW-",
        "outputId": "1952608d-9848-442f-e2b3-f9d6154636cc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d2dbc7f69bca4ed8aa0fab11d6c23264",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6e3ec84fbb8c4139943a6bff7d2f334d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a92286d8461d449db35a076b391a39c9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6fcc712426df4335ad1965932422f542",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ocjo4pIl3TgM",
        "outputId": "3bcf0a79-c0b2-4789-d786-386fcdd32d24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recevied a csv file\n",
            "Recevied a csv file\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "reddit_training_set  = RedditDataset(REDDIT_TRAINING_PATH_DATAFRAME, SYNTACTIC_TRAINING_OUTPUT, device)\n",
        "reddit_training_data_loader = DataLoader(reddit_training_set, batch_size= 8, shuffle=True, collate_fn=custom_collate_fn, generator=torch.Generator(device=device))\n",
        "\n",
        "reddit_validation_set  = RedditDataset(REDDIT_VALIDATION_PATH_DATAFRAME,  SYNTACTIC_VALIDATION_OUTPUT,  device)\n",
        "reddit_validation_data_loader = DataLoader(reddit_validation_set, batch_size=8, shuffle=False, collate_fn=custom_collate_fn, generator=torch.Generator(device=device))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5hXK0sSAj_q"
      },
      "source": [
        "## Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "oM0o7BNJAnql"
      },
      "outputs": [],
      "source": [
        "criterion = nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLDPl6mKApbD"
      },
      "source": [
        "## Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "2d036f48c7d742a695f7fceeb64eb4bf",
            "5845280e32834a48b283988e2baab942",
            "6143a79c4b4240a28fff3ca77fe4775a",
            "d056a81931d4400fb6431c5644e6ad63",
            "49af1c865fdd44f385cbdc0960d04760",
            "8e1a65de386543988032506e2e4a41aa",
            "6fc8fd792d494845a999fcb92b882cc9",
            "2af0cd20c29d4d99801f50eb38563a24",
            "53b947439596427182fb9fdc397f5729",
            "7ab224df975d4fadb7ea2c8e6fb8112f",
            "e7e844efbdb74e56b328341c957227c0"
          ]
        },
        "id": "daahJ8D1AuES",
        "outputId": "e6a950ba-f29b-4673-98bb-a1f1bc873342"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2d036f48c7d742a695f7fceeb64eb4bf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = HierarchicalBERT().to(device)\n",
        "from transformers import AdamW\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Bbw8-TUM0JQc",
        "outputId": "c9080cde-5ee9-4eb6-a45f-9c5236508804"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer: sentence_encoder.context_encoder.embeddings.word_embeddings.weight | Mean: -0.013846821151673794 | Std: 0.044778406620025635\n",
            "Layer: sentence_encoder.context_encoder.embeddings.position_embeddings.weight | Mean: 4.375215212348849e-06 | Std: 0.014552488923072815\n",
            "Layer: sentence_encoder.context_encoder.embeddings.token_type_embeddings.weight | Mean: -0.0004567124124150723 | Std: 0.025746047496795654\n",
            "Layer: sentence_encoder.context_encoder.embeddings.LayerNorm.weight | Mean: 0.8866543769836426 | Std: 0.09250417351722717\n",
            "Layer: sentence_encoder.context_encoder.embeddings.LayerNorm.bias | Mean: -0.019885143265128136 | Std: 0.06001076102256775\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.0.attention.self.query.weight | Mean: -1.3739396308665164e-05 | Std: 0.0344940721988678\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.0.attention.self.query.bias | Mean: -0.01033021043986082 | Std: 0.214474156498909\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.0.attention.self.key.weight | Mean: 2.1116933567100205e-05 | Std: 0.0339859239757061\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.0.attention.self.key.bias | Mean: -2.224700256192591e-05 | Std: 0.0015585526125505567\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.0.attention.self.value.weight | Mean: 1.0369047231506556e-05 | Std: 0.02527768909931183\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.0.attention.self.value.bias | Mean: 0.0007503176457248628 | Std: 0.03720814362168312\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.0.attention.output.dense.weight | Mean: -3.6421245113160694e-06 | Std: 0.024761490523815155\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.0.attention.output.dense.bias | Mean: -0.0018155656289309263 | Std: 0.0330154225230217\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.0.attention.output.LayerNorm.weight | Mean: 0.9871233701705933 | Std: 0.08989590406417847\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.0.attention.output.LayerNorm.bias | Mean: -0.012962600216269493 | Std: 0.24887695908546448\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.0.intermediate.dense.weight | Mean: -4.095903477718821e-06 | Std: 0.03330910950899124\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.0.intermediate.dense.bias | Mean: -0.08185876905918121 | Std: 0.041359663009643555\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.0.output.dense.weight | Mean: -3.770137846004218e-05 | Std: 0.031355224549770355\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.0.output.dense.bias | Mean: -0.0007258158875629306 | Std: 0.06903459876775742\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.0.output.LayerNorm.weight | Mean: 0.8939478397369385 | Std: 0.053671639412641525\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.0.output.LayerNorm.bias | Mean: -0.03920302540063858 | Std: 0.07369592785835266\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.1.attention.self.query.weight | Mean: 1.173121745523531e-05 | Std: 0.03841559588909149\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.1.attention.self.query.bias | Mean: -0.003118776250630617 | Std: 0.1203642189502716\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.1.attention.self.key.weight | Mean: -1.5655703464290127e-05 | Std: 0.038339368999004364\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.1.attention.self.key.bias | Mean: -7.818375888746232e-05 | Std: 0.002824404276907444\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.1.attention.self.value.weight | Mean: -2.6620209609973244e-05 | Std: 0.026602376252412796\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.1.attention.self.value.bias | Mean: 0.0012437974801287055 | Std: 0.030507182702422142\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.1.attention.output.dense.weight | Mean: 3.415161472730688e-06 | Std: 0.026631783694028854\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.1.attention.output.dense.bias | Mean: -0.0010803377954289317 | Std: 0.06734296679496765\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.1.attention.output.LayerNorm.weight | Mean: 0.9616678953170776 | Std: 0.07337253540754318\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.1.attention.output.LayerNorm.bias | Mean: -0.008811134845018387 | Std: 0.13873903453350067\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.1.intermediate.dense.weight | Mean: -4.156054274062626e-05 | Std: 0.03588266298174858\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.1.intermediate.dense.bias | Mean: -0.06761752814054489 | Std: 0.03901844471693039\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.1.output.dense.weight | Mean: -3.4443935874151066e-05 | Std: 0.03422658145427704\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.1.output.dense.bias | Mean: -0.0008649376104585826 | Std: 0.05324529483914375\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.1.output.LayerNorm.weight | Mean: 0.9423097968101501 | Std: 0.052177298814058304\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.1.output.LayerNorm.bias | Mean: -0.03462633490562439 | Std: 0.07001306861639023\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.2.attention.self.query.weight | Mean: -4.5720807975158095e-05 | Std: 0.04117968678474426\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.2.attention.self.query.bias | Mean: 0.0009442029404453933 | Std: 0.0910048633813858\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.2.attention.self.key.weight | Mean: -9.029969078255817e-05 | Std: 0.040630776435136795\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.2.attention.self.key.bias | Mean: 0.00011806102702394128 | Std: 0.002429635962471366\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.2.attention.self.value.weight | Mean: -1.6708898328943178e-05 | Std: 0.027623837813735008\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.2.attention.self.value.bias | Mean: 0.0005418008076958358 | Std: 0.0329880490899086\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.2.attention.output.dense.weight | Mean: -1.1268550679233158e-06 | Std: 0.026684947311878204\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.2.attention.output.dense.bias | Mean: -0.0007429927354678512 | Std: 0.06926459074020386\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.2.attention.output.LayerNorm.weight | Mean: 0.9397428035736084 | Std: 0.05958188325166702\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.2.attention.output.LayerNorm.bias | Mean: -0.005440631881356239 | Std: 0.11731145530939102\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.2.intermediate.dense.weight | Mean: -0.00018189431284554303 | Std: 0.0367986299097538\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.2.intermediate.dense.bias | Mean: -0.06542064249515533 | Std: 0.0418451689183712\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.2.output.dense.weight | Mean: -2.9077695216983557e-05 | Std: 0.03510678932070732\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.2.output.dense.bias | Mean: -0.000616271048784256 | Std: 0.050226736813783646\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.2.output.LayerNorm.weight | Mean: 0.9326571226119995 | Std: 0.047890424728393555\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.2.output.LayerNorm.bias | Mean: -0.021123012527823448 | Std: 0.05935630574822426\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.3.attention.self.query.weight | Mean: -1.5129688108572736e-05 | Std: 0.04032899811863899\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.3.attention.self.query.bias | Mean: 0.0026025730185210705 | Std: 0.07573476433753967\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.3.attention.self.key.weight | Mean: 1.7893811673275195e-05 | Std: 0.03996877372264862\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.3.attention.self.key.bias | Mean: 3.4006996429525316e-05 | Std: 0.0026193992234766483\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.3.attention.self.value.weight | Mean: -3.1394415600516368e-06 | Std: 0.029084326699376106\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.3.attention.self.value.bias | Mean: -0.0015018207486718893 | Std: 0.022650547325611115\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.3.attention.output.dense.weight | Mean: -8.192538189177867e-06 | Std: 0.027064841240644455\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.3.attention.output.dense.bias | Mean: -0.0007563908584415913 | Std: 0.045756034553050995\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.3.attention.output.LayerNorm.weight | Mean: 0.9336506128311157 | Std: 0.06915732473134995\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.3.attention.output.LayerNorm.bias | Mean: -0.0064951772801578045 | Std: 0.10827533155679703\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.3.intermediate.dense.weight | Mean: -0.00017378502525389194 | Std: 0.03719846531748772\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.3.intermediate.dense.bias | Mean: -0.06184960529208183 | Std: 0.04470451921224594\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.3.output.dense.weight | Mean: -2.7730249712476507e-05 | Std: 0.035497814416885376\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.3.output.dense.bias | Mean: -0.0005835049669258296 | Std: 0.05560445040464401\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.3.output.LayerNorm.weight | Mean: 0.9260044097900391 | Std: 0.040823087096214294\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.3.output.LayerNorm.bias | Mean: -0.013032548129558563 | Std: 0.04878973960876465\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.4.attention.self.query.weight | Mean: 2.0511026377789676e-05 | Std: 0.03993092477321625\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.4.attention.self.query.bias | Mean: 0.003689834848046303 | Std: 0.09054067730903625\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.4.attention.self.key.weight | Mean: -2.012664481298998e-05 | Std: 0.039772145450115204\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.4.attention.self.key.bias | Mean: -0.0002942732535302639 | Std: 0.0035899870563298464\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.4.attention.self.value.weight | Mean: -4.466691825655289e-05 | Std: 0.03260822594165802\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.4.attention.self.value.bias | Mean: 0.0003292691835667938 | Std: 0.019994420930743217\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.4.attention.output.dense.weight | Mean: 1.229688677995e-05 | Std: 0.029778247699141502\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.4.attention.output.dense.bias | Mean: -0.0002568657509982586 | Std: 0.0701582133769989\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.4.attention.output.LayerNorm.weight | Mean: 0.9234088659286499 | Std: 0.08278868347406387\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.4.attention.output.LayerNorm.bias | Mean: -0.00781102292239666 | Std: 0.10546780377626419\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.4.intermediate.dense.weight | Mean: -0.00016747272456996143 | Std: 0.037287261337041855\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.4.intermediate.dense.bias | Mean: -0.06000804901123047 | Std: 0.04617423564195633\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.4.output.dense.weight | Mean: -2.7359346859157085e-05 | Std: 0.035571448504924774\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.4.output.dense.bias | Mean: -0.0005683714989572763 | Std: 0.04893733561038971\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.4.output.LayerNorm.weight | Mean: 0.9722874164581299 | Std: 0.04486837983131409\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.4.output.LayerNorm.bias | Mean: -0.008890301920473576 | Std: 0.0454581081867218\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.5.attention.self.query.weight | Mean: -9.176114872389007e-06 | Std: 0.0432087704539299\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.5.attention.self.query.bias | Mean: 0.00015388635802082717 | Std: 0.07998623698949814\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.5.attention.self.key.weight | Mean: 2.7982121082459344e-06 | Std: 0.042482178658246994\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.5.attention.self.key.bias | Mean: 0.00015598462778143585 | Std: 0.0033290530554950237\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.5.attention.self.value.weight | Mean: -2.2738471670891158e-05 | Std: 0.03085952438414097\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.5.attention.self.value.bias | Mean: -0.00038420408964157104 | Std: 0.02029254287481308\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.5.attention.output.dense.weight | Mean: 3.5118234791298164e-06 | Std: 0.029024191200733185\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.5.attention.output.dense.bias | Mean: 0.0001877722970675677 | Std: 0.04271339252591133\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.5.attention.output.LayerNorm.weight | Mean: 0.9184279441833496 | Std: 0.07562825083732605\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.5.attention.output.LayerNorm.bias | Mean: -0.008532686159014702 | Std: 0.09993864595890045\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.5.intermediate.dense.weight | Mean: -0.00017314533761236817 | Std: 0.037426698952913284\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.5.intermediate.dense.bias | Mean: -0.061584487557411194 | Std: 0.047446299344301224\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.5.output.dense.weight | Mean: -1.4587159967049956e-05 | Std: 0.03585827723145485\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.5.output.dense.bias | Mean: -0.0007905864040367305 | Std: 0.052213557064533234\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.5.output.LayerNorm.weight | Mean: 0.982016921043396 | Std: 0.04599210247397423\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.5.output.LayerNorm.bias | Mean: -0.006820447742938995 | Std: 0.043879542499780655\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.6.attention.self.query.weight | Mean: 6.0868231230415404e-05 | Std: 0.04352632164955139\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.6.attention.self.query.bias | Mean: -0.003938603214919567 | Std: 0.09089437872171402\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.6.attention.self.key.weight | Mean: -6.2688341131433845e-06 | Std: 0.04252249002456665\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.6.attention.self.key.bias | Mean: -0.0002151592925656587 | Std: 0.0032326935324817896\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.6.attention.self.value.weight | Mean: 2.492097337380983e-05 | Std: 0.03065793216228485\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.6.attention.self.value.bias | Mean: 0.0007412877748720348 | Std: 0.025251764804124832\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.6.attention.output.dense.weight | Mean: -4.280041707716009e-07 | Std: 0.02918425388634205\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.6.attention.output.dense.bias | Mean: 0.0002887183800339699 | Std: 0.04726181551814079\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.6.attention.output.LayerNorm.weight | Mean: 0.927282989025116 | Std: 0.08004210889339447\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.6.attention.output.LayerNorm.bias | Mean: -0.009445756673812866 | Std: 0.10024876892566681\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.6.intermediate.dense.weight | Mean: -0.00019215645443182439 | Std: 0.03761505335569382\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.6.intermediate.dense.bias | Mean: -0.061157699674367905 | Std: 0.04424021765589714\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.6.output.dense.weight | Mean: -1.3221809240349103e-05 | Std: 0.03582245483994484\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.6.output.dense.bias | Mean: -0.0008927574381232262 | Std: 0.07378645241260529\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.6.output.LayerNorm.weight | Mean: 0.9508413076400757 | Std: 0.04866402596235275\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.6.output.LayerNorm.bias | Mean: -0.011740703135728836 | Std: 0.044784367084503174\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.7.attention.self.query.weight | Mean: 1.993973091884982e-05 | Std: 0.041983384639024734\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.7.attention.self.query.bias | Mean: -0.0016945129027590156 | Std: 0.10681872069835663\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.7.attention.self.key.weight | Mean: 1.0337621461076196e-05 | Std: 0.04151614382863045\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.7.attention.self.key.bias | Mean: 8.74624092830345e-05 | Std: 0.00373716838657856\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.7.attention.self.value.weight | Mean: 4.060670471517369e-05 | Std: 0.03175009414553642\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.7.attention.self.value.bias | Mean: -0.00011619855649769306 | Std: 0.03238512948155403\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.7.attention.output.dense.weight | Mean: 5.7704683058545925e-06 | Std: 0.03035713918507099\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.7.attention.output.dense.bias | Mean: 0.00047035104944370687 | Std: 0.04929639771580696\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.7.attention.output.LayerNorm.weight | Mean: 0.9260122179985046 | Std: 0.07593078911304474\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.7.attention.output.LayerNorm.bias | Mean: -0.010572006925940514 | Std: 0.09844765067100525\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.7.intermediate.dense.weight | Mean: -0.00013390625827014446 | Std: 0.036532238125801086\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.7.intermediate.dense.bias | Mean: -0.06375500559806824 | Std: 0.04073246568441391\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.7.output.dense.weight | Mean: -6.737090643582633e-06 | Std: 0.03505273535847664\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.7.output.dense.bias | Mean: -0.0006541122565977275 | Std: 0.07919678092002869\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.7.output.LayerNorm.weight | Mean: 0.9138193130493164 | Std: 0.040204014629125595\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.7.output.LayerNorm.bias | Mean: -0.02532968297600746 | Std: 0.04703393578529358\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.8.attention.self.query.weight | Mean: 4.4443837396102026e-05 | Std: 0.04134617745876312\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.8.attention.self.query.bias | Mean: -0.001904933713376522 | Std: 0.10567699372768402\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.8.attention.self.key.weight | Mean: 3.2841959182405844e-05 | Std: 0.0412522628903389\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.8.attention.self.key.bias | Mean: 7.191110489657149e-05 | Std: 0.004672490060329437\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.8.attention.self.value.weight | Mean: -2.5411718524992466e-06 | Std: 0.032665759325027466\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.8.attention.self.value.bias | Mean: -2.166072044929024e-05 | Std: 0.02468949183821678\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.8.attention.output.dense.weight | Mean: 1.0118742466147523e-05 | Std: 0.03073732927441597\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.8.attention.output.dense.bias | Mean: 0.0003177075122948736 | Std: 0.04517028480768204\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.8.attention.output.LayerNorm.weight | Mean: 0.919428288936615 | Std: 0.08227855712175369\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.8.attention.output.LayerNorm.bias | Mean: -0.011672941967844963 | Std: 0.09198395162820816\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.8.intermediate.dense.weight | Mean: -0.00014530510816257447 | Std: 0.035998858511447906\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.8.intermediate.dense.bias | Mean: -0.06365418434143066 | Std: 0.03383168205618858\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.8.output.dense.weight | Mean: -3.418551386857871e-06 | Std: 0.034732189029455185\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.8.output.dense.bias | Mean: -0.00030310763395391405 | Std: 0.07184493541717529\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.8.output.LayerNorm.weight | Mean: 0.9270167350769043 | Std: 0.035617705434560776\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.8.output.LayerNorm.bias | Mean: -0.029353830963373184 | Std: 0.0517648421227932\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.9.attention.self.query.weight | Mean: 5.2074610721319914e-05 | Std: 0.041730478405952454\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.9.attention.self.query.bias | Mean: -0.004102841019630432 | Std: 0.08511124551296234\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.9.attention.self.key.weight | Mean: 4.6035066247895884e-07 | Std: 0.04139886423945427\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.9.attention.self.key.bias | Mean: -9.958765986084472e-06 | Std: 0.004282957408577204\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.9.attention.self.value.weight | Mean: -4.764561708725523e-06 | Std: 0.03230907395482063\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.9.attention.self.value.bias | Mean: 0.0004425996448844671 | Std: 0.030592989176511765\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.9.attention.output.dense.weight | Mean: -3.121951692719449e-07 | Std: 0.029802829027175903\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.9.attention.output.dense.bias | Mean: 8.245356002589688e-05 | Std: 0.047199003398418427\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.9.attention.output.LayerNorm.weight | Mean: 0.9060143828392029 | Std: 0.05843347683548927\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.9.attention.output.LayerNorm.bias | Mean: -0.01439870335161686 | Std: 0.08900115638971329\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.9.intermediate.dense.weight | Mean: -0.00010822757030837238 | Std: 0.036357659846544266\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.9.intermediate.dense.bias | Mean: -0.06519988924264908 | Std: 0.02785978466272354\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.9.output.dense.weight | Mean: -4.1633661567175295e-06 | Std: 0.03547593206167221\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.9.output.dense.bias | Mean: 0.0002648322843015194 | Std: 0.0717833936214447\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.9.output.LayerNorm.weight | Mean: 0.928630530834198 | Std: 0.0425347164273262\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.9.output.LayerNorm.bias | Mean: -0.028730016201734543 | Std: 0.044719815254211426\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.10.attention.self.query.weight | Mean: 7.663191354367882e-05 | Std: 0.043213531374931335\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.10.attention.self.query.bias | Mean: -0.005275401286780834 | Std: 0.11517941206693649\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.10.attention.self.key.weight | Mean: -2.1688532797270454e-05 | Std: 0.043018292635679245\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.10.attention.self.key.bias | Mean: 0.00029187503969296813 | Std: 0.004044414963573217\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.10.attention.self.value.weight | Mean: -2.8072032364434563e-05 | Std: 0.03217615932226181\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.10.attention.self.value.bias | Mean: 0.0005803594831377268 | Std: 0.02342165820300579\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.10.attention.output.dense.weight | Mean: 1.323013839282794e-05 | Std: 0.029201500117778778\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.10.attention.output.dense.bias | Mean: 1.8477363482816145e-05 | Std: 0.05488155782222748\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.10.attention.output.LayerNorm.weight | Mean: 0.901975154876709 | Std: 0.11721955984830856\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.10.attention.output.LayerNorm.bias | Mean: -0.014900550246238708 | Std: 0.10233353823423386\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.10.intermediate.dense.weight | Mean: -0.0002278953033965081 | Std: 0.03690323233604431\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.10.intermediate.dense.bias | Mean: -0.06227999925613403 | Std: 0.026578357443213463\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.10.output.dense.weight | Mean: 8.433934226559359e-07 | Std: 0.03657408058643341\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.10.output.dense.bias | Mean: -9.280559606850147e-05 | Std: 0.0759490430355072\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.10.output.LayerNorm.weight | Mean: 0.9288660883903503 | Std: 0.06120772659778595\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.10.output.LayerNorm.bias | Mean: -0.02814055047929287 | Std: 0.057219378650188446\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.11.attention.self.query.weight | Mean: -7.846535481803585e-06 | Std: 0.04254070669412613\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.11.attention.self.query.bias | Mean: -0.003219383303076029 | Std: 0.1353084146976471\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.11.attention.self.key.weight | Mean: -4.192604137642775e-06 | Std: 0.04204651340842247\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.11.attention.self.key.bias | Mean: -1.870564119599294e-05 | Std: 0.0037590418942272663\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.11.attention.self.value.weight | Mean: 1.2508998224802781e-05 | Std: 0.039245568215847015\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.11.attention.self.value.bias | Mean: 0.0011480285320430994 | Std: 0.016659894958138466\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.11.attention.output.dense.weight | Mean: -6.5161666498170234e-06 | Std: 0.03605278208851814\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.11.attention.output.dense.bias | Mean: 4.543596878647804e-05 | Std: 0.04791777580976486\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.11.attention.output.LayerNorm.weight | Mean: 0.9016667604446411 | Std: 0.06388186663389206\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.11.attention.output.LayerNorm.bias | Mean: -0.02323704957962036 | Std: 0.08509137481451035\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.11.intermediate.dense.weight | Mean: -6.639526691287756e-05 | Std: 0.03792420029640198\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.11.intermediate.dense.bias | Mean: -0.04456540197134018 | Std: 0.04272228851914406\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.11.output.dense.weight | Mean: -2.95222804425066e-07 | Std: 0.03518391773104668\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.11.output.dense.bias | Mean: -0.0005702704074792564 | Std: 0.059928059577941895\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.11.output.LayerNorm.weight | Mean: 0.7497214674949646 | Std: 0.02688252367079258\n",
            "Layer: sentence_encoder.context_encoder.encoder.layer.11.output.LayerNorm.bias | Mean: -0.014550065621733665 | Std: 0.04981132596731186\n",
            "Layer: sentence_encoder.context_encoder.pooler.dense.weight | Mean: -3.911355815944262e-05 | Std: 0.030852332711219788\n",
            "Layer: sentence_encoder.context_encoder.pooler.dense.bias | Mean: -0.0007650607731193304 | Std: 0.034072306007146835\n",
            "Layer: sentence_encoder.response_encoder.embeddings.word_embeddings.weight | Mean: -0.013846821151673794 | Std: 0.044778406620025635\n",
            "Layer: sentence_encoder.response_encoder.embeddings.position_embeddings.weight | Mean: 4.375215212348849e-06 | Std: 0.014552488923072815\n",
            "Layer: sentence_encoder.response_encoder.embeddings.token_type_embeddings.weight | Mean: -0.0004567124124150723 | Std: 0.025746047496795654\n",
            "Layer: sentence_encoder.response_encoder.embeddings.LayerNorm.weight | Mean: 0.8866543769836426 | Std: 0.09250417351722717\n",
            "Layer: sentence_encoder.response_encoder.embeddings.LayerNorm.bias | Mean: -0.019885143265128136 | Std: 0.06001076102256775\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.0.attention.self.query.weight | Mean: -1.3739396308665164e-05 | Std: 0.0344940721988678\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.0.attention.self.query.bias | Mean: -0.01033021043986082 | Std: 0.214474156498909\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.0.attention.self.key.weight | Mean: 2.1116933567100205e-05 | Std: 0.0339859239757061\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.0.attention.self.key.bias | Mean: -2.224700256192591e-05 | Std: 0.0015585526125505567\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.0.attention.self.value.weight | Mean: 1.0369047231506556e-05 | Std: 0.02527768909931183\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.0.attention.self.value.bias | Mean: 0.0007503176457248628 | Std: 0.03720814362168312\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.0.attention.output.dense.weight | Mean: -3.6421245113160694e-06 | Std: 0.024761490523815155\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.0.attention.output.dense.bias | Mean: -0.0018155656289309263 | Std: 0.0330154225230217\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.0.attention.output.LayerNorm.weight | Mean: 0.9871233701705933 | Std: 0.08989590406417847\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.0.attention.output.LayerNorm.bias | Mean: -0.012962600216269493 | Std: 0.24887695908546448\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.0.intermediate.dense.weight | Mean: -4.095903477718821e-06 | Std: 0.03330910950899124\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.0.intermediate.dense.bias | Mean: -0.08185876905918121 | Std: 0.041359663009643555\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.0.output.dense.weight | Mean: -3.770137846004218e-05 | Std: 0.031355224549770355\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.0.output.dense.bias | Mean: -0.0007258158875629306 | Std: 0.06903459876775742\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.0.output.LayerNorm.weight | Mean: 0.8939478397369385 | Std: 0.053671639412641525\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.0.output.LayerNorm.bias | Mean: -0.03920302540063858 | Std: 0.07369592785835266\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.1.attention.self.query.weight | Mean: 1.173121745523531e-05 | Std: 0.03841559588909149\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.1.attention.self.query.bias | Mean: -0.003118776250630617 | Std: 0.1203642189502716\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.1.attention.self.key.weight | Mean: -1.5655703464290127e-05 | Std: 0.038339368999004364\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.1.attention.self.key.bias | Mean: -7.818375888746232e-05 | Std: 0.002824404276907444\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.1.attention.self.value.weight | Mean: -2.6620209609973244e-05 | Std: 0.026602376252412796\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.1.attention.self.value.bias | Mean: 0.0012437974801287055 | Std: 0.030507182702422142\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.1.attention.output.dense.weight | Mean: 3.415161472730688e-06 | Std: 0.026631783694028854\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.1.attention.output.dense.bias | Mean: -0.0010803377954289317 | Std: 0.06734296679496765\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.1.attention.output.LayerNorm.weight | Mean: 0.9616678953170776 | Std: 0.07337253540754318\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.1.attention.output.LayerNorm.bias | Mean: -0.008811134845018387 | Std: 0.13873903453350067\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.1.intermediate.dense.weight | Mean: -4.156054274062626e-05 | Std: 0.03588266298174858\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.1.intermediate.dense.bias | Mean: -0.06761752814054489 | Std: 0.03901844471693039\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.1.output.dense.weight | Mean: -3.4443935874151066e-05 | Std: 0.03422658145427704\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.1.output.dense.bias | Mean: -0.0008649376104585826 | Std: 0.05324529483914375\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.1.output.LayerNorm.weight | Mean: 0.9423097968101501 | Std: 0.052177298814058304\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.1.output.LayerNorm.bias | Mean: -0.03462633490562439 | Std: 0.07001306861639023\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.2.attention.self.query.weight | Mean: -4.5720807975158095e-05 | Std: 0.04117968678474426\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.2.attention.self.query.bias | Mean: 0.0009442029404453933 | Std: 0.0910048633813858\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.2.attention.self.key.weight | Mean: -9.029969078255817e-05 | Std: 0.040630776435136795\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.2.attention.self.key.bias | Mean: 0.00011806102702394128 | Std: 0.002429635962471366\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.2.attention.self.value.weight | Mean: -1.6708898328943178e-05 | Std: 0.027623837813735008\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.2.attention.self.value.bias | Mean: 0.0005418008076958358 | Std: 0.0329880490899086\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.2.attention.output.dense.weight | Mean: -1.1268550679233158e-06 | Std: 0.026684947311878204\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.2.attention.output.dense.bias | Mean: -0.0007429927354678512 | Std: 0.06926459074020386\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.2.attention.output.LayerNorm.weight | Mean: 0.9397428035736084 | Std: 0.05958188325166702\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.2.attention.output.LayerNorm.bias | Mean: -0.005440631881356239 | Std: 0.11731145530939102\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.2.intermediate.dense.weight | Mean: -0.00018189431284554303 | Std: 0.0367986299097538\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.2.intermediate.dense.bias | Mean: -0.06542064249515533 | Std: 0.0418451689183712\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.2.output.dense.weight | Mean: -2.9077695216983557e-05 | Std: 0.03510678932070732\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.2.output.dense.bias | Mean: -0.000616271048784256 | Std: 0.050226736813783646\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.2.output.LayerNorm.weight | Mean: 0.9326571226119995 | Std: 0.047890424728393555\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.2.output.LayerNorm.bias | Mean: -0.021123012527823448 | Std: 0.05935630574822426\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.3.attention.self.query.weight | Mean: -1.5129688108572736e-05 | Std: 0.04032899811863899\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.3.attention.self.query.bias | Mean: 0.0026025730185210705 | Std: 0.07573476433753967\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.3.attention.self.key.weight | Mean: 1.7893811673275195e-05 | Std: 0.03996877372264862\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.3.attention.self.key.bias | Mean: 3.4006996429525316e-05 | Std: 0.0026193992234766483\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.3.attention.self.value.weight | Mean: -3.1394415600516368e-06 | Std: 0.029084326699376106\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.3.attention.self.value.bias | Mean: -0.0015018207486718893 | Std: 0.022650547325611115\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.3.attention.output.dense.weight | Mean: -8.192538189177867e-06 | Std: 0.027064841240644455\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.3.attention.output.dense.bias | Mean: -0.0007563908584415913 | Std: 0.045756034553050995\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.3.attention.output.LayerNorm.weight | Mean: 0.9336506128311157 | Std: 0.06915732473134995\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.3.attention.output.LayerNorm.bias | Mean: -0.0064951772801578045 | Std: 0.10827533155679703\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.3.intermediate.dense.weight | Mean: -0.00017378502525389194 | Std: 0.03719846531748772\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.3.intermediate.dense.bias | Mean: -0.06184960529208183 | Std: 0.04470451921224594\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.3.output.dense.weight | Mean: -2.7730249712476507e-05 | Std: 0.035497814416885376\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.3.output.dense.bias | Mean: -0.0005835049669258296 | Std: 0.05560445040464401\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.3.output.LayerNorm.weight | Mean: 0.9260044097900391 | Std: 0.040823087096214294\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.3.output.LayerNorm.bias | Mean: -0.013032548129558563 | Std: 0.04878973960876465\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.4.attention.self.query.weight | Mean: 2.0511026377789676e-05 | Std: 0.03993092477321625\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.4.attention.self.query.bias | Mean: 0.003689834848046303 | Std: 0.09054067730903625\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.4.attention.self.key.weight | Mean: -2.012664481298998e-05 | Std: 0.039772145450115204\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.4.attention.self.key.bias | Mean: -0.0002942732535302639 | Std: 0.0035899870563298464\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.4.attention.self.value.weight | Mean: -4.466691825655289e-05 | Std: 0.03260822594165802\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.4.attention.self.value.bias | Mean: 0.0003292691835667938 | Std: 0.019994420930743217\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.4.attention.output.dense.weight | Mean: 1.229688677995e-05 | Std: 0.029778247699141502\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.4.attention.output.dense.bias | Mean: -0.0002568657509982586 | Std: 0.0701582133769989\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.4.attention.output.LayerNorm.weight | Mean: 0.9234088659286499 | Std: 0.08278868347406387\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.4.attention.output.LayerNorm.bias | Mean: -0.00781102292239666 | Std: 0.10546780377626419\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.4.intermediate.dense.weight | Mean: -0.00016747272456996143 | Std: 0.037287261337041855\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.4.intermediate.dense.bias | Mean: -0.06000804901123047 | Std: 0.04617423564195633\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.4.output.dense.weight | Mean: -2.7359346859157085e-05 | Std: 0.035571448504924774\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.4.output.dense.bias | Mean: -0.0005683714989572763 | Std: 0.04893733561038971\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.4.output.LayerNorm.weight | Mean: 0.9722874164581299 | Std: 0.04486837983131409\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.4.output.LayerNorm.bias | Mean: -0.008890301920473576 | Std: 0.0454581081867218\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.5.attention.self.query.weight | Mean: -9.176114872389007e-06 | Std: 0.0432087704539299\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.5.attention.self.query.bias | Mean: 0.00015388635802082717 | Std: 0.07998623698949814\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.5.attention.self.key.weight | Mean: 2.7982121082459344e-06 | Std: 0.042482178658246994\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.5.attention.self.key.bias | Mean: 0.00015598462778143585 | Std: 0.0033290530554950237\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.5.attention.self.value.weight | Mean: -2.2738471670891158e-05 | Std: 0.03085952438414097\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.5.attention.self.value.bias | Mean: -0.00038420408964157104 | Std: 0.02029254287481308\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.5.attention.output.dense.weight | Mean: 3.5118234791298164e-06 | Std: 0.029024191200733185\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.5.attention.output.dense.bias | Mean: 0.0001877722970675677 | Std: 0.04271339252591133\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.5.attention.output.LayerNorm.weight | Mean: 0.9184279441833496 | Std: 0.07562825083732605\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.5.attention.output.LayerNorm.bias | Mean: -0.008532686159014702 | Std: 0.09993864595890045\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.5.intermediate.dense.weight | Mean: -0.00017314533761236817 | Std: 0.037426698952913284\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.5.intermediate.dense.bias | Mean: -0.061584487557411194 | Std: 0.047446299344301224\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.5.output.dense.weight | Mean: -1.4587159967049956e-05 | Std: 0.03585827723145485\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.5.output.dense.bias | Mean: -0.0007905864040367305 | Std: 0.052213557064533234\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.5.output.LayerNorm.weight | Mean: 0.982016921043396 | Std: 0.04599210247397423\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.5.output.LayerNorm.bias | Mean: -0.006820447742938995 | Std: 0.043879542499780655\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.6.attention.self.query.weight | Mean: 6.0868231230415404e-05 | Std: 0.04352632164955139\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.6.attention.self.query.bias | Mean: -0.003938603214919567 | Std: 0.09089437872171402\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.6.attention.self.key.weight | Mean: -6.2688341131433845e-06 | Std: 0.04252249002456665\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.6.attention.self.key.bias | Mean: -0.0002151592925656587 | Std: 0.0032326935324817896\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.6.attention.self.value.weight | Mean: 2.492097337380983e-05 | Std: 0.03065793216228485\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.6.attention.self.value.bias | Mean: 0.0007412877748720348 | Std: 0.025251764804124832\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.6.attention.output.dense.weight | Mean: -4.280041707716009e-07 | Std: 0.02918425388634205\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.6.attention.output.dense.bias | Mean: 0.0002887183800339699 | Std: 0.04726181551814079\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.6.attention.output.LayerNorm.weight | Mean: 0.927282989025116 | Std: 0.08004210889339447\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.6.attention.output.LayerNorm.bias | Mean: -0.009445756673812866 | Std: 0.10024876892566681\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.6.intermediate.dense.weight | Mean: -0.00019215645443182439 | Std: 0.03761505335569382\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.6.intermediate.dense.bias | Mean: -0.061157699674367905 | Std: 0.04424021765589714\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.6.output.dense.weight | Mean: -1.3221809240349103e-05 | Std: 0.03582245483994484\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.6.output.dense.bias | Mean: -0.0008927574381232262 | Std: 0.07378645241260529\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.6.output.LayerNorm.weight | Mean: 0.9508413076400757 | Std: 0.04866402596235275\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.6.output.LayerNorm.bias | Mean: -0.011740703135728836 | Std: 0.044784367084503174\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.7.attention.self.query.weight | Mean: 1.993973091884982e-05 | Std: 0.041983384639024734\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.7.attention.self.query.bias | Mean: -0.0016945129027590156 | Std: 0.10681872069835663\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.7.attention.self.key.weight | Mean: 1.0337621461076196e-05 | Std: 0.04151614382863045\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.7.attention.self.key.bias | Mean: 8.74624092830345e-05 | Std: 0.00373716838657856\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.7.attention.self.value.weight | Mean: 4.060670471517369e-05 | Std: 0.03175009414553642\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.7.attention.self.value.bias | Mean: -0.00011619855649769306 | Std: 0.03238512948155403\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.7.attention.output.dense.weight | Mean: 5.7704683058545925e-06 | Std: 0.03035713918507099\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.7.attention.output.dense.bias | Mean: 0.00047035104944370687 | Std: 0.04929639771580696\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.7.attention.output.LayerNorm.weight | Mean: 0.9260122179985046 | Std: 0.07593078911304474\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.7.attention.output.LayerNorm.bias | Mean: -0.010572006925940514 | Std: 0.09844765067100525\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.7.intermediate.dense.weight | Mean: -0.00013390625827014446 | Std: 0.036532238125801086\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.7.intermediate.dense.bias | Mean: -0.06375500559806824 | Std: 0.04073246568441391\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.7.output.dense.weight | Mean: -6.737090643582633e-06 | Std: 0.03505273535847664\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.7.output.dense.bias | Mean: -0.0006541122565977275 | Std: 0.07919678092002869\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.7.output.LayerNorm.weight | Mean: 0.9138193130493164 | Std: 0.040204014629125595\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.7.output.LayerNorm.bias | Mean: -0.02532968297600746 | Std: 0.04703393578529358\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.8.attention.self.query.weight | Mean: 4.4443837396102026e-05 | Std: 0.04134617745876312\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.8.attention.self.query.bias | Mean: -0.001904933713376522 | Std: 0.10567699372768402\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.8.attention.self.key.weight | Mean: 3.2841959182405844e-05 | Std: 0.0412522628903389\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.8.attention.self.key.bias | Mean: 7.191110489657149e-05 | Std: 0.004672490060329437\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.8.attention.self.value.weight | Mean: -2.5411718524992466e-06 | Std: 0.032665759325027466\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.8.attention.self.value.bias | Mean: -2.166072044929024e-05 | Std: 0.02468949183821678\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.8.attention.output.dense.weight | Mean: 1.0118742466147523e-05 | Std: 0.03073732927441597\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.8.attention.output.dense.bias | Mean: 0.0003177075122948736 | Std: 0.04517028480768204\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.8.attention.output.LayerNorm.weight | Mean: 0.919428288936615 | Std: 0.08227855712175369\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.8.attention.output.LayerNorm.bias | Mean: -0.011672941967844963 | Std: 0.09198395162820816\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.8.intermediate.dense.weight | Mean: -0.00014530510816257447 | Std: 0.035998858511447906\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.8.intermediate.dense.bias | Mean: -0.06365418434143066 | Std: 0.03383168205618858\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.8.output.dense.weight | Mean: -3.418551386857871e-06 | Std: 0.034732189029455185\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.8.output.dense.bias | Mean: -0.00030310763395391405 | Std: 0.07184493541717529\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.8.output.LayerNorm.weight | Mean: 0.9270167350769043 | Std: 0.035617705434560776\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.8.output.LayerNorm.bias | Mean: -0.029353830963373184 | Std: 0.0517648421227932\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.9.attention.self.query.weight | Mean: 5.2074610721319914e-05 | Std: 0.041730478405952454\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.9.attention.self.query.bias | Mean: -0.004102841019630432 | Std: 0.08511124551296234\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.9.attention.self.key.weight | Mean: 4.6035066247895884e-07 | Std: 0.04139886423945427\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.9.attention.self.key.bias | Mean: -9.958765986084472e-06 | Std: 0.004282957408577204\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.9.attention.self.value.weight | Mean: -4.764561708725523e-06 | Std: 0.03230907395482063\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.9.attention.self.value.bias | Mean: 0.0004425996448844671 | Std: 0.030592989176511765\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.9.attention.output.dense.weight | Mean: -3.121951692719449e-07 | Std: 0.029802829027175903\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.9.attention.output.dense.bias | Mean: 8.245356002589688e-05 | Std: 0.047199003398418427\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.9.attention.output.LayerNorm.weight | Mean: 0.9060143828392029 | Std: 0.05843347683548927\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.9.attention.output.LayerNorm.bias | Mean: -0.01439870335161686 | Std: 0.08900115638971329\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.9.intermediate.dense.weight | Mean: -0.00010822757030837238 | Std: 0.036357659846544266\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.9.intermediate.dense.bias | Mean: -0.06519988924264908 | Std: 0.02785978466272354\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.9.output.dense.weight | Mean: -4.1633661567175295e-06 | Std: 0.03547593206167221\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.9.output.dense.bias | Mean: 0.0002648322843015194 | Std: 0.0717833936214447\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.9.output.LayerNorm.weight | Mean: 0.928630530834198 | Std: 0.0425347164273262\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.9.output.LayerNorm.bias | Mean: -0.028730016201734543 | Std: 0.044719815254211426\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.10.attention.self.query.weight | Mean: 7.663191354367882e-05 | Std: 0.043213531374931335\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.10.attention.self.query.bias | Mean: -0.005275401286780834 | Std: 0.11517941206693649\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.10.attention.self.key.weight | Mean: -2.1688532797270454e-05 | Std: 0.043018292635679245\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.10.attention.self.key.bias | Mean: 0.00029187503969296813 | Std: 0.004044414963573217\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.10.attention.self.value.weight | Mean: -2.8072032364434563e-05 | Std: 0.03217615932226181\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.10.attention.self.value.bias | Mean: 0.0005803594831377268 | Std: 0.02342165820300579\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.10.attention.output.dense.weight | Mean: 1.323013839282794e-05 | Std: 0.029201500117778778\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.10.attention.output.dense.bias | Mean: 1.8477363482816145e-05 | Std: 0.05488155782222748\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.10.attention.output.LayerNorm.weight | Mean: 0.901975154876709 | Std: 0.11721955984830856\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.10.attention.output.LayerNorm.bias | Mean: -0.014900550246238708 | Std: 0.10233353823423386\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.10.intermediate.dense.weight | Mean: -0.0002278953033965081 | Std: 0.03690323233604431\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.10.intermediate.dense.bias | Mean: -0.06227999925613403 | Std: 0.026578357443213463\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.10.output.dense.weight | Mean: 8.433934226559359e-07 | Std: 0.03657408058643341\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.10.output.dense.bias | Mean: -9.280559606850147e-05 | Std: 0.0759490430355072\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.10.output.LayerNorm.weight | Mean: 0.9288660883903503 | Std: 0.06120772659778595\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.10.output.LayerNorm.bias | Mean: -0.02814055047929287 | Std: 0.057219378650188446\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.11.attention.self.query.weight | Mean: -7.846535481803585e-06 | Std: 0.04254070669412613\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.11.attention.self.query.bias | Mean: -0.003219383303076029 | Std: 0.1353084146976471\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.11.attention.self.key.weight | Mean: -4.192604137642775e-06 | Std: 0.04204651340842247\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.11.attention.self.key.bias | Mean: -1.870564119599294e-05 | Std: 0.0037590418942272663\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.11.attention.self.value.weight | Mean: 1.2508998224802781e-05 | Std: 0.039245568215847015\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.11.attention.self.value.bias | Mean: 0.0011480285320430994 | Std: 0.016659894958138466\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.11.attention.output.dense.weight | Mean: -6.5161666498170234e-06 | Std: 0.03605278208851814\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.11.attention.output.dense.bias | Mean: 4.543596878647804e-05 | Std: 0.04791777580976486\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.11.attention.output.LayerNorm.weight | Mean: 0.9016667604446411 | Std: 0.06388186663389206\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.11.attention.output.LayerNorm.bias | Mean: -0.02323704957962036 | Std: 0.08509137481451035\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.11.intermediate.dense.weight | Mean: -6.639526691287756e-05 | Std: 0.03792420029640198\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.11.intermediate.dense.bias | Mean: -0.04456540197134018 | Std: 0.04272228851914406\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.11.output.dense.weight | Mean: -2.95222804425066e-07 | Std: 0.03518391773104668\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.11.output.dense.bias | Mean: -0.0005702704074792564 | Std: 0.059928059577941895\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.11.output.LayerNorm.weight | Mean: 0.7497214674949646 | Std: 0.02688252367079258\n",
            "Layer: sentence_encoder.response_encoder.encoder.layer.11.output.LayerNorm.bias | Mean: -0.014550065621733665 | Std: 0.04981132596731186\n",
            "Layer: sentence_encoder.response_encoder.pooler.dense.weight | Mean: -3.911355815944262e-05 | Std: 0.030852332711219788\n",
            "Layer: sentence_encoder.response_encoder.pooler.dense.bias | Mean: -0.0007650607731193304 | Std: 0.034072306007146835\n",
            "Layer: sentence_encoder.lstm.weight_ih_l0 | Mean: 1.8750160961644724e-05 | Std: 0.03333636745810509\n",
            "Layer: sentence_encoder.lstm.weight_hh_l0 | Mean: -7.912894034234341e-06 | Std: 0.03334880992770195\n",
            "Layer: sentence_encoder.lstm.bias_ih_l0 | Mean: -0.0005125314346514642 | Std: 0.032939549535512924\n",
            "Layer: sentence_encoder.lstm.bias_hh_l0 | Mean: -5.363866875995882e-05 | Std: 0.032921601086854935\n",
            "Layer: sentence_encoder.lstm.weight_ih_l0_reverse | Mean: -3.137843668810092e-05 | Std: 0.033314503729343414\n",
            "Layer: sentence_encoder.lstm.weight_hh_l0_reverse | Mean: -9.64584614848718e-06 | Std: 0.03330607712268829\n",
            "Layer: sentence_encoder.lstm.bias_ih_l0_reverse | Mean: -0.0003073485568165779 | Std: 0.032857831567525864\n",
            "Layer: sentence_encoder.lstm.bias_hh_l0_reverse | Mean: 0.0010899141198024154 | Std: 0.032709989696741104\n",
            "Layer: context_summarizer.conv2D.weight | Mean: -2.1534549432544736e-06 | Std: 0.010424104519188404\n",
            "Layer: context_summarizer.conv2D.bias | Mean: 0.0001561286044307053 | Std: 0.009677846916019917\n",
            "Layer: context_encoder.bilstm.weight_ih_l0 | Mean: -1.0956498954328708e-05 | Std: 0.03333764150738716\n",
            "Layer: context_encoder.bilstm.weight_hh_l0 | Mean: -2.4708917408133857e-05 | Std: 0.033359359949827194\n",
            "Layer: context_encoder.bilstm.bias_ih_l0 | Mean: -0.0011251561809331179 | Std: 0.033439960330724716\n",
            "Layer: context_encoder.bilstm.bias_hh_l0 | Mean: -0.0006327245500870049 | Std: 0.032933346927165985\n",
            "Layer: context_encoder.bilstm.weight_ih_l0_reverse | Mean: -9.56232634052867e-06 | Std: 0.033329688012599945\n",
            "Layer: context_encoder.bilstm.weight_hh_l0_reverse | Mean: 6.978121382417157e-05 | Std: 0.033346112817525864\n",
            "Layer: context_encoder.bilstm.bias_ih_l0_reverse | Mean: 0.0008340487838722765 | Std: 0.03360730782151222\n",
            "Layer: context_encoder.bilstm.bias_hh_l0_reverse | Mean: -0.0025011245161294937 | Std: 0.03315114229917526\n",
            "Layer: syntactic_feature_fc.weight | Mean: -0.0015077854041010141 | Std: 0.29327845573425293\n",
            "Layer: syntactic_feature_fc.bias | Mean: 0.007403429597616196 | Std: 0.2932247519493103\n",
            "Layer: cnn_layer.convs.0.weight | Mean: 0.011139837093651295 | Std: 0.28916487097740173\n",
            "Layer: cnn_layer.convs.0.bias | Mean: 0.020992359146475792 | Std: 0.2942315340042114\n",
            "Layer: cnn_layer.convs.1.weight | Mean: -0.0061454470269382 | Std: 0.2356327474117279\n",
            "Layer: cnn_layer.convs.1.bias | Mean: -0.00040207523852586746 | Std: 0.2334619015455246\n",
            "Layer: cnn_layer.convs.2.weight | Mean: 0.003880572272464633 | Std: 0.18368832767009735\n",
            "Layer: cnn_layer.convs.2.bias | Mean: 0.009681967087090015 | Std: 0.18587085604667664\n",
            "Layer: fc_layer.fc.weight | Mean: 0.0001140418098657392 | Std: 0.03878223896026611\n",
            "Layer: fc_layer.fc.bias | Mean: 0.0 | Std: nan\n"
          ]
        }
      ],
      "source": [
        "for name, param in model.named_parameters():\n",
        "   print(f\"Layer: {name} | Mean: {param.mean().item()} | Std: {param.std().item()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jSIrnRaXDgEd",
        "outputId": "a3882e49-f75e-45d0-f1c5-ed79c3a60fbb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs:   0%|          | 0/25 [00:00<?, ?it/s]\n",
            "Training Progress:   0%|          | 0/440 [00:00<?, ?it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   0%|          | 1/440 [00:04<35:43,  4.88s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   0%|          | 2/440 [00:07<25:03,  3.43s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   1%|          | 3/440 [00:09<21:36,  2.97s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   1%|          | 4/440 [00:12<20:00,  2.75s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   1%|          | 5/440 [00:14<19:01,  2.62s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   1%|         | 6/440 [00:16<18:32,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   2%|         | 7/440 [00:19<18:11,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   2%|         | 8/440 [00:21<17:54,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   2%|         | 9/440 [00:24<18:32,  2.58s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   2%|         | 10/440 [00:27<18:10,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   2%|         | 11/440 [00:29<17:57,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   3%|         | 12/440 [00:31<17:43,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   3%|         | 13/440 [00:34<17:37,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   3%|         | 14/440 [00:36<17:31,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   3%|         | 15/440 [00:39<17:25,  2.46s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   4%|         | 16/440 [00:41<17:23,  2.46s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   4%|         | 17/440 [00:44<17:22,  2.46s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   4%|         | 18/440 [00:46<17:20,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   4%|         | 19/440 [00:49<17:21,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   5%|         | 20/440 [00:51<17:18,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   5%|         | 21/440 [00:54<17:18,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   5%|         | 22/440 [00:56<17:15,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   5%|         | 23/440 [00:59<17:13,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   5%|         | 24/440 [01:01<17:11,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   6%|         | 25/440 [01:04<17:10,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   6%|         | 26/440 [01:06<17:10,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   6%|         | 27/440 [01:09<17:09,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   6%|         | 28/440 [01:11<17:06,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   7%|         | 29/440 [01:14<17:08,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   7%|         | 30/440 [01:16<17:06,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   7%|         | 31/440 [01:19<17:06,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   7%|         | 32/440 [01:21<17:06,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   8%|         | 33/440 [01:24<17:02,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   8%|         | 34/440 [01:26<16:58,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   8%|         | 35/440 [01:29<16:56,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   8%|         | 36/440 [01:31<16:55,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   8%|         | 37/440 [01:34<16:50,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   9%|         | 38/440 [01:36<16:44,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   9%|         | 39/440 [01:39<16:42,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   9%|         | 40/440 [01:41<16:38,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   9%|         | 41/440 [01:44<16:42,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  10%|         | 42/440 [01:46<16:39,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  10%|         | 43/440 [01:49<16:33,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  10%|         | 44/440 [01:51<16:26,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  10%|         | 45/440 [01:54<17:00,  2.58s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  10%|         | 46/440 [01:56<16:50,  2.57s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  11%|         | 47/440 [01:59<16:34,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  11%|         | 48/440 [02:01<16:22,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  11%|         | 49/440 [02:04<16:14,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  11%|        | 50/440 [02:06<16:08,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  12%|        | 51/440 [02:09<16:10,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  12%|        | 52/440 [02:11<16:06,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  12%|        | 53/440 [02:14<16:03,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  12%|        | 54/440 [02:16<15:59,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  12%|        | 55/440 [02:19<15:56,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  13%|        | 56/440 [02:21<15:55,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  13%|        | 57/440 [02:24<15:54,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  13%|        | 58/440 [02:26<16:00,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  13%|        | 59/440 [02:29<15:52,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  14%|        | 60/440 [02:31<15:51,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  14%|        | 61/440 [02:34<15:49,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  14%|        | 62/440 [02:36<15:46,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  14%|        | 63/440 [02:39<15:41,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  15%|        | 64/440 [02:41<15:37,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  15%|        | 65/440 [02:44<15:36,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  15%|        | 66/440 [02:46<15:30,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  15%|        | 67/440 [02:49<15:30,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  15%|        | 68/440 [02:51<15:27,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  16%|        | 69/440 [02:54<15:24,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  16%|        | 70/440 [02:56<15:25,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  16%|        | 71/440 [02:59<15:19,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  16%|        | 72/440 [03:01<15:16,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  17%|        | 73/440 [03:04<15:11,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  17%|        | 74/440 [03:06<15:09,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  17%|        | 75/440 [03:09<15:11,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  17%|        | 76/440 [03:11<15:09,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  18%|        | 77/440 [03:14<15:04,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  18%|        | 78/440 [03:16<14:59,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  18%|        | 79/440 [03:19<14:56,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  18%|        | 80/440 [03:21<14:53,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  18%|        | 81/440 [03:24<14:50,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  19%|        | 82/440 [03:26<14:46,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  19%|        | 83/440 [03:29<14:43,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  19%|        | 84/440 [03:31<14:42,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  19%|        | 85/440 [03:33<14:40,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  20%|        | 86/440 [03:36<15:14,  2.58s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  20%|        | 87/440 [03:39<15:01,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  20%|        | 88/440 [03:41<14:51,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  20%|        | 89/440 [03:44<14:43,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  20%|        | 90/440 [03:46<14:41,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  21%|        | 91/440 [03:49<14:36,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  21%|        | 92/440 [03:51<14:30,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  21%|        | 93/440 [03:54<14:27,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  21%|       | 94/440 [03:56<14:24,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  22%|       | 95/440 [03:59<14:24,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  22%|       | 96/440 [04:01<14:19,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  22%|       | 97/440 [04:04<14:15,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  22%|       | 98/440 [04:06<14:12,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  22%|       | 99/440 [04:09<14:10,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  23%|       | 100/440 [04:11<14:12,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  23%|       | 101/440 [04:14<14:08,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  23%|       | 102/440 [04:16<14:07,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  23%|       | 103/440 [04:19<14:02,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  24%|       | 104/440 [04:21<14:00,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  24%|       | 105/440 [04:24<14:02,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  24%|       | 106/440 [04:26<13:57,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  24%|       | 107/440 [04:29<13:54,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  25%|       | 108/440 [04:31<13:51,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  25%|       | 109/440 [04:34<13:47,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  25%|       | 110/440 [04:36<13:46,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  25%|       | 111/440 [04:39<13:43,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  25%|       | 112/440 [04:41<13:41,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  26%|       | 113/440 [04:44<13:38,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  26%|       | 114/440 [04:46<13:38,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  26%|       | 115/440 [04:49<13:37,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  26%|       | 116/440 [04:51<13:32,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  27%|       | 117/440 [04:54<13:29,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  27%|       | 118/440 [04:56<13:25,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  27%|       | 119/440 [04:59<13:22,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  27%|       | 120/440 [05:01<13:19,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  28%|       | 121/440 [05:04<13:16,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  28%|       | 122/440 [05:06<13:14,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  28%|       | 123/440 [05:09<13:45,  2.61s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  28%|       | 124/440 [05:12<13:35,  2.58s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  28%|       | 125/440 [05:14<13:24,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  29%|       | 126/440 [05:17<13:15,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  29%|       | 127/440 [05:19<13:08,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  29%|       | 128/440 [05:22<13:03,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  29%|       | 129/440 [05:24<13:02,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  30%|       | 130/440 [05:27<12:57,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  30%|       | 131/440 [05:29<12:54,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  30%|       | 132/440 [05:32<12:49,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  30%|       | 133/440 [05:34<12:45,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  30%|       | 134/440 [05:37<12:44,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  31%|       | 135/440 [05:39<12:40,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  31%|       | 136/440 [05:42<12:37,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  31%|       | 137/440 [05:44<12:34,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  31%|      | 138/440 [05:47<12:30,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  32%|      | 139/440 [05:49<12:32,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  32%|      | 140/440 [05:52<12:28,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  32%|      | 141/440 [05:54<12:24,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  32%|      | 142/440 [05:57<12:23,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  32%|      | 143/440 [05:59<12:21,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  33%|      | 144/440 [06:02<12:19,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  33%|      | 145/440 [06:04<12:16,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  33%|      | 146/440 [06:07<12:13,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  33%|      | 147/440 [06:09<12:10,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  34%|      | 148/440 [06:12<12:08,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  34%|      | 149/440 [06:14<12:07,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  34%|      | 150/440 [06:17<12:03,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  34%|      | 151/440 [06:19<12:01,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  35%|      | 152/440 [06:22<11:58,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  35%|      | 153/440 [06:24<11:57,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  35%|      | 154/440 [06:27<11:56,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  35%|      | 155/440 [06:29<11:53,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  35%|      | 156/440 [06:32<11:49,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  36%|      | 157/440 [06:34<11:44,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  36%|      | 158/440 [06:37<11:41,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  36%|      | 159/440 [06:39<11:42,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  36%|      | 160/440 [06:42<11:40,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  37%|      | 161/440 [06:44<11:36,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  37%|      | 162/440 [06:47<11:33,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  37%|      | 163/440 [06:49<12:03,  2.61s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  37%|      | 164/440 [06:52<11:52,  2.58s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  38%|      | 165/440 [06:54<11:42,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  38%|      | 166/440 [06:57<11:38,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  38%|      | 167/440 [06:59<11:31,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  38%|      | 168/440 [07:02<11:27,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  38%|      | 169/440 [07:04<11:22,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  39%|      | 170/440 [07:07<11:16,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  39%|      | 171/440 [07:09<11:13,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  39%|      | 172/440 [07:12<11:09,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  39%|      | 173/440 [07:14<11:06,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  40%|      | 174/440 [07:17<11:01,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  40%|      | 175/440 [07:19<11:00,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  40%|      | 176/440 [07:22<10:56,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  40%|      | 177/440 [07:24<10:55,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  40%|      | 178/440 [07:27<10:55,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  41%|      | 179/440 [07:29<10:54,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  41%|      | 180/440 [07:32<10:52,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  41%|      | 181/440 [07:34<10:47,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  41%|     | 182/440 [07:37<10:43,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  42%|     | 183/440 [07:39<10:41,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  42%|     | 184/440 [07:42<10:40,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  42%|     | 185/440 [07:44<10:37,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  42%|     | 186/440 [07:47<10:33,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  42%|     | 187/440 [07:49<10:32,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  43%|     | 188/440 [07:52<10:34,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  43%|     | 189/440 [07:54<10:30,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  43%|     | 190/440 [07:57<10:29,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  43%|     | 191/440 [07:59<10:25,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  44%|     | 192/440 [08:02<10:21,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  44%|     | 193/440 [08:05<10:21,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  44%|     | 194/440 [08:07<10:17,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  44%|     | 195/440 [08:10<10:14,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  45%|     | 196/440 [08:12<10:15,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  45%|     | 197/440 [08:15<10:10,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  45%|     | 198/440 [08:17<10:11,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  45%|     | 199/440 [08:20<10:33,  2.63s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  45%|     | 200/440 [08:22<10:21,  2.59s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  46%|     | 201/440 [08:25<10:14,  2.57s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  46%|     | 202/440 [08:28<10:09,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  46%|     | 203/440 [08:30<10:04,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  46%|     | 204/440 [08:33<09:58,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  47%|     | 205/440 [08:35<09:51,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  47%|     | 206/440 [08:38<09:48,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  47%|     | 207/440 [08:40<09:45,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  47%|     | 208/440 [08:43<09:41,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  48%|     | 209/440 [08:45<09:37,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  48%|     | 210/440 [08:48<09:35,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  48%|     | 211/440 [08:50<09:32,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  48%|     | 212/440 [08:53<09:30,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  48%|     | 213/440 [08:55<09:26,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  49%|     | 214/440 [08:57<09:22,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  49%|     | 215/440 [09:00<09:19,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  49%|     | 216/440 [09:02<09:15,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  49%|     | 217/440 [09:05<09:12,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  50%|     | 218/440 [09:07<09:11,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  50%|     | 219/440 [09:10<09:09,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  50%|     | 220/440 [09:12<09:09,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  50%|     | 221/440 [09:15<09:07,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  50%|     | 222/440 [09:17<09:07,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  51%|     | 223/440 [09:20<09:03,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  51%|     | 224/440 [09:22<08:59,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  51%|     | 225/440 [09:25<08:56,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  51%|    | 226/440 [09:27<08:54,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  52%|    | 227/440 [09:30<08:54,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  52%|    | 228/440 [09:32<08:51,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  52%|    | 229/440 [09:35<08:48,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  52%|    | 230/440 [09:37<08:44,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  52%|    | 231/440 [09:40<08:42,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  53%|    | 232/440 [09:42<08:41,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  53%|    | 233/440 [09:45<08:37,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  53%|    | 234/440 [09:47<08:35,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  53%|    | 235/440 [09:50<08:32,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  54%|    | 236/440 [09:52<08:30,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  54%|    | 237/440 [09:55<08:29,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  54%|    | 238/440 [09:58<08:26,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  54%|    | 239/440 [10:00<08:43,  2.61s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  55%|    | 240/440 [10:03<08:34,  2.57s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  55%|    | 241/440 [10:05<08:27,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  55%|    | 242/440 [10:08<08:23,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  55%|    | 243/440 [10:10<08:18,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  55%|    | 244/440 [10:13<08:13,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  56%|    | 245/440 [10:15<08:08,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  56%|    | 246/440 [10:18<08:05,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  56%|    | 247/440 [10:20<08:04,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  56%|    | 248/440 [10:23<08:00,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  57%|    | 249/440 [10:25<07:57,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  57%|    | 250/440 [10:28<07:54,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  57%|    | 251/440 [10:30<07:52,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  57%|    | 252/440 [10:33<07:49,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  57%|    | 253/440 [10:35<07:46,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  58%|    | 254/440 [10:38<07:44,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  58%|    | 255/440 [10:40<07:42,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  58%|    | 256/440 [10:43<07:41,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  58%|    | 257/440 [10:45<07:36,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  59%|    | 258/440 [10:48<07:33,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  59%|    | 259/440 [10:50<07:30,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  59%|    | 260/440 [10:53<07:29,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  59%|    | 261/440 [10:55<07:26,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  60%|    | 262/440 [10:58<07:24,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  60%|    | 263/440 [11:00<07:21,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  60%|    | 264/440 [11:03<07:19,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  60%|    | 265/440 [11:05<07:15,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  60%|    | 266/440 [11:08<07:14,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  61%|    | 267/440 [11:10<07:10,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  61%|    | 268/440 [11:13<07:09,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  61%|    | 269/440 [11:15<07:06,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  61%|   | 270/440 [11:18<07:03,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  62%|   | 271/440 [11:20<07:02,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  62%|   | 272/440 [11:23<06:58,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  62%|   | 273/440 [11:25<06:54,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  62%|   | 274/440 [11:28<06:53,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  62%|   | 275/440 [11:30<07:06,  2.58s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  63%|   | 276/440 [11:33<07:01,  2.57s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  63%|   | 277/440 [11:35<06:54,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  63%|   | 278/440 [11:38<06:48,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  63%|   | 279/440 [11:40<06:45,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  64%|   | 280/440 [11:43<06:42,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  64%|   | 281/440 [11:45<06:39,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  64%|   | 282/440 [11:48<06:35,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  64%|   | 283/440 [11:50<06:32,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  65%|   | 284/440 [11:53<06:29,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  65%|   | 285/440 [11:55<06:26,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  65%|   | 286/440 [11:58<06:27,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  65%|   | 287/440 [12:01<06:24,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  65%|   | 288/440 [12:03<06:21,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  66%|   | 289/440 [12:06<06:18,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  66%|   | 290/440 [12:08<06:15,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  66%|   | 291/440 [12:11<06:13,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  66%|   | 292/440 [12:13<06:11,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  67%|   | 293/440 [12:16<06:08,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  67%|   | 294/440 [12:18<06:04,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  67%|   | 295/440 [12:21<06:02,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  67%|   | 296/440 [12:23<06:01,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  68%|   | 297/440 [12:26<05:59,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  68%|   | 298/440 [12:28<05:56,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  68%|   | 299/440 [12:31<05:52,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  68%|   | 300/440 [12:33<05:50,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  68%|   | 301/440 [12:36<05:47,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  69%|   | 302/440 [12:38<05:44,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  69%|   | 303/440 [12:41<05:41,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  69%|   | 304/440 [12:43<05:38,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  69%|   | 305/440 [12:46<05:37,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  70%|   | 306/440 [12:48<05:33,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  70%|   | 307/440 [12:50<05:31,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  70%|   | 308/440 [12:53<05:28,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  70%|   | 309/440 [12:55<05:26,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  70%|   | 310/440 [12:58<05:24,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  71%|   | 311/440 [13:00<05:22,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  71%|   | 312/440 [13:03<05:19,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  71%|   | 313/440 [13:05<05:16,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  71%|  | 314/440 [13:08<05:12,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  72%|  | 315/440 [13:11<05:24,  2.60s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  72%|  | 316/440 [13:13<05:18,  2.57s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  72%|  | 317/440 [13:16<05:12,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  72%|  | 318/440 [13:18<05:07,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  72%|  | 319/440 [13:21<05:03,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  73%|  | 320/440 [13:23<05:00,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  73%|  | 321/440 [13:26<04:56,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  73%|  | 322/440 [13:28<04:54,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  73%|  | 323/440 [13:31<04:51,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  74%|  | 324/440 [13:33<04:48,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  74%|  | 325/440 [13:36<04:49,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  74%|  | 326/440 [13:38<04:45,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  74%|  | 327/440 [13:41<04:42,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  75%|  | 328/440 [13:43<04:39,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  75%|  | 329/440 [13:46<04:36,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  75%|  | 330/440 [13:48<04:35,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  75%|  | 331/440 [13:51<04:32,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  75%|  | 332/440 [13:53<04:29,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  76%|  | 333/440 [13:56<04:27,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  76%|  | 334/440 [13:58<04:26,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  76%|  | 335/440 [14:01<04:24,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  76%|  | 336/440 [14:03<04:20,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  77%|  | 337/440 [14:06<04:18,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  77%|  | 338/440 [14:08<04:15,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  77%|  | 339/440 [14:11<04:12,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  77%|  | 340/440 [14:13<04:11,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  78%|  | 341/440 [14:16<04:08,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  78%|  | 342/440 [14:18<04:05,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  78%|  | 343/440 [14:21<04:02,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  78%|  | 344/440 [14:23<04:00,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  78%|  | 345/440 [14:26<03:57,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  79%|  | 346/440 [14:28<03:54,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  79%|  | 347/440 [14:31<03:51,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  79%|  | 348/440 [14:33<03:50,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  79%|  | 349/440 [14:36<03:49,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  80%|  | 350/440 [14:38<03:46,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  80%|  | 351/440 [14:41<03:53,  2.62s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  80%|  | 352/440 [14:44<03:47,  2.59s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  80%|  | 353/440 [14:46<03:42,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  80%|  | 354/440 [14:49<03:38,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  81%|  | 355/440 [14:51<03:35,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  81%|  | 356/440 [14:54<03:32,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  81%|  | 357/440 [14:56<03:28,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  81%| | 358/440 [14:59<03:26,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  82%| | 359/440 [15:01<03:24,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  82%| | 360/440 [15:04<03:21,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  82%| | 361/440 [15:06<03:19,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  82%| | 362/440 [15:09<03:17,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  82%| | 363/440 [15:11<03:15,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  83%| | 364/440 [15:14<03:13,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  83%| | 365/440 [15:16<03:10,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  83%| | 366/440 [15:19<03:06,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  83%| | 367/440 [15:21<03:03,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  84%| | 368/440 [15:24<03:00,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  84%| | 369/440 [15:27<02:59,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  84%| | 370/440 [15:29<02:56,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  84%| | 371/440 [15:32<02:53,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  85%| | 372/440 [15:34<02:50,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  85%| | 373/440 [15:36<02:47,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  85%| | 374/440 [15:39<02:45,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  85%| | 375/440 [15:42<02:43,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  85%| | 376/440 [15:44<02:40,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  86%| | 377/440 [15:47<02:37,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  86%| | 378/440 [15:49<02:34,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  86%| | 379/440 [15:51<02:32,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  86%| | 380/440 [15:54<02:29,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  87%| | 381/440 [15:56<02:26,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  87%| | 382/440 [15:59<02:24,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  87%| | 383/440 [16:01<02:21,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  87%| | 384/440 [16:04<02:19,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  88%| | 385/440 [16:06<02:16,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  88%| | 386/440 [16:09<02:14,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  88%| | 387/440 [16:11<02:11,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  88%| | 388/440 [16:14<02:09,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  88%| | 389/440 [16:16<02:07,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  89%| | 390/440 [16:19<02:04,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  89%| | 391/440 [16:22<02:07,  2.60s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  89%| | 392/440 [16:24<02:03,  2.58s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  89%| | 393/440 [16:27<02:00,  2.57s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  90%| | 394/440 [16:29<01:57,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  90%| | 395/440 [16:32<01:53,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  90%| | 396/440 [16:34<01:50,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  90%| | 397/440 [16:37<01:47,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  90%| | 398/440 [16:39<01:45,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  91%| | 399/440 [16:42<01:43,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  91%| | 400/440 [16:44<01:40,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  91%| | 401/440 [16:47<01:37,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  91%|| 402/440 [16:49<01:35,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  92%|| 403/440 [16:52<01:32,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  92%|| 404/440 [16:54<01:29,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  92%|| 405/440 [16:57<01:27,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  92%|| 406/440 [16:59<01:24,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  92%|| 407/440 [17:02<01:22,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  93%|| 408/440 [17:04<01:19,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  93%|| 409/440 [17:07<01:17,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  93%|| 410/440 [17:09<01:14,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  93%|| 411/440 [17:12<01:12,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  94%|| 412/440 [17:14<01:09,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  94%|| 413/440 [17:17<01:07,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  94%|| 414/440 [17:19<01:04,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  94%|| 415/440 [17:22<01:02,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  95%|| 416/440 [17:24<00:59,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  95%|| 417/440 [17:27<00:57,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  95%|| 418/440 [17:29<00:55,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  95%|| 419/440 [17:32<00:52,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  95%|| 420/440 [17:34<00:49,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  96%|| 421/440 [17:37<00:47,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  96%|| 422/440 [17:39<00:44,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  96%|| 423/440 [17:42<00:42,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  96%|| 424/440 [17:44<00:40,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  97%|| 425/440 [17:47<00:37,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  97%|| 426/440 [17:49<00:35,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  97%|| 427/440 [17:52<00:33,  2.61s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  97%|| 428/440 [17:55<00:30,  2.58s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  98%|| 429/440 [17:57<00:28,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  98%|| 430/440 [18:00<00:25,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  98%|| 431/440 [18:02<00:22,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  98%|| 432/440 [18:05<00:20,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  98%|| 433/440 [18:07<00:17,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  99%|| 434/440 [18:10<00:15,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  99%|| 435/440 [18:12<00:12,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  99%|| 436/440 [18:15<00:09,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  99%|| 437/440 [18:17<00:07,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress: 100%|| 438/440 [18:20<00:04,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress: 100%|| 439/440 [18:22<00:02,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress: 100%|| 440/440 [18:24<00:00,  2.51s/it]\n",
            "\n",
            "Validation Progress:   0%|          | 0/110 [00:00<?, ?it/s]\u001b[A\n",
            "Validation Progress:   1%|          | 1/110 [00:00<01:26,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   2%|         | 2/110 [00:01<01:24,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   3%|         | 3/110 [00:02<01:23,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   4%|         | 4/110 [00:03<01:23,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   5%|         | 5/110 [00:03<01:23,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   5%|         | 6/110 [00:04<01:22,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   6%|         | 7/110 [00:05<01:21,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   7%|         | 8/110 [00:06<01:20,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   8%|         | 9/110 [00:07<01:19,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   9%|         | 10/110 [00:07<01:18,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  10%|         | 11/110 [00:08<01:17,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  11%|         | 12/110 [00:09<01:16,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  12%|        | 13/110 [00:10<01:15,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  13%|        | 14/110 [00:11<01:15,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  14%|        | 15/110 [00:11<01:14,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  15%|        | 16/110 [00:12<01:13,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  15%|        | 17/110 [00:13<01:12,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  16%|        | 18/110 [00:14<01:11,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  17%|        | 19/110 [00:14<01:11,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  18%|        | 20/110 [00:15<01:11,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  19%|        | 21/110 [00:16<01:10,  1.25it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  20%|        | 22/110 [00:17<01:09,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  21%|        | 23/110 [00:18<01:08,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  22%|       | 24/110 [00:18<01:07,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  23%|       | 25/110 [00:19<01:06,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  24%|       | 26/110 [00:20<01:06,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  25%|       | 27/110 [00:21<01:05,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  25%|       | 28/110 [00:22<01:04,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  26%|       | 29/110 [00:22<01:03,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  27%|       | 30/110 [00:23<01:02,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  28%|       | 31/110 [00:24<01:01,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  29%|       | 32/110 [00:25<01:01,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  30%|       | 33/110 [00:25<01:00,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  31%|       | 34/110 [00:26<00:59,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  32%|      | 35/110 [00:27<00:58,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  33%|      | 36/110 [00:28<00:58,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  34%|      | 37/110 [00:29<00:57,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  35%|      | 38/110 [00:29<00:56,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  35%|      | 39/110 [00:30<00:55,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  36%|      | 40/110 [00:31<00:55,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  37%|      | 41/110 [00:32<00:54,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  38%|      | 42/110 [00:33<00:53,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  39%|      | 43/110 [00:33<00:52,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  40%|      | 44/110 [00:34<00:51,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  41%|      | 45/110 [00:35<00:50,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  42%|     | 46/110 [00:36<00:49,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  43%|     | 47/110 [00:36<00:49,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  44%|     | 48/110 [00:37<00:48,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  45%|     | 49/110 [00:38<00:47,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  45%|     | 50/110 [00:39<00:46,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  46%|     | 51/110 [00:40<00:46,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  47%|     | 52/110 [00:40<00:45,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  48%|     | 53/110 [00:41<00:44,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  49%|     | 54/110 [00:42<00:44,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  50%|     | 55/110 [00:43<00:43,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  51%|     | 56/110 [00:43<00:42,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  52%|    | 57/110 [00:44<00:41,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  53%|    | 58/110 [00:45<00:40,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  54%|    | 59/110 [00:46<00:39,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  55%|    | 60/110 [00:47<00:39,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  55%|    | 61/110 [00:47<00:38,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  56%|    | 62/110 [00:48<00:37,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  57%|    | 63/110 [00:49<00:36,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  58%|    | 64/110 [00:50<00:36,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  59%|    | 65/110 [00:51<00:35,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  60%|    | 66/110 [00:51<00:34,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  61%|    | 67/110 [00:52<00:33,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  62%|   | 68/110 [00:53<00:33,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  63%|   | 69/110 [00:54<00:32,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  64%|   | 70/110 [00:54<00:31,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  65%|   | 71/110 [00:55<00:30,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  65%|   | 72/110 [00:56<00:29,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  66%|   | 73/110 [00:57<00:28,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  67%|   | 74/110 [00:58<00:28,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  68%|   | 75/110 [00:58<00:27,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  69%|   | 76/110 [00:59<00:26,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  70%|   | 77/110 [01:00<00:25,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  71%|   | 78/110 [01:01<00:25,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  72%|  | 79/110 [01:02<00:24,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  73%|  | 80/110 [01:02<00:23,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  74%|  | 81/110 [01:03<00:22,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  75%|  | 82/110 [01:04<00:22,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  75%|  | 83/110 [01:05<00:21,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  76%|  | 84/110 [01:05<00:20,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  77%|  | 85/110 [01:06<00:19,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  78%|  | 86/110 [01:07<00:18,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  79%|  | 87/110 [01:08<00:18,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  80%|  | 88/110 [01:09<00:17,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  81%|  | 89/110 [01:09<00:16,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  82%| | 90/110 [01:10<00:15,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  83%| | 91/110 [01:11<00:14,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  84%| | 92/110 [01:12<00:14,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  85%| | 93/110 [01:13<00:13,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  85%| | 94/110 [01:13<00:12,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  86%| | 95/110 [01:14<00:11,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  87%| | 96/110 [01:15<00:10,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  88%| | 97/110 [01:16<00:10,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  89%| | 98/110 [01:16<00:09,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  90%| | 99/110 [01:17<00:08,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  91%| | 100/110 [01:18<00:07,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  92%|| 101/110 [01:19<00:07,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  93%|| 102/110 [01:20<00:06,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  94%|| 103/110 [01:20<00:05,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  95%|| 104/110 [01:21<00:04,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  95%|| 105/110 [01:22<00:03,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  96%|| 106/110 [01:23<00:03,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  97%|| 107/110 [01:24<00:02,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  98%|| 108/110 [01:24<00:01,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  99%|| 109/110 [01:25<00:00,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress: 100%|| 110/110 [01:26<00:00,  1.27it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n",
            "[Epoch 1]\n",
            "  [Training] Loss: 0.6787, Accuracy: 58.35%\n",
            "  [Validation] Loss: 0.6213, Accuracy: 68.98%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Epochs:   4%|         | 1/25 [19:53<7:57:33, 1193.88s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [INFO] Validation loss improved. Model saved to /content/drive/MyDrive/FALL2024/comp550/final_project/models/best_model_2024-12-16_03-02-41.pth.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   0%|          | 0/440 [00:00<?, ?it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   0%|          | 1/440 [00:02<18:40,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   0%|          | 2/440 [00:05<18:19,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   1%|          | 3/440 [00:07<18:24,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   1%|          | 4/440 [00:10<18:35,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   1%|          | 5/440 [00:12<18:50,  2.60s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   1%|         | 6/440 [00:15<19:19,  2.67s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   2%|         | 7/440 [00:18<18:59,  2.63s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   2%|         | 8/440 [00:20<19:05,  2.65s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   2%|         | 9/440 [00:23<18:52,  2.63s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   2%|         | 10/440 [00:26<18:38,  2.60s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   2%|         | 11/440 [00:28<18:23,  2.57s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   3%|         | 12/440 [00:31<18:12,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   3%|         | 13/440 [00:33<18:08,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   3%|         | 14/440 [00:36<17:59,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   3%|         | 15/440 [00:38<17:54,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   4%|         | 16/440 [00:41<17:46,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   4%|         | 17/440 [00:43<17:40,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   4%|         | 18/440 [00:46<17:38,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   4%|         | 19/440 [00:48<17:38,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   5%|         | 20/440 [00:51<17:36,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   5%|         | 21/440 [00:53<17:27,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   5%|         | 22/440 [00:56<17:23,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   5%|         | 23/440 [00:58<17:19,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   5%|         | 24/440 [01:01<18:01,  2.60s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   6%|         | 25/440 [01:03<17:51,  2.58s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   6%|         | 26/440 [01:06<17:37,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   6%|         | 27/440 [01:08<17:24,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   6%|         | 28/440 [01:11<17:15,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   7%|         | 29/440 [01:13<17:10,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   7%|         | 30/440 [01:16<17:09,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   7%|         | 31/440 [01:18<17:05,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   7%|         | 32/440 [01:21<16:59,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   8%|         | 33/440 [01:23<16:55,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   8%|         | 34/440 [01:26<16:51,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   8%|         | 35/440 [01:28<16:52,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   8%|         | 36/440 [01:31<16:49,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   8%|         | 37/440 [01:33<16:47,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   9%|         | 38/440 [01:36<16:41,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   9%|         | 39/440 [01:38<16:37,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   9%|         | 40/440 [01:41<16:38,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   9%|         | 41/440 [01:43<16:37,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  10%|         | 42/440 [01:46<16:33,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  10%|         | 43/440 [01:48<16:33,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  10%|         | 44/440 [01:51<16:29,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  10%|         | 45/440 [01:53<16:31,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  10%|         | 46/440 [01:56<16:25,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  11%|         | 47/440 [01:58<16:21,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  11%|         | 48/440 [02:01<16:16,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  11%|         | 49/440 [02:03<16:15,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  11%|        | 50/440 [02:06<16:14,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  12%|        | 51/440 [02:08<16:10,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  12%|        | 52/440 [02:11<16:06,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  12%|        | 53/440 [02:13<16:02,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  12%|        | 54/440 [02:16<16:01,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  12%|        | 55/440 [02:18<16:02,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  13%|        | 56/440 [02:21<16:01,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  13%|        | 57/440 [02:23<15:58,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  13%|        | 58/440 [02:26<15:52,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  13%|        | 59/440 [02:28<15:51,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  14%|        | 60/440 [02:31<15:46,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  14%|        | 61/440 [02:33<15:45,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  14%|        | 62/440 [02:36<15:41,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  14%|        | 63/440 [02:38<15:37,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  15%|        | 64/440 [02:41<15:35,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  15%|        | 65/440 [02:44<16:08,  2.58s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  15%|        | 66/440 [02:46<15:54,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  15%|        | 67/440 [02:49<15:46,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  15%|        | 68/440 [02:51<15:40,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  16%|        | 69/440 [02:54<15:39,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  16%|        | 70/440 [02:56<15:31,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  16%|        | 71/440 [02:59<15:25,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  16%|        | 72/440 [03:01<15:21,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  17%|        | 73/440 [03:04<15:19,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  17%|        | 74/440 [03:06<15:20,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  17%|        | 75/440 [03:09<15:15,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  17%|        | 76/440 [03:11<15:09,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  18%|        | 77/440 [03:14<15:05,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  18%|        | 78/440 [03:16<15:01,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  18%|        | 79/440 [03:19<15:04,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  18%|        | 80/440 [03:21<14:59,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  18%|        | 81/440 [03:24<14:59,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  19%|        | 82/440 [03:26<14:58,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  19%|        | 83/440 [03:29<14:54,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  19%|        | 84/440 [03:31<14:58,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  19%|        | 85/440 [03:34<14:57,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  20%|        | 86/440 [03:36<14:51,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  20%|        | 87/440 [03:39<14:47,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  20%|        | 88/440 [03:41<14:40,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  20%|        | 89/440 [03:44<14:45,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  20%|        | 90/440 [03:46<14:41,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  21%|        | 91/440 [03:49<14:40,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  21%|        | 92/440 [03:51<14:34,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  21%|        | 93/440 [03:54<14:34,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  21%|       | 94/440 [03:56<14:31,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  22%|       | 95/440 [03:59<14:27,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  22%|       | 96/440 [04:01<14:26,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  22%|       | 97/440 [04:04<14:21,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  22%|       | 98/440 [04:06<14:20,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  22%|       | 99/440 [04:09<14:16,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  23%|       | 100/440 [04:11<14:13,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  23%|       | 101/440 [04:14<14:09,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  23%|       | 102/440 [04:17<14:42,  2.61s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  23%|       | 103/440 [04:19<14:33,  2.59s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  24%|       | 104/440 [04:22<14:19,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  24%|       | 105/440 [04:24<14:11,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  24%|       | 106/440 [04:27<14:03,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  24%|       | 107/440 [04:29<13:56,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  25%|       | 108/440 [04:32<13:55,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  25%|       | 109/440 [04:34<13:49,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  25%|       | 110/440 [04:37<13:45,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  25%|       | 111/440 [04:39<13:47,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  25%|       | 112/440 [04:42<13:40,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  26%|       | 113/440 [04:44<13:39,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  26%|       | 114/440 [04:47<13:35,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  26%|       | 115/440 [04:49<13:35,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  26%|       | 116/440 [04:52<13:31,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  27%|       | 117/440 [04:54<13:29,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  27%|       | 118/440 [04:57<13:28,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  27%|       | 119/440 [04:59<13:26,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  27%|       | 120/440 [05:02<13:23,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  28%|       | 121/440 [05:04<13:18,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  28%|       | 122/440 [05:07<13:16,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  28%|       | 123/440 [05:09<13:16,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  28%|       | 124/440 [05:12<13:12,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  28%|       | 125/440 [05:14<13:07,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  29%|       | 126/440 [05:17<13:04,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  29%|       | 127/440 [05:19<13:03,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  29%|       | 128/440 [05:22<13:02,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  29%|       | 129/440 [05:24<12:59,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  30%|       | 130/440 [05:27<12:56,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  30%|       | 131/440 [05:29<12:51,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  30%|       | 132/440 [05:32<12:51,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  30%|       | 133/440 [05:34<12:51,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  30%|       | 134/440 [05:37<12:46,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  31%|       | 135/440 [05:39<12:43,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  31%|       | 136/440 [05:42<12:41,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  31%|       | 137/440 [05:44<12:39,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  31%|      | 138/440 [05:47<12:34,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  32%|      | 139/440 [05:49<12:33,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  32%|      | 140/440 [05:52<13:01,  2.61s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  32%|      | 141/440 [05:55<12:48,  2.57s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  32%|      | 142/440 [05:57<12:42,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  32%|      | 143/440 [06:00<12:35,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  33%|      | 144/440 [06:02<12:28,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  33%|      | 145/440 [06:05<12:21,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  33%|      | 146/440 [06:07<12:20,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  33%|      | 147/440 [06:10<12:18,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  34%|      | 148/440 [06:12<12:13,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  34%|      | 149/440 [06:15<12:08,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  34%|      | 150/440 [06:17<12:04,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  34%|      | 151/440 [06:20<12:02,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  35%|      | 152/440 [06:22<12:01,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  35%|      | 153/440 [06:25<11:56,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  35%|      | 154/440 [06:27<11:52,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  35%|      | 155/440 [06:30<11:49,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  35%|      | 156/440 [06:32<11:44,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  36%|      | 157/440 [06:35<11:45,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  36%|      | 158/440 [06:37<11:41,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  36%|      | 159/440 [06:40<11:38,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  36%|      | 160/440 [06:42<11:37,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  37%|      | 161/440 [06:45<11:37,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  37%|      | 162/440 [06:47<11:36,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  37%|      | 163/440 [06:50<11:32,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  37%|      | 164/440 [06:52<11:27,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  38%|      | 165/440 [06:55<11:25,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  38%|      | 166/440 [06:57<11:23,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  38%|      | 167/440 [07:00<11:21,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  38%|      | 168/440 [07:02<11:19,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  38%|      | 169/440 [07:05<11:15,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  39%|      | 170/440 [07:07<11:12,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  39%|      | 171/440 [07:10<11:09,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  39%|      | 172/440 [07:12<11:08,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  39%|      | 173/440 [07:15<11:05,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  40%|      | 174/440 [07:17<11:01,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  40%|      | 175/440 [07:20<11:01,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  40%|      | 176/440 [07:22<11:28,  2.61s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  40%|      | 177/440 [07:25<11:17,  2.58s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  40%|      | 178/440 [07:27<11:06,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  41%|      | 179/440 [07:30<10:59,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  41%|      | 180/440 [07:32<10:54,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  41%|      | 181/440 [07:35<10:52,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  41%|     | 182/440 [07:37<10:46,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  42%|     | 183/440 [07:40<10:41,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  42%|     | 184/440 [07:42<10:37,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  42%|     | 185/440 [07:45<10:36,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  42%|     | 186/440 [07:47<10:32,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  42%|     | 187/440 [07:50<10:30,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  43%|     | 188/440 [07:52<10:27,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  43%|     | 189/440 [07:55<10:25,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  43%|     | 190/440 [07:57<10:23,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  43%|     | 191/440 [08:00<10:21,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  44%|     | 192/440 [08:02<10:17,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  44%|     | 193/440 [08:05<10:13,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  44%|     | 194/440 [08:07<10:10,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  44%|     | 195/440 [08:10<10:06,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  45%|     | 196/440 [08:12<10:05,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  45%|     | 197/440 [08:15<10:03,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  45%|     | 198/440 [08:17<10:01,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  45%|     | 199/440 [08:20<09:58,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  45%|     | 200/440 [08:22<09:56,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  46%|     | 201/440 [08:25<09:55,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  46%|     | 202/440 [08:27<09:54,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  46%|     | 203/440 [08:30<09:50,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  46%|     | 204/440 [08:32<09:47,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  47%|     | 205/440 [08:35<09:44,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  47%|     | 206/440 [08:37<09:43,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  47%|     | 207/440 [08:40<09:38,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  47%|     | 208/440 [08:42<09:36,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  48%|     | 209/440 [08:45<09:33,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  48%|     | 210/440 [08:47<09:30,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  48%|     | 211/440 [08:49<09:29,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  48%|     | 212/440 [08:52<09:26,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  48%|     | 213/440 [08:54<09:25,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  49%|     | 214/440 [08:57<09:22,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  49%|     | 215/440 [08:59<09:19,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  49%|     | 216/440 [09:02<09:23,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  49%|     | 217/440 [09:05<09:40,  2.60s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  50%|     | 218/440 [09:07<09:30,  2.57s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  50%|     | 219/440 [09:10<09:22,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  50%|     | 220/440 [09:12<09:16,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  50%|     | 221/440 [09:15<09:11,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  50%|     | 222/440 [09:17<09:06,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  51%|     | 223/440 [09:20<09:03,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  51%|     | 224/440 [09:22<08:59,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  51%|     | 225/440 [09:25<08:57,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  51%|    | 226/440 [09:27<08:56,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  52%|    | 227/440 [09:30<08:53,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  52%|    | 228/440 [09:32<08:49,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  52%|    | 229/440 [09:35<08:45,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  52%|    | 230/440 [09:37<08:43,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  52%|    | 231/440 [09:40<08:39,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  53%|    | 232/440 [09:42<08:36,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  53%|    | 233/440 [09:45<08:33,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  53%|    | 234/440 [09:47<08:31,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  53%|    | 235/440 [09:50<08:31,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  54%|    | 236/440 [09:52<08:27,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  54%|    | 237/440 [09:55<08:25,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  54%|    | 238/440 [09:57<08:23,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  54%|    | 239/440 [10:00<08:20,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  55%|    | 240/440 [10:02<08:17,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  55%|    | 241/440 [10:05<08:15,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  55%|    | 242/440 [10:07<08:13,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  55%|    | 243/440 [10:10<08:10,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  55%|    | 244/440 [10:12<08:08,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  56%|    | 245/440 [10:15<08:07,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  56%|    | 246/440 [10:17<08:04,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  56%|    | 247/440 [10:20<08:01,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  56%|    | 248/440 [10:22<07:59,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  57%|    | 249/440 [10:25<07:56,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  57%|    | 250/440 [10:27<07:54,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  57%|    | 251/440 [10:30<07:51,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  57%|    | 252/440 [10:32<07:48,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  57%|    | 253/440 [10:35<07:45,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  58%|    | 254/440 [10:37<08:03,  2.60s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  58%|    | 255/440 [10:40<07:56,  2.58s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  58%|    | 256/440 [10:42<07:49,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  58%|    | 257/440 [10:45<07:44,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  59%|    | 258/440 [10:47<07:40,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  59%|    | 259/440 [10:50<07:37,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  59%|    | 260/440 [10:52<07:34,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  59%|    | 261/440 [10:55<07:31,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  60%|    | 262/440 [10:57<07:27,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  60%|    | 263/440 [11:00<07:23,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  60%|    | 264/440 [11:02<07:21,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  60%|    | 265/440 [11:05<07:16,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  60%|    | 266/440 [11:07<07:12,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  61%|    | 267/440 [11:10<07:09,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  61%|    | 268/440 [11:12<07:07,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  61%|    | 269/440 [11:15<07:05,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  61%|   | 270/440 [11:17<07:02,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  62%|   | 271/440 [11:20<07:00,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  62%|   | 272/440 [11:22<06:56,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  62%|   | 273/440 [11:25<06:54,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  62%|   | 274/440 [11:27<06:52,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  62%|   | 275/440 [11:30<06:49,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  63%|   | 276/440 [11:32<06:48,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  63%|   | 277/440 [11:35<06:46,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  63%|   | 278/440 [11:37<06:46,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  63%|   | 279/440 [11:40<06:45,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  64%|   | 280/440 [11:42<06:42,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  64%|   | 281/440 [11:45<06:40,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  64%|   | 282/440 [11:47<06:37,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  64%|   | 283/440 [11:50<06:35,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  65%|   | 284/440 [11:52<06:32,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  65%|   | 285/440 [11:55<06:29,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  65%|   | 286/440 [11:57<06:24,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  65%|   | 287/440 [12:00<06:22,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  65%|   | 288/440 [12:02<06:19,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  66%|   | 289/440 [12:05<06:19,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  66%|   | 290/440 [12:07<06:16,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  66%|   | 291/440 [12:10<06:12,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  66%|   | 292/440 [12:13<06:25,  2.61s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  67%|   | 293/440 [12:15<06:17,  2.57s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  67%|   | 294/440 [12:18<06:12,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  67%|   | 295/440 [12:20<06:07,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  67%|   | 296/440 [12:23<06:03,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  68%|   | 297/440 [12:25<05:58,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  68%|   | 298/440 [12:28<05:54,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  68%|   | 299/440 [12:30<05:52,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  68%|   | 300/440 [12:33<05:48,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  68%|   | 301/440 [12:35<05:45,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  69%|   | 302/440 [12:38<05:42,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  69%|   | 303/440 [12:40<05:40,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  69%|   | 304/440 [12:43<05:39,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  69%|   | 305/440 [12:45<05:36,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  70%|   | 306/440 [12:48<05:34,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  70%|   | 307/440 [12:50<05:31,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  70%|   | 308/440 [12:53<05:29,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  70%|   | 309/440 [12:55<05:27,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  70%|   | 310/440 [12:58<05:24,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  71%|   | 311/440 [13:00<05:21,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  71%|   | 312/440 [13:03<05:18,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  71%|   | 313/440 [13:05<05:17,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  71%|  | 314/440 [13:08<05:15,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  72%|  | 315/440 [13:10<05:13,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  72%|  | 316/440 [13:13<05:09,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  72%|  | 317/440 [13:15<05:08,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  72%|  | 318/440 [13:18<05:06,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  72%|  | 319/440 [13:20<05:02,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  73%|  | 320/440 [13:23<04:58,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  73%|  | 321/440 [13:25<04:55,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  73%|  | 322/440 [13:28<04:53,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  73%|  | 323/440 [13:30<04:51,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  74%|  | 324/440 [13:33<04:49,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  74%|  | 325/440 [13:35<04:46,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  74%|  | 326/440 [13:38<04:43,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  74%|  | 327/440 [13:40<04:40,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  75%|  | 328/440 [13:42<04:38,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  75%|  | 329/440 [13:45<04:35,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  75%|  | 330/440 [13:47<04:33,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  75%|  | 331/440 [13:50<04:42,  2.59s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  75%|  | 332/440 [13:53<04:36,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  76%|  | 333/440 [13:55<04:32,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  76%|  | 334/440 [13:58<04:27,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  76%|  | 335/440 [14:00<04:23,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  76%|  | 336/440 [14:03<04:20,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  77%|  | 337/440 [14:05<04:17,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  77%|  | 338/440 [14:08<04:15,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  77%|  | 339/440 [14:10<04:12,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  77%|  | 340/440 [14:13<04:10,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  78%|  | 341/440 [14:15<04:07,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  78%|  | 342/440 [14:18<04:05,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  78%|  | 343/440 [14:20<04:04,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  78%|  | 344/440 [14:23<04:01,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  78%|  | 345/440 [14:25<03:58,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  79%|  | 346/440 [14:28<03:55,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  79%|  | 347/440 [14:30<03:52,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  79%|  | 348/440 [14:33<03:52,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  79%|  | 349/440 [14:35<03:48,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  80%|  | 350/440 [14:38<03:44,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  80%|  | 351/440 [14:40<03:42,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  80%|  | 352/440 [14:43<03:40,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  80%|  | 353/440 [14:45<03:38,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  80%|  | 354/440 [14:48<03:36,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  81%|  | 355/440 [14:50<03:33,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  81%|  | 356/440 [14:53<03:29,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  81%|  | 357/440 [14:55<03:27,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  81%| | 358/440 [14:58<03:24,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  82%| | 359/440 [15:00<03:21,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  82%| | 360/440 [15:03<03:18,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  82%| | 361/440 [15:05<03:15,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  82%| | 362/440 [15:08<03:13,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  82%| | 363/440 [15:10<03:11,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  83%| | 364/440 [15:13<03:08,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  83%| | 365/440 [15:15<03:05,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  83%| | 366/440 [15:18<03:03,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  83%| | 367/440 [15:20<03:00,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  84%| | 368/440 [15:23<02:58,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  84%| | 369/440 [15:25<02:55,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  84%| | 370/440 [15:28<02:53,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  84%| | 371/440 [15:30<02:58,  2.58s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  85%| | 372/440 [15:33<02:54,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  85%| | 373/440 [15:35<02:49,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  85%| | 374/440 [15:38<02:45,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  85%| | 375/440 [15:40<02:42,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  85%| | 376/440 [15:43<02:39,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  86%| | 377/440 [15:45<02:37,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  86%| | 378/440 [15:48<02:34,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  86%| | 379/440 [15:50<02:31,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  86%| | 380/440 [15:53<02:29,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  87%| | 381/440 [15:55<02:26,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  87%| | 382/440 [15:58<02:24,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  87%| | 383/440 [16:00<02:21,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  87%| | 384/440 [16:03<02:19,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  88%| | 385/440 [16:05<02:16,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  88%| | 386/440 [16:08<02:13,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  88%| | 387/440 [16:10<02:11,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  88%| | 388/440 [16:13<02:08,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  88%| | 389/440 [16:15<02:06,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  89%| | 390/440 [16:17<02:03,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  89%| | 391/440 [16:20<02:01,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  89%| | 392/440 [16:22<01:59,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  89%| | 393/440 [16:25<01:56,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  90%| | 394/440 [16:27<01:54,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  90%| | 395/440 [16:30<01:51,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  90%| | 396/440 [16:32<01:49,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  90%| | 397/440 [16:35<01:47,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  90%| | 398/440 [16:37<01:44,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  91%| | 399/440 [16:40<01:41,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  91%| | 400/440 [16:42<01:39,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  91%| | 401/440 [16:45<01:36,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  91%|| 402/440 [16:47<01:34,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  92%|| 403/440 [16:50<01:31,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  92%|| 404/440 [16:52<01:29,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  92%|| 405/440 [16:55<01:26,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  92%|| 406/440 [16:57<01:24,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  92%|| 407/440 [17:00<01:25,  2.60s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  93%|| 408/440 [17:03<01:21,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  93%|| 409/440 [17:05<01:18,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  93%|| 410/440 [17:07<01:15,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  93%|| 411/440 [17:10<01:12,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  94%|| 412/440 [17:12<01:09,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  94%|| 413/440 [17:15<01:07,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  94%|| 414/440 [17:17<01:04,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  94%|| 415/440 [17:20<01:02,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  95%|| 416/440 [17:22<00:59,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  95%|| 417/440 [17:25<00:57,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  95%|| 418/440 [17:27<00:54,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  95%|| 419/440 [17:30<00:52,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  95%|| 420/440 [17:32<00:49,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  96%|| 421/440 [17:35<00:47,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  96%|| 422/440 [17:37<00:44,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  96%|| 423/440 [17:40<00:42,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  96%|| 424/440 [17:42<00:39,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  97%|| 425/440 [17:45<00:37,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  97%|| 426/440 [17:47<00:34,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  97%|| 427/440 [17:50<00:32,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  97%|| 428/440 [17:52<00:29,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  98%|| 429/440 [17:55<00:27,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  98%|| 430/440 [17:57<00:24,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  98%|| 431/440 [18:00<00:22,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  98%|| 432/440 [18:02<00:19,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  98%|| 433/440 [18:05<00:17,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  99%|| 434/440 [18:07<00:14,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  99%|| 435/440 [18:09<00:12,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  99%|| 436/440 [18:12<00:09,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  99%|| 437/440 [18:14<00:07,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress: 100%|| 438/440 [18:17<00:04,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress: 100%|| 439/440 [18:19<00:02,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress: 100%|| 440/440 [18:22<00:00,  2.51s/it]\n",
            "\n",
            "Validation Progress:   0%|          | 0/110 [00:00<?, ?it/s]\u001b[A\n",
            "Validation Progress:   1%|          | 1/110 [00:00<01:24,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   2%|         | 2/110 [00:01<01:24,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   3%|         | 3/110 [00:02<01:23,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   4%|         | 4/110 [00:03<01:22,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   5%|         | 5/110 [00:03<01:22,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   5%|         | 6/110 [00:04<01:20,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   6%|         | 7/110 [00:05<01:20,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   7%|         | 8/110 [00:06<01:19,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   8%|         | 9/110 [00:07<01:18,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   9%|         | 10/110 [00:07<01:17,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  10%|         | 11/110 [00:08<01:17,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  11%|         | 12/110 [00:09<01:16,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  12%|        | 13/110 [00:10<01:15,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  13%|        | 14/110 [00:10<01:14,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  14%|        | 15/110 [00:11<01:13,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  15%|        | 16/110 [00:12<01:13,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  15%|        | 17/110 [00:13<01:12,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  16%|        | 18/110 [00:14<01:12,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  17%|        | 19/110 [00:14<01:11,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  18%|        | 20/110 [00:15<01:10,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  19%|        | 21/110 [00:16<01:09,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  20%|        | 22/110 [00:17<01:08,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  21%|        | 23/110 [00:17<01:07,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  22%|       | 24/110 [00:18<01:07,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  23%|       | 25/110 [00:19<01:06,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  24%|       | 26/110 [00:20<01:05,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  25%|       | 27/110 [00:21<01:04,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  25%|       | 28/110 [00:21<01:03,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  26%|       | 29/110 [00:22<01:03,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  27%|       | 30/110 [00:23<01:02,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  28%|       | 31/110 [00:24<01:01,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  29%|       | 32/110 [00:24<01:00,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  30%|       | 33/110 [00:25<01:00,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  31%|       | 34/110 [00:26<00:59,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  32%|      | 35/110 [00:27<00:58,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  33%|      | 36/110 [00:28<00:57,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  34%|      | 37/110 [00:28<00:56,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  35%|      | 38/110 [00:29<00:56,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  35%|      | 39/110 [00:30<00:55,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  36%|      | 40/110 [00:31<00:54,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  37%|      | 41/110 [00:31<00:53,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  38%|      | 42/110 [00:32<00:52,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  39%|      | 43/110 [00:33<00:52,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  40%|      | 44/110 [00:34<00:51,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  41%|      | 45/110 [00:35<00:50,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  42%|     | 46/110 [00:35<00:49,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  43%|     | 47/110 [00:36<00:49,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  44%|     | 48/110 [00:37<00:48,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  45%|     | 49/110 [00:38<00:48,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  45%|     | 50/110 [00:39<00:47,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  46%|     | 51/110 [00:39<00:46,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  47%|     | 52/110 [00:40<00:45,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  48%|     | 53/110 [00:41<00:44,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  49%|     | 54/110 [00:42<00:43,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  50%|     | 55/110 [00:42<00:42,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  51%|     | 56/110 [00:43<00:42,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  52%|    | 57/110 [00:44<00:41,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  53%|    | 58/110 [00:45<00:40,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  54%|    | 59/110 [00:46<00:39,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  55%|    | 60/110 [00:46<00:39,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  55%|    | 61/110 [00:47<00:38,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  56%|    | 62/110 [00:48<00:37,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  57%|    | 63/110 [00:49<00:36,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  58%|    | 64/110 [00:49<00:36,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  59%|    | 65/110 [00:50<00:35,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  60%|    | 66/110 [00:51<00:34,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  61%|    | 67/110 [00:52<00:33,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  62%|   | 68/110 [00:53<00:32,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  63%|   | 69/110 [00:53<00:32,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  64%|   | 70/110 [00:54<00:31,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  65%|   | 71/110 [00:55<00:30,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  65%|   | 72/110 [00:56<00:29,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  66%|   | 73/110 [00:57<00:28,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  67%|   | 74/110 [00:57<00:28,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  68%|   | 75/110 [00:58<00:27,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  69%|   | 76/110 [00:59<00:26,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  70%|   | 77/110 [01:00<00:25,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  71%|   | 78/110 [01:00<00:25,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  72%|  | 79/110 [01:01<00:24,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  73%|  | 80/110 [01:02<00:23,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  74%|  | 81/110 [01:03<00:22,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  75%|  | 82/110 [01:04<00:21,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  75%|  | 83/110 [01:04<00:21,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  76%|  | 84/110 [01:05<00:20,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  77%|  | 85/110 [01:06<00:19,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  78%|  | 86/110 [01:07<00:18,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  79%|  | 87/110 [01:08<00:18,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  80%|  | 88/110 [01:08<00:17,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  81%|  | 89/110 [01:09<00:16,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  82%| | 90/110 [01:10<00:15,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  83%| | 91/110 [01:11<00:14,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  84%| | 92/110 [01:11<00:14,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  85%| | 93/110 [01:12<00:13,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  85%| | 94/110 [01:13<00:12,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  86%| | 95/110 [01:14<00:11,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  87%| | 96/110 [01:15<00:10,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  88%| | 97/110 [01:15<00:10,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  89%| | 98/110 [01:16<00:09,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  90%| | 99/110 [01:17<00:08,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  91%| | 100/110 [01:18<00:07,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  92%|| 101/110 [01:18<00:06,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  93%|| 102/110 [01:19<00:06,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  94%|| 103/110 [01:20<00:05,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  95%|| 104/110 [01:21<00:04,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  95%|| 105/110 [01:22<00:03,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  96%|| 106/110 [01:22<00:03,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  97%|| 107/110 [01:23<00:02,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  98%|| 108/110 [01:24<00:01,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  99%|| 109/110 [01:25<00:00,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress: 100%|| 110/110 [01:25<00:00,  1.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n",
            "[Epoch 2]\n",
            "  [Training] Loss: 0.5982, Accuracy: 70.91%\n",
            "  [Validation] Loss: 0.5831, Accuracy: 70.45%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Epochs:   8%|         | 2/25 [39:44<7:37:01, 1192.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [INFO] Validation loss improved. Model saved to /content/drive/MyDrive/FALL2024/comp550/final_project/models/best_model_2024-12-16_03-02-41.pth.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   0%|          | 0/440 [00:00<?, ?it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   0%|          | 1/440 [00:02<18:37,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   0%|          | 2/440 [00:05<18:14,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   1%|          | 3/440 [00:07<18:06,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   1%|          | 4/440 [00:10<18:21,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   1%|          | 5/440 [00:12<18:41,  2.58s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   1%|         | 6/440 [00:15<18:38,  2.58s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   2%|         | 7/440 [00:18<19:25,  2.69s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   2%|         | 8/440 [00:20<19:11,  2.66s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   2%|         | 9/440 [00:23<18:58,  2.64s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   2%|         | 10/440 [00:25<18:35,  2.59s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   2%|         | 11/440 [00:28<18:23,  2.57s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   3%|         | 12/440 [00:30<18:11,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   3%|         | 13/440 [00:33<18:04,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   3%|         | 14/440 [00:35<17:56,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   3%|         | 15/440 [00:38<17:49,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   4%|         | 16/440 [00:40<17:48,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   4%|         | 17/440 [00:43<17:41,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   4%|         | 18/440 [00:45<17:37,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   4%|         | 19/440 [00:48<17:31,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   5%|         | 20/440 [00:50<17:31,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   5%|         | 21/440 [00:53<17:27,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   5%|         | 22/440 [00:55<17:24,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   5%|         | 23/440 [00:58<17:22,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   5%|         | 24/440 [01:00<17:19,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   6%|         | 25/440 [01:03<17:21,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   6%|         | 26/440 [01:05<17:17,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   6%|         | 27/440 [01:08<17:10,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   6%|         | 28/440 [01:10<17:04,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   7%|         | 29/440 [01:13<17:05,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   7%|         | 30/440 [01:15<17:00,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   7%|         | 31/440 [01:18<16:55,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   7%|         | 32/440 [01:20<16:51,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   8%|         | 33/440 [01:23<16:52,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   8%|         | 34/440 [01:25<16:52,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   8%|         | 35/440 [01:28<16:49,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   8%|         | 36/440 [01:30<16:43,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   8%|         | 37/440 [01:33<16:40,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   9%|         | 38/440 [01:35<16:37,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   9%|         | 39/440 [01:38<16:35,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   9%|         | 40/440 [01:40<16:31,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   9%|         | 41/440 [01:43<16:27,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  10%|         | 42/440 [01:45<16:25,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  10%|         | 43/440 [01:48<16:22,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  10%|         | 44/440 [01:50<17:03,  2.59s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  10%|         | 45/440 [01:53<16:47,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  10%|         | 46/440 [01:55<16:36,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  11%|         | 47/440 [01:58<16:28,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  11%|         | 48/440 [02:00<16:25,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  11%|         | 49/440 [02:03<16:22,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  11%|        | 50/440 [02:05<16:18,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  12%|        | 51/440 [02:08<16:09,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  12%|        | 52/440 [02:10<16:06,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  12%|        | 53/440 [02:13<16:06,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  12%|        | 54/440 [02:15<16:01,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  12%|        | 55/440 [02:18<15:58,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  13%|        | 56/440 [02:20<15:53,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  13%|        | 57/440 [02:23<15:50,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  13%|        | 58/440 [02:25<15:53,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  13%|        | 59/440 [02:28<15:53,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  14%|        | 60/440 [02:30<15:51,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  14%|        | 61/440 [02:33<15:45,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  14%|        | 62/440 [02:35<15:38,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  14%|        | 63/440 [02:38<15:38,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  15%|        | 64/440 [02:40<15:35,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  15%|        | 65/440 [02:43<15:32,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  15%|        | 66/440 [02:45<15:30,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  15%|        | 67/440 [02:48<15:26,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  15%|        | 68/440 [02:50<15:21,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  16%|        | 69/440 [02:53<15:17,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  16%|        | 70/440 [02:55<15:15,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  16%|        | 71/440 [02:58<15:14,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  16%|        | 72/440 [03:00<15:11,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  17%|        | 73/440 [03:03<15:09,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  17%|        | 74/440 [03:05<15:06,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  17%|        | 75/440 [03:08<15:04,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  17%|        | 76/440 [03:10<15:03,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  18%|        | 77/440 [03:13<15:03,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  18%|        | 78/440 [03:15<15:04,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  18%|        | 79/440 [03:18<14:57,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  18%|        | 80/440 [03:20<14:52,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  18%|        | 81/440 [03:22<14:49,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  19%|        | 82/440 [03:25<14:47,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  19%|        | 83/440 [03:27<14:49,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  19%|        | 84/440 [03:30<15:22,  2.59s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  19%|        | 85/440 [03:33<15:09,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  20%|        | 86/440 [03:35<14:58,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  20%|        | 87/440 [03:38<14:50,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  20%|        | 88/440 [03:40<14:45,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  20%|        | 89/440 [03:43<14:39,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  20%|        | 90/440 [03:45<14:34,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  21%|        | 91/440 [03:48<14:30,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  21%|        | 92/440 [03:50<14:26,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  21%|        | 93/440 [03:53<14:26,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  21%|       | 94/440 [03:55<14:22,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  22%|       | 95/440 [03:58<14:20,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  22%|       | 96/440 [04:00<14:15,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  22%|       | 97/440 [04:03<14:13,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  22%|       | 98/440 [04:05<14:12,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  22%|       | 99/440 [04:08<14:09,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  23%|       | 100/440 [04:10<14:05,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  23%|       | 101/440 [04:13<14:04,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  23%|       | 102/440 [04:15<14:04,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  23%|       | 103/440 [04:18<14:01,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  24%|       | 104/440 [04:20<13:58,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  24%|       | 105/440 [04:23<13:56,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  24%|       | 106/440 [04:25<13:55,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  24%|       | 107/440 [04:28<13:55,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  25%|       | 108/440 [04:30<13:51,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  25%|       | 109/440 [04:33<13:48,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  25%|       | 110/440 [04:35<13:44,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  25%|       | 111/440 [04:38<13:40,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  25%|       | 112/440 [04:40<13:39,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  26%|       | 113/440 [04:43<13:38,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  26%|       | 114/440 [04:45<13:37,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  26%|       | 115/440 [04:48<13:33,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  26%|       | 116/440 [04:50<13:30,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  27%|       | 117/440 [04:53<13:27,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  27%|       | 118/440 [04:55<13:25,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  27%|       | 119/440 [04:58<13:22,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  27%|       | 120/440 [05:00<13:18,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  28%|       | 121/440 [05:03<13:15,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  28%|       | 122/440 [05:05<13:11,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  28%|       | 123/440 [05:08<13:08,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  28%|       | 124/440 [05:10<13:37,  2.59s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  28%|       | 125/440 [05:13<13:23,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  29%|       | 126/440 [05:15<13:12,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  29%|       | 127/440 [05:18<13:07,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  29%|       | 128/440 [05:20<13:02,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  29%|       | 129/440 [05:23<12:59,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  30%|       | 130/440 [05:25<12:56,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  30%|       | 131/440 [05:28<12:53,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  30%|       | 132/440 [05:30<12:53,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  30%|       | 133/440 [05:33<12:49,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  30%|       | 134/440 [05:35<12:43,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  31%|       | 135/440 [05:38<12:39,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  31%|       | 136/440 [05:40<12:37,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  31%|       | 137/440 [05:43<12:35,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  31%|      | 138/440 [05:45<12:35,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  32%|      | 139/440 [05:48<12:32,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  32%|      | 140/440 [05:50<12:28,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  32%|      | 141/440 [05:53<12:25,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  32%|      | 142/440 [05:55<12:25,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  32%|      | 143/440 [05:58<12:22,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  33%|      | 144/440 [06:00<12:19,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  33%|      | 145/440 [06:03<12:16,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  33%|      | 146/440 [06:05<12:13,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  33%|      | 147/440 [06:08<12:11,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  34%|      | 148/440 [06:10<12:08,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  34%|      | 149/440 [06:13<12:05,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  34%|      | 150/440 [06:15<12:01,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  34%|      | 151/440 [06:18<12:02,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  35%|      | 152/440 [06:20<11:56,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  35%|      | 153/440 [06:23<11:52,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  35%|      | 154/440 [06:25<11:49,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  35%|      | 155/440 [06:28<11:48,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  35%|      | 156/440 [06:30<11:46,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  36%|      | 157/440 [06:33<11:43,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  36%|      | 158/440 [06:35<11:40,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  36%|      | 159/440 [06:38<11:38,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  36%|      | 160/440 [06:40<11:36,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  37%|      | 161/440 [06:43<11:35,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  37%|      | 162/440 [06:45<12:03,  2.60s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  37%|      | 163/440 [06:48<11:52,  2.57s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  37%|      | 164/440 [06:50<11:43,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  38%|      | 165/440 [06:53<11:36,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  38%|      | 166/440 [06:55<11:31,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  38%|      | 167/440 [06:58<11:28,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  38%|      | 168/440 [07:00<11:22,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  38%|      | 169/440 [07:03<11:19,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  39%|      | 170/440 [07:05<11:16,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  39%|      | 171/440 [07:08<11:15,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  39%|      | 172/440 [07:10<11:09,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  39%|      | 173/440 [07:13<11:07,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  40%|      | 174/440 [07:15<11:03,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  40%|      | 175/440 [07:18<10:59,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  40%|      | 176/440 [07:20<11:00,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  40%|      | 177/440 [07:23<10:57,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  40%|      | 178/440 [07:25<10:55,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  41%|      | 179/440 [07:28<10:52,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  41%|      | 180/440 [07:30<10:48,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  41%|      | 181/440 [07:33<10:47,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  41%|     | 182/440 [07:35<10:44,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  42%|     | 183/440 [07:38<10:39,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  42%|     | 184/440 [07:40<10:38,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  42%|     | 185/440 [07:43<10:35,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  42%|     | 186/440 [07:45<10:34,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  42%|     | 187/440 [07:48<10:33,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  43%|     | 188/440 [07:50<10:30,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  43%|     | 189/440 [07:53<10:25,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  43%|     | 190/440 [07:55<10:25,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  43%|     | 191/440 [07:58<10:22,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  44%|     | 192/440 [08:00<10:19,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  44%|     | 193/440 [08:03<10:16,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  44%|     | 194/440 [08:05<10:12,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  44%|     | 195/440 [08:08<10:11,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  45%|     | 196/440 [08:10<10:08,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  45%|     | 197/440 [08:13<10:04,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  45%|     | 198/440 [08:16<10:26,  2.59s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  45%|     | 199/440 [08:18<10:16,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  45%|     | 200/440 [08:21<10:10,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  46%|     | 201/440 [08:23<10:04,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  46%|     | 202/440 [08:26<09:59,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  46%|     | 203/440 [08:28<09:54,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  46%|     | 204/440 [08:31<09:50,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  47%|     | 205/440 [08:33<09:49,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  47%|     | 206/440 [08:36<09:45,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  47%|     | 207/440 [08:38<09:40,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  47%|     | 208/440 [08:41<09:37,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  48%|     | 209/440 [08:43<09:35,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  48%|     | 210/440 [08:46<09:36,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  48%|     | 211/440 [08:48<09:32,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  48%|     | 212/440 [08:51<09:28,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  48%|     | 213/440 [08:53<09:25,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  49%|     | 214/440 [08:56<09:21,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  49%|     | 215/440 [08:58<09:21,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  49%|     | 216/440 [09:01<09:18,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  49%|     | 217/440 [09:03<09:15,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  50%|     | 218/440 [09:05<09:11,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  50%|     | 219/440 [09:08<09:09,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  50%|     | 220/440 [09:11<09:11,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  50%|     | 221/440 [09:13<09:07,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  50%|     | 222/440 [09:15<09:05,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  51%|     | 223/440 [09:18<09:02,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  51%|     | 224/440 [09:21<09:00,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  51%|     | 225/440 [09:23<08:58,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  51%|    | 226/440 [09:26<08:55,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  52%|    | 227/440 [09:28<08:53,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  52%|    | 228/440 [09:30<08:49,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  52%|    | 229/440 [09:33<08:47,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  52%|    | 230/440 [09:35<08:44,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  52%|    | 231/440 [09:38<08:41,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  53%|    | 232/440 [09:40<08:38,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  53%|    | 233/440 [09:43<08:34,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  53%|    | 234/440 [09:45<08:33,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  53%|    | 235/440 [09:48<08:30,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  54%|    | 236/440 [09:50<08:28,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  54%|    | 237/440 [09:53<08:25,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  54%|    | 238/440 [09:55<08:23,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  54%|    | 239/440 [09:58<08:21,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  55%|    | 240/440 [10:01<08:38,  2.59s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  55%|    | 241/440 [10:03<08:29,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  55%|    | 242/440 [10:06<08:21,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  55%|    | 243/440 [10:08<08:16,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  55%|    | 244/440 [10:11<08:13,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  56%|    | 245/440 [10:13<08:08,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  56%|    | 246/440 [10:16<08:05,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  56%|    | 247/440 [10:18<08:02,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  56%|    | 248/440 [10:21<07:58,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  57%|    | 249/440 [10:23<07:56,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  57%|    | 250/440 [10:26<07:53,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  57%|    | 251/440 [10:28<07:51,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  57%|    | 252/440 [10:31<07:47,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  57%|    | 253/440 [10:33<07:46,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  58%|    | 254/440 [10:36<07:46,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  58%|    | 255/440 [10:38<07:41,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  58%|    | 256/440 [10:41<07:37,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  58%|    | 257/440 [10:43<07:35,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  59%|    | 258/440 [10:46<07:33,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  59%|    | 259/440 [10:48<07:32,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  59%|    | 260/440 [10:51<07:29,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  59%|    | 261/440 [10:53<07:26,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  60%|    | 262/440 [10:56<07:23,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  60%|    | 263/440 [10:58<07:20,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  60%|    | 264/440 [11:01<07:18,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  60%|    | 265/440 [11:03<07:16,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  60%|    | 266/440 [11:06<07:13,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  61%|    | 267/440 [11:08<07:10,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  61%|    | 268/440 [11:10<07:07,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  61%|    | 269/440 [11:13<07:04,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  61%|   | 270/440 [11:15<07:01,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  62%|   | 271/440 [11:18<06:59,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  62%|   | 272/440 [11:20<06:57,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  62%|   | 273/440 [11:23<06:54,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  62%|   | 274/440 [11:25<06:51,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  62%|   | 275/440 [11:28<06:49,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  63%|   | 276/440 [11:30<06:46,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  63%|   | 277/440 [11:33<07:01,  2.59s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  63%|   | 278/440 [11:36<06:55,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  63%|   | 279/440 [11:38<06:49,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  64%|   | 280/440 [11:41<06:44,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  64%|   | 281/440 [11:43<06:40,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  64%|   | 282/440 [11:46<06:37,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  64%|   | 283/440 [11:48<06:35,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  65%|   | 284/440 [11:51<06:31,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  65%|   | 285/440 [11:53<06:28,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  65%|   | 286/440 [11:56<06:24,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  65%|   | 287/440 [11:58<06:21,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  65%|   | 288/440 [12:01<06:20,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  66%|   | 289/440 [12:03<06:17,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  66%|   | 290/440 [12:06<06:13,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  66%|   | 291/440 [12:08<06:11,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  66%|   | 292/440 [12:11<06:08,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  67%|   | 293/440 [12:13<06:06,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  67%|   | 294/440 [12:16<06:03,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  67%|   | 295/440 [12:18<06:00,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  67%|   | 296/440 [12:21<05:58,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  68%|   | 297/440 [12:23<05:55,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  68%|   | 298/440 [12:26<05:53,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  68%|   | 299/440 [12:28<05:51,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  68%|   | 300/440 [12:31<05:48,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  68%|   | 301/440 [12:33<05:46,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  69%|   | 302/440 [12:36<05:43,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  69%|   | 303/440 [12:38<05:42,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  69%|   | 304/440 [12:41<05:39,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  69%|   | 305/440 [12:43<05:37,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  70%|   | 306/440 [12:46<05:35,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  70%|   | 307/440 [12:48<05:32,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  70%|   | 308/440 [12:51<05:30,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  70%|   | 309/440 [12:53<05:27,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  70%|   | 310/440 [12:56<05:24,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  71%|   | 311/440 [12:58<05:22,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  71%|   | 312/440 [13:01<05:19,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  71%|   | 313/440 [13:03<05:19,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  71%|  | 314/440 [13:06<05:15,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  72%|  | 315/440 [13:08<05:25,  2.60s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  72%|  | 316/440 [13:11<05:18,  2.57s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  72%|  | 317/440 [13:13<05:13,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  72%|  | 318/440 [13:16<05:07,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  72%|  | 319/440 [13:18<05:03,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  73%|  | 320/440 [13:21<04:58,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  73%|  | 321/440 [13:23<04:56,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  73%|  | 322/440 [13:26<04:53,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  73%|  | 323/440 [13:28<04:51,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  74%|  | 324/440 [13:31<04:49,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  74%|  | 325/440 [13:33<04:46,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  74%|  | 326/440 [13:36<04:42,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  74%|  | 327/440 [13:38<04:40,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  75%|  | 328/440 [13:41<04:37,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  75%|  | 329/440 [13:43<04:35,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  75%|  | 330/440 [13:46<04:33,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  75%|  | 331/440 [13:48<04:30,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  75%|  | 332/440 [13:51<04:28,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  76%|  | 333/440 [13:53<04:25,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  76%|  | 334/440 [13:56<04:23,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  76%|  | 335/440 [13:58<04:20,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  76%|  | 336/440 [14:01<04:18,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  77%|  | 337/440 [14:03<04:17,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  77%|  | 338/440 [14:06<04:14,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  77%|  | 339/440 [14:08<04:12,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  77%|  | 340/440 [14:11<04:09,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  78%|  | 341/440 [14:13<04:07,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  78%|  | 342/440 [14:16<04:05,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  78%|  | 343/440 [14:18<04:02,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  78%|  | 344/440 [14:21<03:59,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  78%|  | 345/440 [14:23<03:56,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  79%|  | 346/440 [14:25<03:53,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  79%|  | 347/440 [14:28<03:53,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  79%|  | 348/440 [14:31<03:50,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  79%|  | 349/440 [14:33<03:46,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  80%|  | 350/440 [14:35<03:43,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  80%|  | 351/440 [14:38<03:49,  2.58s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  80%|  | 352/440 [14:41<03:45,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  80%|  | 353/440 [14:43<03:40,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  80%|  | 354/440 [14:46<03:36,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  81%|  | 355/440 [14:48<03:33,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  81%|  | 356/440 [14:51<03:29,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  81%|  | 357/440 [14:53<03:27,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  81%| | 358/440 [14:56<03:24,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  82%| | 359/440 [14:58<03:21,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  82%| | 360/440 [15:01<03:18,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  82%| | 361/440 [15:03<03:16,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  82%| | 362/440 [15:06<03:13,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  82%| | 363/440 [15:08<03:11,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  83%| | 364/440 [15:11<03:08,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  83%| | 365/440 [15:13<03:06,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  83%| | 366/440 [15:16<03:04,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  83%| | 367/440 [15:18<03:02,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  84%| | 368/440 [15:21<02:58,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  84%| | 369/440 [15:23<02:56,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  84%| | 370/440 [15:26<02:53,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  84%| | 371/440 [15:28<02:52,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  85%| | 372/440 [15:30<02:49,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  85%| | 373/440 [15:33<02:46,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  85%| | 374/440 [15:35<02:43,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  85%| | 375/440 [15:38<02:41,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  85%| | 376/440 [15:40<02:39,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  86%| | 377/440 [15:43<02:37,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  86%| | 378/440 [15:45<02:34,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  86%| | 379/440 [15:48<02:32,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  86%| | 380/440 [15:50<02:29,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  87%| | 381/440 [15:53<02:27,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  87%| | 382/440 [15:55<02:24,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  87%| | 383/440 [15:58<02:21,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  87%| | 384/440 [16:00<02:18,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  88%| | 385/440 [16:03<02:17,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  88%| | 386/440 [16:05<02:14,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  88%| | 387/440 [16:08<02:11,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  88%| | 388/440 [16:10<02:09,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  88%| | 389/440 [16:13<02:06,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  89%| | 390/440 [16:15<02:03,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  89%| | 391/440 [16:18<02:01,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  89%| | 392/440 [16:21<02:04,  2.59s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  89%| | 393/440 [16:23<01:59,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  90%| | 394/440 [16:25<01:56,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  90%| | 395/440 [16:28<01:53,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  90%| | 396/440 [16:30<01:50,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  90%| | 397/440 [16:33<01:47,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  90%| | 398/440 [16:35<01:44,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  91%| | 399/440 [16:38<01:42,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  91%| | 400/440 [16:40<01:39,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  91%| | 401/440 [16:43<01:37,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  91%|| 402/440 [16:45<01:34,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  92%|| 403/440 [16:48<01:32,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  92%|| 404/440 [16:50<01:29,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  92%|| 405/440 [16:53<01:27,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  92%|| 406/440 [16:55<01:24,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  92%|| 407/440 [16:58<01:22,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  93%|| 408/440 [17:00<01:20,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  93%|| 409/440 [17:03<01:17,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  93%|| 410/440 [17:05<01:14,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  93%|| 411/440 [17:08<01:12,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  94%|| 412/440 [17:10<01:09,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  94%|| 413/440 [17:13<01:07,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  94%|| 414/440 [17:15<01:04,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  94%|| 415/440 [17:18<01:02,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  95%|| 416/440 [17:20<00:59,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  95%|| 417/440 [17:23<00:57,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  95%|| 418/440 [17:25<00:54,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  95%|| 419/440 [17:28<00:52,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  95%|| 420/440 [17:30<00:50,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  96%|| 421/440 [17:33<00:47,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  96%|| 422/440 [17:35<00:44,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  96%|| 423/440 [17:38<00:42,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  96%|| 424/440 [17:40<00:39,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  97%|| 425/440 [17:43<00:37,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  97%|| 426/440 [17:45<00:34,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  97%|| 427/440 [17:48<00:32,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  97%|| 428/440 [17:50<00:29,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  98%|| 429/440 [17:53<00:28,  2.58s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  98%|| 430/440 [17:55<00:25,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  98%|| 431/440 [17:58<00:22,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  98%|| 432/440 [18:00<00:20,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  98%|| 433/440 [18:03<00:17,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  99%|| 434/440 [18:05<00:14,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  99%|| 435/440 [18:08<00:12,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  99%|| 436/440 [18:10<00:09,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  99%|| 437/440 [18:13<00:07,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress: 100%|| 438/440 [18:15<00:04,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress: 100%|| 439/440 [18:18<00:02,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress: 100%|| 440/440 [18:20<00:00,  2.50s/it]\n",
            "\n",
            "Validation Progress:   0%|          | 0/110 [00:00<?, ?it/s]\u001b[A\n",
            "Validation Progress:   1%|          | 1/110 [00:00<01:25,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   2%|         | 2/110 [00:01<01:24,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   3%|         | 3/110 [00:02<01:23,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   4%|         | 4/110 [00:03<01:22,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   5%|         | 5/110 [00:03<01:22,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   5%|         | 6/110 [00:04<01:21,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   6%|         | 7/110 [00:05<01:20,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   7%|         | 8/110 [00:06<01:19,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   8%|         | 9/110 [00:07<01:18,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   9%|         | 10/110 [00:07<01:18,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  10%|         | 11/110 [00:08<01:17,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  11%|         | 12/110 [00:09<01:16,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  12%|        | 13/110 [00:10<01:16,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  13%|        | 14/110 [00:10<01:16,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  14%|        | 15/110 [00:11<01:15,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  15%|        | 16/110 [00:12<01:14,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  15%|        | 17/110 [00:13<01:13,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  16%|        | 18/110 [00:14<01:12,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  17%|        | 19/110 [00:14<01:11,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  18%|        | 20/110 [00:15<01:10,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  19%|        | 21/110 [00:16<01:09,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  20%|        | 22/110 [00:17<01:09,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  21%|        | 23/110 [00:18<01:08,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  22%|       | 24/110 [00:18<01:07,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  23%|       | 25/110 [00:19<01:06,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  24%|       | 26/110 [00:20<01:05,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  25%|       | 27/110 [00:21<01:05,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  25%|       | 28/110 [00:21<01:04,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  26%|       | 29/110 [00:22<01:03,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  27%|       | 30/110 [00:23<01:02,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  28%|       | 31/110 [00:24<01:01,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  29%|       | 32/110 [00:25<01:01,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  30%|       | 33/110 [00:25<01:00,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  31%|       | 34/110 [00:26<00:59,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  32%|      | 35/110 [00:27<00:58,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  33%|      | 36/110 [00:28<00:57,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  34%|      | 37/110 [00:28<00:56,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  35%|      | 38/110 [00:29<00:56,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  35%|      | 39/110 [00:30<00:55,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  36%|      | 40/110 [00:31<00:54,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  37%|      | 41/110 [00:32<00:53,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  38%|      | 42/110 [00:32<00:52,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  39%|      | 43/110 [00:33<00:51,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  40%|      | 44/110 [00:34<00:51,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  41%|      | 45/110 [00:35<00:50,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  42%|     | 46/110 [00:36<00:50,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  43%|     | 47/110 [00:36<00:49,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  44%|     | 48/110 [00:37<00:49,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  45%|     | 49/110 [00:38<00:48,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  45%|     | 50/110 [00:39<00:47,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  46%|     | 51/110 [00:39<00:46,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  47%|     | 52/110 [00:40<00:45,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  48%|     | 53/110 [00:41<00:44,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  49%|     | 54/110 [00:42<00:43,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  50%|     | 55/110 [00:43<00:43,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  51%|     | 56/110 [00:43<00:42,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  52%|    | 57/110 [00:44<00:41,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  53%|    | 58/110 [00:45<00:41,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  54%|    | 59/110 [00:46<00:40,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  55%|    | 60/110 [00:47<00:39,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  55%|    | 61/110 [00:47<00:38,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  56%|    | 62/110 [00:48<00:37,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  57%|    | 63/110 [00:49<00:36,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  58%|    | 64/110 [00:50<00:36,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  59%|    | 65/110 [00:50<00:35,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  60%|    | 66/110 [00:51<00:34,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  61%|    | 67/110 [00:52<00:33,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  62%|   | 68/110 [00:53<00:32,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  63%|   | 69/110 [00:54<00:32,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  64%|   | 70/110 [00:54<00:31,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  65%|   | 71/110 [00:55<00:30,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  65%|   | 72/110 [00:56<00:29,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  66%|   | 73/110 [00:57<00:28,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  67%|   | 74/110 [00:57<00:28,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  68%|   | 75/110 [00:58<00:27,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  69%|   | 76/110 [00:59<00:26,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  70%|   | 77/110 [01:00<00:25,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  71%|   | 78/110 [01:01<00:25,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  72%|  | 79/110 [01:01<00:24,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  73%|  | 80/110 [01:02<00:23,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  74%|  | 81/110 [01:03<00:22,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  75%|  | 82/110 [01:04<00:21,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  75%|  | 83/110 [01:05<00:21,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  76%|  | 84/110 [01:05<00:20,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  77%|  | 85/110 [01:06<00:19,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  78%|  | 86/110 [01:07<00:18,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  79%|  | 87/110 [01:08<00:17,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  80%|  | 88/110 [01:08<00:17,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  81%|  | 89/110 [01:09<00:16,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  82%| | 90/110 [01:10<00:15,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  83%| | 91/110 [01:11<00:14,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  84%| | 92/110 [01:12<00:14,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  85%| | 93/110 [01:12<00:13,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  85%| | 94/110 [01:13<00:12,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  86%| | 95/110 [01:14<00:11,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  87%| | 96/110 [01:15<00:10,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  88%| | 97/110 [01:15<00:10,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  89%| | 98/110 [01:16<00:09,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  90%| | 99/110 [01:17<00:08,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  91%| | 100/110 [01:18<00:07,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  92%|| 101/110 [01:19<00:07,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  93%|| 102/110 [01:19<00:06,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  94%|| 103/110 [01:20<00:05,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  95%|| 104/110 [01:21<00:04,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  95%|| 105/110 [01:22<00:03,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  96%|| 106/110 [01:23<00:03,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  97%|| 107/110 [01:23<00:02,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  98%|| 108/110 [01:24<00:01,  1.25it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  99%|| 109/110 [01:25<00:00,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress: 100%|| 110/110 [01:26<00:00,  1.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n",
            "[Epoch 3]\n",
            "  [Training] Loss: 0.5295, Accuracy: 76.51%\n",
            "  [Validation] Loss: 0.5724, Accuracy: 71.36%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Epochs:  12%|        | 3/25 [59:34<7:16:42, 1191.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [INFO] Validation loss improved. Model saved to /content/drive/MyDrive/FALL2024/comp550/final_project/models/best_model_2024-12-16_03-02-41.pth.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   0%|          | 0/440 [00:00<?, ?it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   0%|          | 1/440 [00:02<18:18,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   0%|          | 2/440 [00:04<18:11,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   1%|          | 3/440 [00:07<18:16,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   1%|          | 4/440 [00:10<18:17,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   1%|          | 5/440 [00:12<18:36,  2.57s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   1%|         | 6/440 [00:15<18:35,  2.57s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   2%|         | 7/440 [00:17<18:31,  2.57s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   2%|         | 8/440 [00:20<18:33,  2.58s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   2%|         | 9/440 [00:23<18:29,  2.57s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   2%|         | 10/440 [00:25<18:17,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   2%|         | 11/440 [00:27<18:04,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   3%|         | 12/440 [00:30<17:59,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   3%|         | 13/440 [00:33<17:56,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   3%|         | 14/440 [00:35<17:49,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   3%|         | 15/440 [00:38<17:46,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   4%|         | 16/440 [00:40<17:39,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   4%|         | 17/440 [00:42<17:36,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   4%|         | 18/440 [00:45<17:31,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   4%|         | 19/440 [00:47<17:29,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   5%|         | 20/440 [00:50<17:24,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   5%|         | 21/440 [00:52<17:25,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   5%|         | 22/440 [00:55<17:26,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   5%|         | 23/440 [00:57<17:25,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   5%|         | 24/440 [01:00<17:16,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   6%|         | 25/440 [01:02<17:09,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   6%|         | 26/440 [01:05<17:04,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   6%|         | 27/440 [01:07<17:04,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   6%|         | 28/440 [01:10<17:00,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   7%|         | 29/440 [01:12<16:57,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   7%|         | 30/440 [01:15<17:35,  2.57s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   7%|         | 31/440 [01:18<17:22,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   7%|         | 32/440 [01:20<17:10,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   8%|         | 33/440 [01:23<17:11,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   8%|         | 34/440 [01:25<17:02,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   8%|         | 35/440 [01:28<16:54,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   8%|         | 36/440 [01:30<16:51,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   8%|         | 37/440 [01:33<16:46,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   9%|         | 38/440 [01:35<16:48,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   9%|         | 39/440 [01:38<16:47,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   9%|         | 40/440 [01:40<16:42,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   9%|         | 41/440 [01:43<16:36,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  10%|         | 42/440 [01:45<16:32,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  10%|         | 43/440 [01:48<16:30,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  10%|         | 44/440 [01:50<16:28,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  10%|         | 45/440 [01:53<16:25,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  10%|         | 46/440 [01:55<16:20,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  11%|         | 47/440 [01:58<16:24,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  11%|         | 48/440 [02:00<16:21,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  11%|         | 49/440 [02:03<16:15,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  11%|        | 50/440 [02:05<16:12,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  12%|        | 51/440 [02:08<16:10,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  12%|        | 52/440 [02:10<16:08,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  12%|        | 53/440 [02:12<16:03,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  12%|        | 54/440 [02:15<16:02,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  12%|        | 55/440 [02:17<15:58,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  13%|        | 56/440 [02:20<15:55,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  13%|        | 57/440 [02:22<15:55,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  13%|        | 58/440 [02:25<15:51,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  13%|        | 59/440 [02:27<15:48,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  14%|        | 60/440 [02:30<15:45,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  14%|        | 61/440 [02:32<15:40,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  14%|        | 62/440 [02:35<15:40,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  14%|        | 63/440 [02:37<15:41,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  15%|        | 64/440 [02:40<15:37,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  15%|        | 65/440 [02:42<15:34,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  15%|        | 66/440 [02:45<15:31,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  15%|        | 67/440 [02:48<16:10,  2.60s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  15%|        | 68/440 [02:50<15:55,  2.57s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  16%|        | 69/440 [02:53<15:44,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  16%|        | 70/440 [02:55<15:34,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  16%|        | 71/440 [02:58<15:29,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  16%|        | 72/440 [03:00<15:28,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  17%|        | 73/440 [03:03<15:22,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  17%|        | 74/440 [03:05<15:18,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  17%|        | 75/440 [03:08<15:14,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  17%|        | 76/440 [03:10<15:09,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  18%|        | 77/440 [03:13<15:09,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  18%|        | 78/440 [03:15<15:03,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  18%|        | 79/440 [03:18<15:01,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  18%|        | 80/440 [03:20<14:56,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  18%|        | 81/440 [03:23<14:54,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  19%|        | 82/440 [03:25<14:55,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  19%|        | 83/440 [03:28<14:50,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  19%|        | 84/440 [03:30<14:45,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  19%|        | 85/440 [03:33<14:41,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  20%|        | 86/440 [03:35<14:40,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  20%|        | 87/440 [03:38<14:42,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  20%|        | 88/440 [03:40<14:37,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  20%|        | 89/440 [03:43<14:35,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  20%|        | 90/440 [03:45<14:31,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  21%|        | 91/440 [03:48<14:28,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  21%|        | 92/440 [03:50<14:28,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  21%|        | 93/440 [03:53<14:24,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  21%|       | 94/440 [03:55<14:21,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  22%|       | 95/440 [03:58<14:18,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  22%|       | 96/440 [04:00<14:16,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  22%|       | 97/440 [04:03<14:12,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  22%|       | 98/440 [04:05<14:08,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  22%|       | 99/440 [04:07<14:07,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  23%|       | 100/440 [04:10<14:04,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  23%|       | 101/440 [04:12<14:02,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  23%|       | 102/440 [04:15<13:57,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  23%|       | 103/440 [04:17<13:54,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  24%|       | 104/440 [04:20<13:52,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  24%|       | 105/440 [04:23<14:25,  2.58s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  24%|       | 106/440 [04:25<14:14,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  24%|       | 107/440 [04:28<14:03,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  25%|       | 108/440 [04:30<13:57,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  25%|       | 109/440 [04:33<13:50,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  25%|       | 110/440 [04:35<13:45,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  25%|       | 111/440 [04:38<13:47,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  25%|       | 112/440 [04:40<13:40,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  26%|       | 113/440 [04:43<13:35,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  26%|       | 114/440 [04:45<13:32,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  26%|       | 115/440 [04:48<13:28,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  26%|       | 116/440 [04:50<13:28,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  27%|       | 117/440 [04:53<13:25,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  27%|       | 118/440 [04:55<13:21,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  27%|       | 119/440 [04:58<13:20,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  27%|       | 120/440 [05:00<13:15,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  28%|       | 121/440 [05:03<13:14,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  28%|       | 122/440 [05:05<13:08,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  28%|       | 123/440 [05:07<13:08,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  28%|       | 124/440 [05:10<13:04,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  28%|       | 125/440 [05:12<12:59,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  29%|       | 126/440 [05:15<13:00,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  29%|       | 127/440 [05:17<12:55,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  29%|       | 128/440 [05:20<12:51,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  29%|       | 129/440 [05:22<12:51,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  30%|       | 130/440 [05:25<12:47,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  30%|       | 131/440 [05:27<12:46,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  30%|       | 132/440 [05:30<12:43,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  30%|       | 133/440 [05:32<12:40,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  30%|       | 134/440 [05:35<12:37,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  31%|       | 135/440 [05:37<12:38,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  31%|       | 136/440 [05:40<12:36,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  31%|       | 137/440 [05:42<12:33,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  31%|      | 138/440 [05:45<12:29,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  32%|      | 139/440 [05:47<12:25,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  32%|      | 140/440 [05:50<12:23,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  32%|      | 141/440 [05:53<13:00,  2.61s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  32%|      | 142/440 [05:55<12:46,  2.57s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  32%|      | 143/440 [05:58<12:36,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  33%|      | 144/440 [06:00<12:30,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  33%|      | 145/440 [06:03<12:24,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  33%|      | 146/440 [06:05<12:17,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  33%|      | 147/440 [06:07<12:14,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  34%|      | 148/440 [06:10<12:08,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  34%|      | 149/440 [06:12<12:04,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  34%|      | 150/440 [06:15<12:01,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  34%|      | 151/440 [06:17<11:59,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  35%|      | 152/440 [06:20<11:58,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  35%|      | 153/440 [06:22<11:58,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  35%|      | 154/440 [06:25<11:52,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  35%|      | 155/440 [06:27<11:48,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  35%|      | 156/440 [06:30<11:45,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  36%|      | 157/440 [06:32<11:42,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  36%|      | 158/440 [06:35<11:39,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  36%|      | 159/440 [06:37<11:37,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  36%|      | 160/440 [06:40<11:38,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  37%|      | 161/440 [06:42<11:34,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  37%|      | 162/440 [06:45<11:29,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  37%|      | 163/440 [06:47<11:26,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  37%|      | 164/440 [06:50<11:23,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  38%|      | 165/440 [06:52<11:24,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  38%|      | 166/440 [06:55<11:20,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  38%|      | 167/440 [06:57<11:18,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  38%|      | 168/440 [07:00<11:13,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  38%|      | 169/440 [07:02<11:11,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  39%|      | 170/440 [07:05<11:07,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  39%|      | 171/440 [07:07<11:05,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  39%|      | 172/440 [07:10<11:03,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  39%|      | 173/440 [07:12<11:01,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  40%|      | 174/440 [07:14<10:58,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  40%|      | 175/440 [07:17<10:57,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  40%|      | 176/440 [07:19<10:55,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  40%|      | 177/440 [07:22<10:52,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  40%|      | 178/440 [07:24<10:49,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  41%|      | 179/440 [07:27<10:46,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  41%|      | 180/440 [07:29<10:52,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  41%|      | 181/440 [07:32<10:46,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  41%|     | 182/440 [07:35<11:08,  2.59s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  42%|     | 183/440 [07:37<10:56,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  42%|     | 184/440 [07:40<10:49,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  42%|     | 185/440 [07:42<10:41,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  42%|     | 186/440 [07:45<10:35,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  42%|     | 187/440 [07:47<10:32,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  43%|     | 188/440 [07:50<10:27,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  43%|     | 189/440 [07:52<10:25,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  43%|     | 190/440 [07:55<10:22,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  43%|     | 191/440 [07:57<10:18,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  44%|     | 192/440 [08:00<10:15,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  44%|     | 193/440 [08:02<10:12,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  44%|     | 194/440 [08:05<10:11,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  44%|     | 195/440 [08:07<10:08,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  45%|     | 196/440 [08:09<10:05,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  45%|     | 197/440 [08:12<10:02,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  45%|     | 198/440 [08:14<09:59,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  45%|     | 199/440 [08:17<10:00,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  45%|     | 200/440 [08:19<09:55,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  46%|     | 201/440 [08:22<09:52,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  46%|     | 202/440 [08:24<09:49,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  46%|     | 203/440 [08:27<09:46,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  46%|     | 204/440 [08:29<09:43,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  47%|     | 205/440 [08:32<09:42,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  47%|     | 206/440 [08:34<09:38,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  47%|     | 207/440 [08:37<09:35,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  47%|     | 208/440 [08:39<09:32,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  48%|     | 209/440 [08:42<09:31,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  48%|     | 210/440 [08:44<09:28,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  48%|     | 211/440 [08:47<09:25,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  48%|     | 212/440 [08:49<09:21,  2.46s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  48%|     | 213/440 [08:52<09:19,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  49%|     | 214/440 [08:54<09:17,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  49%|     | 215/440 [08:56<09:14,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  49%|     | 216/440 [08:59<09:11,  2.46s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  49%|     | 217/440 [09:01<09:09,  2.46s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  50%|     | 218/440 [09:04<09:07,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  50%|     | 219/440 [09:07<09:29,  2.57s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  50%|     | 220/440 [09:09<09:20,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  50%|     | 221/440 [09:12<09:12,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  50%|     | 222/440 [09:14<09:06,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  51%|     | 223/440 [09:17<09:01,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  51%|     | 224/440 [09:19<09:00,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  51%|     | 225/440 [09:22<08:55,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  51%|    | 226/440 [09:24<08:52,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  52%|    | 227/440 [09:26<08:48,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  52%|    | 228/440 [09:29<08:44,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  52%|    | 229/440 [09:31<08:43,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  52%|    | 230/440 [09:34<08:40,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  52%|    | 231/440 [09:36<08:38,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  53%|    | 232/440 [09:39<08:35,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  53%|    | 233/440 [09:41<08:33,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  53%|    | 234/440 [09:44<08:31,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  53%|    | 235/440 [09:46<08:28,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  54%|    | 236/440 [09:49<08:25,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  54%|    | 237/440 [09:51<08:23,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  54%|    | 238/440 [09:54<08:19,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  54%|    | 239/440 [09:56<08:19,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  55%|    | 240/440 [09:59<08:17,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  55%|    | 241/440 [10:01<08:13,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  55%|    | 242/440 [10:04<08:10,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  55%|    | 243/440 [10:06<08:07,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  55%|    | 244/440 [10:09<08:08,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  56%|    | 245/440 [10:11<08:04,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  56%|    | 246/440 [10:14<08:01,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  56%|    | 247/440 [10:16<07:57,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  56%|    | 248/440 [10:19<07:54,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  57%|    | 249/440 [10:21<07:53,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  57%|    | 250/440 [10:24<07:49,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  57%|    | 251/440 [10:26<07:46,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  57%|    | 252/440 [10:28<07:43,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  57%|    | 253/440 [10:31<07:41,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  58%|    | 254/440 [10:33<07:39,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  58%|    | 255/440 [10:36<07:37,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  58%|    | 256/440 [10:38<07:35,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  58%|    | 257/440 [10:41<07:50,  2.57s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  59%|    | 258/440 [10:44<07:45,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  59%|    | 259/440 [10:46<07:39,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  59%|    | 260/440 [10:49<07:33,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  59%|    | 261/440 [10:51<07:27,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  60%|    | 262/440 [10:54<07:22,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  60%|    | 263/440 [10:56<07:21,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  60%|    | 264/440 [10:59<07:18,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  60%|    | 265/440 [11:01<07:15,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  60%|    | 266/440 [11:03<07:12,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  61%|    | 267/440 [11:06<07:09,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  61%|    | 268/440 [11:08<07:06,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  61%|    | 269/440 [11:11<07:03,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  61%|   | 270/440 [11:13<07:01,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  62%|   | 271/440 [11:16<06:58,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  62%|   | 272/440 [11:18<06:55,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  62%|   | 273/440 [11:21<06:52,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  62%|   | 274/440 [11:23<06:50,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  62%|   | 275/440 [11:26<06:47,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  63%|   | 276/440 [11:28<06:44,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  63%|   | 277/440 [11:31<06:42,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  63%|   | 278/440 [11:33<06:41,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  63%|   | 279/440 [11:36<06:39,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  64%|   | 280/440 [11:38<06:36,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  64%|   | 281/440 [11:41<06:33,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  64%|   | 282/440 [11:43<06:30,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  64%|   | 283/440 [11:46<06:29,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  65%|   | 284/440 [11:48<06:26,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  65%|   | 285/440 [11:51<06:23,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  65%|   | 286/440 [11:53<06:20,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  65%|   | 287/440 [11:55<06:19,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  65%|   | 288/440 [11:58<06:16,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  66%|   | 289/440 [12:00<06:14,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  66%|   | 290/440 [12:03<06:11,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  66%|   | 291/440 [12:05<06:08,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  66%|   | 292/440 [12:08<06:06,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  67%|   | 293/440 [12:11<06:22,  2.60s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  67%|   | 294/440 [12:13<06:14,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  67%|   | 295/440 [12:16<06:07,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  67%|   | 296/440 [12:18<06:02,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  68%|   | 297/440 [12:21<05:58,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  68%|   | 298/440 [12:23<05:56,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  68%|   | 299/440 [12:26<05:53,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  68%|   | 300/440 [12:28<05:50,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  68%|   | 301/440 [12:31<05:47,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  69%|   | 302/440 [12:33<05:45,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  69%|   | 303/440 [12:36<05:44,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  69%|   | 304/440 [12:38<05:40,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  69%|   | 305/440 [12:41<05:36,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  70%|   | 306/440 [12:43<05:32,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  70%|   | 307/440 [12:46<05:29,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  70%|   | 308/440 [12:48<05:28,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  70%|   | 309/440 [12:51<05:25,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  70%|   | 310/440 [12:53<05:22,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  71%|   | 311/440 [12:55<05:19,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  71%|   | 312/440 [12:58<05:17,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  71%|   | 313/440 [13:00<05:15,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  71%|  | 314/440 [13:03<05:12,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  72%|  | 315/440 [13:05<05:10,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  72%|  | 316/440 [13:08<05:07,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  72%|  | 317/440 [13:10<05:05,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  72%|  | 318/440 [13:13<05:03,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  72%|  | 319/440 [13:15<05:00,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  73%|  | 320/440 [13:18<04:58,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  73%|  | 321/440 [13:20<04:55,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  73%|  | 322/440 [13:23<04:54,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  73%|  | 323/440 [13:25<04:51,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  74%|  | 324/440 [13:28<04:48,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  74%|  | 325/440 [13:30<04:45,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  74%|  | 326/440 [13:33<04:42,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  74%|  | 327/440 [13:35<04:39,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  75%|  | 328/440 [13:38<04:37,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  75%|  | 329/440 [13:40<04:35,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  75%|  | 330/440 [13:43<04:32,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  75%|  | 331/440 [13:45<04:30,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  75%|  | 332/440 [13:48<04:27,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  76%|  | 333/440 [13:50<04:25,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  76%|  | 334/440 [13:53<04:33,  2.58s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  76%|  | 335/440 [13:55<04:27,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  76%|  | 336/440 [13:58<04:22,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  77%|  | 337/440 [14:00<04:18,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  77%|  | 338/440 [14:03<04:14,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  77%|  | 339/440 [14:05<04:11,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  77%|  | 340/440 [14:08<04:09,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  78%|  | 341/440 [14:10<04:06,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  78%|  | 342/440 [14:13<04:05,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  78%|  | 343/440 [14:15<04:02,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  78%|  | 344/440 [14:18<03:59,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  78%|  | 345/440 [14:20<03:56,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  79%|  | 346/440 [14:23<03:53,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  79%|  | 347/440 [14:25<03:51,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  79%|  | 348/440 [14:28<03:48,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  79%|  | 349/440 [14:30<03:44,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  80%|  | 350/440 [14:33<03:42,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  80%|  | 351/440 [14:35<03:39,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  80%|  | 352/440 [14:38<03:38,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  80%|  | 353/440 [14:40<03:34,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  80%|  | 354/440 [14:43<03:32,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  81%|  | 355/440 [14:45<03:29,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  81%|  | 356/440 [14:47<03:27,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  81%|  | 357/440 [14:50<03:26,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  81%| | 358/440 [14:52<03:24,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  82%| | 359/440 [14:55<03:21,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  82%| | 360/440 [14:57<03:18,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  82%| | 361/440 [15:00<03:16,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  82%| | 362/440 [15:02<03:13,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  82%| | 363/440 [15:05<03:10,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  83%| | 364/440 [15:07<03:08,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  83%| | 365/440 [15:10<03:06,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  83%| | 366/440 [15:12<03:04,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  83%| | 367/440 [15:15<03:01,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  84%| | 368/440 [15:17<02:58,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  84%| | 369/440 [15:20<02:56,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  84%| | 370/440 [15:22<02:53,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  84%| | 371/440 [15:25<02:58,  2.59s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  85%| | 372/440 [15:28<02:53,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  85%| | 373/440 [15:30<02:49,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  85%| | 374/440 [15:32<02:45,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  85%| | 375/440 [15:35<02:41,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  85%| | 376/440 [15:37<02:39,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  86%| | 377/440 [15:40<02:36,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  86%| | 378/440 [15:42<02:33,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  86%| | 379/440 [15:45<02:31,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  86%| | 380/440 [15:47<02:28,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  87%| | 381/440 [15:50<02:25,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  87%| | 382/440 [15:52<02:23,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  87%| | 383/440 [15:55<02:20,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  87%| | 384/440 [15:57<02:18,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  88%| | 385/440 [16:00<02:15,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  88%| | 386/440 [16:02<02:13,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  88%| | 387/440 [16:05<02:11,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  88%| | 388/440 [16:07<02:08,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  88%| | 389/440 [16:10<02:06,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  89%| | 390/440 [16:12<02:03,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  89%| | 391/440 [16:15<02:01,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  89%| | 392/440 [16:17<01:58,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  89%| | 393/440 [16:19<01:56,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  90%| | 394/440 [16:22<01:53,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  90%| | 395/440 [16:24<01:51,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  90%| | 396/440 [16:27<01:48,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  90%| | 397/440 [16:29<01:46,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  90%| | 398/440 [16:32<01:43,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  91%| | 399/440 [16:34<01:41,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  91%| | 400/440 [16:37<01:39,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  91%| | 401/440 [16:39<01:37,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  91%|| 402/440 [16:42<01:34,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  92%|| 403/440 [16:44<01:31,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  92%|| 404/440 [16:47<01:29,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  92%|| 405/440 [16:49<01:26,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  92%|| 406/440 [16:52<01:24,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  92%|| 407/440 [16:54<01:21,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  93%|| 408/440 [16:57<01:19,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  93%|| 409/440 [16:59<01:20,  2.58s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  93%|| 410/440 [17:02<01:16,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  93%|| 411/440 [17:04<01:13,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  94%|| 412/440 [17:07<01:10,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  94%|| 413/440 [17:09<01:07,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  94%|| 414/440 [17:12<01:04,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  94%|| 415/440 [17:14<01:02,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  95%|| 416/440 [17:17<01:00,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  95%|| 417/440 [17:19<00:57,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  95%|| 418/440 [17:22<00:54,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  95%|| 419/440 [17:24<00:52,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  95%|| 420/440 [17:27<00:49,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  96%|| 421/440 [17:29<00:47,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  96%|| 422/440 [17:32<00:44,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  96%|| 423/440 [17:34<00:42,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  96%|| 424/440 [17:37<00:39,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  97%|| 425/440 [17:39<00:37,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  97%|| 426/440 [17:42<00:34,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  97%|| 427/440 [17:44<00:32,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  97%|| 428/440 [17:47<00:29,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  98%|| 429/440 [17:49<00:27,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  98%|| 430/440 [17:52<00:24,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  98%|| 431/440 [17:54<00:22,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  98%|| 432/440 [17:57<00:19,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  98%|| 433/440 [17:59<00:17,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  99%|| 434/440 [18:01<00:14,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  99%|| 435/440 [18:04<00:12,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  99%|| 436/440 [18:06<00:09,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  99%|| 437/440 [18:09<00:07,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress: 100%|| 438/440 [18:11<00:04,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress: 100%|| 439/440 [18:14<00:02,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress: 100%|| 440/440 [18:16<00:00,  2.49s/it]\n",
            "\n",
            "Validation Progress:   0%|          | 0/110 [00:00<?, ?it/s]\u001b[A\n",
            "Validation Progress:   1%|          | 1/110 [00:00<01:24,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   2%|         | 2/110 [00:01<01:23,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   3%|         | 3/110 [00:02<01:23,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   4%|         | 4/110 [00:03<01:22,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   5%|         | 5/110 [00:03<01:22,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   5%|         | 6/110 [00:04<01:22,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   6%|         | 7/110 [00:05<01:27,  1.17it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   7%|         | 8/110 [00:06<01:26,  1.17it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   8%|         | 9/110 [00:07<01:23,  1.20it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   9%|         | 10/110 [00:08<01:21,  1.23it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  10%|         | 11/110 [00:08<01:19,  1.24it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  11%|         | 12/110 [00:09<01:19,  1.23it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  12%|        | 13/110 [00:10<01:18,  1.23it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  13%|        | 14/110 [00:11<01:17,  1.23it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  14%|        | 15/110 [00:12<01:16,  1.24it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  15%|        | 16/110 [00:12<01:14,  1.25it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  15%|        | 17/110 [00:13<01:15,  1.24it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  16%|        | 18/110 [00:14<01:13,  1.25it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  17%|        | 19/110 [00:15<01:12,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  18%|        | 20/110 [00:16<01:11,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  19%|        | 21/110 [00:16<01:10,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  20%|        | 22/110 [00:17<01:09,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  21%|        | 23/110 [00:18<01:08,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  22%|       | 24/110 [00:19<01:08,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  23%|       | 25/110 [00:20<01:07,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  24%|       | 26/110 [00:20<01:06,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  25%|       | 27/110 [00:21<01:05,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  25%|       | 28/110 [00:22<01:04,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  26%|       | 29/110 [00:23<01:03,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  27%|       | 30/110 [00:24<01:03,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  28%|       | 31/110 [00:24<01:02,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  29%|       | 32/110 [00:25<01:01,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  30%|       | 33/110 [00:26<01:00,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  31%|       | 34/110 [00:27<00:59,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  32%|      | 35/110 [00:27<00:58,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  33%|      | 36/110 [00:28<00:57,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  34%|      | 37/110 [00:29<00:56,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  35%|      | 38/110 [00:30<00:56,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  35%|      | 39/110 [00:31<00:55,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  36%|      | 40/110 [00:31<00:55,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  37%|      | 41/110 [00:32<00:54,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  38%|      | 42/110 [00:33<00:53,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  39%|      | 43/110 [00:34<00:52,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  40%|      | 44/110 [00:34<00:52,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  41%|      | 45/110 [00:35<00:51,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  42%|     | 46/110 [00:36<00:51,  1.23it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  43%|     | 47/110 [00:37<00:50,  1.24it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  44%|     | 48/110 [00:38<00:49,  1.25it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  45%|     | 49/110 [00:39<00:48,  1.25it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  45%|     | 50/110 [00:39<00:48,  1.25it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  46%|     | 51/110 [00:40<00:47,  1.25it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  47%|     | 52/110 [00:41<00:46,  1.25it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  48%|     | 53/110 [00:42<00:45,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  49%|     | 54/110 [00:42<00:44,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  50%|     | 55/110 [00:43<00:43,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  51%|     | 56/110 [00:44<00:42,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  52%|    | 57/110 [00:45<00:41,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  53%|    | 58/110 [00:46<00:40,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  54%|    | 59/110 [00:46<00:40,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  55%|    | 60/110 [00:47<00:39,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  55%|    | 61/110 [00:48<00:39,  1.25it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  56%|    | 62/110 [00:49<00:38,  1.25it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  57%|    | 63/110 [00:50<00:37,  1.25it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  58%|    | 64/110 [00:50<00:36,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  59%|    | 65/110 [00:51<00:35,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  60%|    | 66/110 [00:52<00:34,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  61%|    | 67/110 [00:53<00:33,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  62%|   | 68/110 [00:54<00:33,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  63%|   | 69/110 [00:54<00:32,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  64%|   | 70/110 [00:55<00:31,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  65%|   | 71/110 [00:56<00:30,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  65%|   | 72/110 [00:57<00:29,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  66%|   | 73/110 [00:57<00:29,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  67%|   | 74/110 [00:58<00:28,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  68%|   | 75/110 [00:59<00:27,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  69%|   | 76/110 [01:00<00:26,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  70%|   | 77/110 [01:01<00:26,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  71%|   | 78/110 [01:01<00:25,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  72%|  | 79/110 [01:02<00:24,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  73%|  | 80/110 [01:03<00:23,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  74%|  | 81/110 [01:04<00:22,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  75%|  | 82/110 [01:05<00:22,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  75%|  | 83/110 [01:05<00:21,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  76%|  | 84/110 [01:06<00:20,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  77%|  | 85/110 [01:07<00:19,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  78%|  | 86/110 [01:08<00:18,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  79%|  | 87/110 [01:08<00:17,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  80%|  | 88/110 [01:09<00:17,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  81%|  | 89/110 [01:10<00:16,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  82%| | 90/110 [01:11<00:15,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  83%| | 91/110 [01:12<00:14,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  84%| | 92/110 [01:12<00:14,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  85%| | 93/110 [01:13<00:13,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  85%| | 94/110 [01:14<00:12,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  86%| | 95/110 [01:15<00:11,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  87%| | 96/110 [01:16<00:11,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  88%| | 97/110 [01:16<00:10,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  89%| | 98/110 [01:17<00:09,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  90%| | 99/110 [01:18<00:08,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  91%| | 100/110 [01:19<00:07,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  92%|| 101/110 [01:20<00:07,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  93%|| 102/110 [01:20<00:06,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  94%|| 103/110 [01:21<00:05,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  95%|| 104/110 [01:22<00:04,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  95%|| 105/110 [01:23<00:03,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  96%|| 106/110 [01:23<00:03,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  97%|| 107/110 [01:24<00:02,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  98%|| 108/110 [01:25<00:01,  1.25it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  99%|| 109/110 [01:26<00:00,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress: 100%|| 110/110 [01:27<00:00,  1.26it/s]\n",
            "Epochs:  16%|        | 4/25 [1:19:18<6:55:54, 1188.31s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n",
            "[Epoch 4]\n",
            "  [Training] Loss: 0.4467, Accuracy: 83.49%\n",
            "  [Validation] Loss: 0.6158, Accuracy: 67.61%\n",
            "  [INFO] No improvement in validation loss. Patience counter: 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   0%|          | 0/440 [00:00<?, ?it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   0%|          | 1/440 [00:02<18:12,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   0%|          | 2/440 [00:04<18:06,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   1%|          | 3/440 [00:07<18:07,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   1%|          | 4/440 [00:09<18:04,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   1%|          | 5/440 [00:12<19:06,  2.64s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   1%|         | 6/440 [00:15<18:44,  2.59s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   2%|         | 7/440 [00:17<18:27,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   2%|         | 8/440 [00:20<18:16,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   2%|         | 9/440 [00:22<18:10,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   2%|         | 10/440 [00:25<18:10,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   2%|         | 11/440 [00:27<18:01,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   3%|         | 12/440 [00:30<17:54,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   3%|         | 13/440 [00:32<17:50,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   3%|         | 14/440 [00:35<17:47,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   3%|         | 15/440 [00:37<17:46,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   4%|         | 16/440 [00:40<17:39,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   4%|         | 17/440 [00:42<17:34,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   4%|         | 18/440 [00:45<17:32,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   4%|         | 19/440 [00:47<17:31,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   5%|         | 20/440 [00:50<17:25,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   5%|         | 21/440 [00:52<17:19,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   5%|         | 22/440 [00:55<17:18,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   5%|         | 23/440 [00:57<17:16,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   5%|         | 24/440 [01:00<17:20,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   6%|         | 25/440 [01:02<17:12,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   6%|         | 26/440 [01:05<17:07,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   6%|         | 27/440 [01:07<17:06,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   6%|         | 28/440 [01:10<17:01,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   7%|         | 29/440 [01:12<17:00,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   7%|         | 30/440 [01:15<16:56,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   7%|         | 31/440 [01:17<16:52,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   7%|         | 32/440 [01:20<16:50,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   8%|         | 33/440 [01:22<16:50,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   8%|         | 34/440 [01:25<16:50,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   8%|         | 35/440 [01:27<16:51,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   8%|         | 36/440 [01:30<16:50,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   8%|         | 37/440 [01:32<16:46,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   9%|         | 38/440 [01:35<16:43,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   9%|         | 39/440 [01:37<16:41,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   9%|         | 40/440 [01:40<16:37,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   9%|         | 41/440 [01:42<16:35,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  10%|         | 42/440 [01:45<16:34,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  10%|         | 43/440 [01:47<16:34,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  10%|         | 44/440 [01:50<16:33,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  10%|         | 45/440 [01:52<16:28,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  10%|         | 46/440 [01:55<17:04,  2.60s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  11%|         | 47/440 [01:57<16:45,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  11%|         | 48/440 [02:00<16:35,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  11%|         | 49/440 [02:02<16:32,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  11%|        | 50/440 [02:05<16:23,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  12%|        | 51/440 [02:07<16:17,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  12%|        | 52/440 [02:10<16:13,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  12%|        | 53/440 [02:12<16:09,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  12%|        | 54/440 [02:15<16:09,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  12%|        | 55/440 [02:17<16:04,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  13%|        | 56/440 [02:20<15:58,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  13%|        | 57/440 [02:22<15:54,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  13%|        | 58/440 [02:25<15:55,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  13%|        | 59/440 [02:27<15:49,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  14%|        | 60/440 [02:30<15:49,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  14%|        | 61/440 [02:32<15:45,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  14%|        | 62/440 [02:35<15:40,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  14%|        | 63/440 [02:37<15:41,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  15%|        | 64/440 [02:40<15:38,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  15%|        | 65/440 [02:42<15:32,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  15%|        | 66/440 [02:45<15:32,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  15%|        | 67/440 [02:47<15:27,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  15%|        | 68/440 [02:50<15:24,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  16%|        | 69/440 [02:52<15:22,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  16%|        | 70/440 [02:55<15:20,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  16%|        | 71/440 [02:57<15:18,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  16%|        | 72/440 [03:00<15:14,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  17%|        | 73/440 [03:02<15:16,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  17%|        | 74/440 [03:05<15:12,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  17%|        | 75/440 [03:07<15:09,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  17%|        | 76/440 [03:10<15:05,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  18%|        | 77/440 [03:12<15:05,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  18%|        | 78/440 [03:15<15:04,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  18%|        | 79/440 [03:17<14:59,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  18%|        | 80/440 [03:20<14:53,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  18%|        | 81/440 [03:22<14:48,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  19%|        | 82/440 [03:25<14:44,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  19%|        | 83/440 [03:27<15:19,  2.57s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  19%|        | 84/440 [03:30<15:06,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  19%|        | 85/440 [03:32<14:57,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  20%|        | 86/440 [03:35<14:50,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  20%|        | 87/440 [03:37<14:43,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  20%|        | 88/440 [03:40<14:38,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  20%|        | 89/440 [03:42<14:32,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  20%|        | 90/440 [03:45<14:28,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  21%|        | 91/440 [03:47<14:25,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  21%|        | 92/440 [03:50<14:20,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  21%|        | 93/440 [03:52<14:19,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  21%|       | 94/440 [03:55<14:16,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  22%|       | 95/440 [03:57<14:11,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  22%|       | 96/440 [04:00<14:09,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  22%|       | 97/440 [04:02<14:06,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  22%|       | 98/440 [04:05<14:10,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  22%|       | 99/440 [04:07<14:05,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  23%|       | 100/440 [04:09<14:01,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  23%|       | 101/440 [04:12<13:58,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  23%|       | 102/440 [04:14<13:56,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  23%|       | 103/440 [04:17<14:03,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  24%|       | 104/440 [04:19<13:57,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  24%|       | 105/440 [04:22<13:53,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  24%|       | 106/440 [04:24<13:50,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  24%|       | 107/440 [04:27<13:48,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  25%|       | 108/440 [04:29<13:47,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  25%|       | 109/440 [04:32<13:41,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  25%|       | 110/440 [04:34<13:38,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  25%|       | 111/440 [04:37<13:36,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  25%|       | 112/440 [04:39<13:33,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  26%|       | 113/440 [04:42<13:33,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  26%|       | 114/440 [04:44<13:28,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  26%|       | 115/440 [04:47<13:25,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  26%|       | 116/440 [04:49<13:23,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  27%|       | 117/440 [04:52<13:20,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  27%|       | 118/440 [04:54<13:18,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  27%|       | 119/440 [04:57<13:15,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  27%|       | 120/440 [04:59<13:13,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  28%|       | 121/440 [05:02<13:42,  2.58s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  28%|       | 122/440 [05:04<13:31,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  28%|       | 123/440 [05:07<13:22,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  28%|       | 124/440 [05:09<13:16,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  28%|       | 125/440 [05:12<13:09,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  29%|       | 126/440 [05:14<13:03,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  29%|       | 127/440 [05:17<13:00,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  29%|       | 128/440 [05:19<12:54,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  29%|       | 129/440 [05:22<12:51,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  30%|       | 130/440 [05:24<12:50,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  30%|       | 131/440 [05:27<12:46,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  30%|       | 132/440 [05:29<12:42,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  30%|       | 133/440 [05:32<12:40,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  30%|       | 134/440 [05:34<12:37,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  31%|       | 135/440 [05:37<12:32,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  31%|       | 136/440 [05:39<12:30,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  31%|       | 137/440 [05:42<12:30,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  31%|      | 138/440 [05:44<12:29,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  32%|      | 139/440 [05:47<12:28,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  32%|      | 140/440 [05:49<12:23,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  32%|      | 141/440 [05:52<12:20,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  32%|      | 142/440 [05:54<12:21,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  32%|      | 143/440 [05:57<12:16,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  33%|      | 144/440 [05:59<12:12,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  33%|      | 145/440 [06:01<12:09,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  33%|      | 146/440 [06:04<12:07,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  33%|      | 147/440 [06:06<12:05,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  34%|      | 148/440 [06:09<12:03,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  34%|      | 149/440 [06:11<11:58,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  34%|      | 150/440 [06:14<11:56,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  34%|      | 151/440 [06:16<11:55,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  35%|      | 152/440 [06:19<11:57,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  35%|      | 153/440 [06:21<11:53,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  35%|      | 154/440 [06:24<11:50,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  35%|      | 155/440 [06:26<11:46,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  35%|      | 156/440 [06:29<11:43,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  36%|      | 157/440 [06:32<12:12,  2.59s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  36%|      | 158/440 [06:34<11:58,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  36%|      | 159/440 [06:36<11:47,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  36%|      | 160/440 [06:39<11:41,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  37%|      | 161/440 [06:41<11:39,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  37%|      | 162/440 [06:44<11:39,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  37%|      | 163/440 [06:46<11:32,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  37%|      | 164/440 [06:49<11:32,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  38%|      | 165/440 [06:51<11:30,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  38%|      | 166/440 [06:54<11:27,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  38%|      | 167/440 [06:56<11:24,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  38%|      | 168/440 [06:59<11:21,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  38%|      | 169/440 [07:01<11:19,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  39%|      | 170/440 [07:04<11:17,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  39%|      | 171/440 [07:07<11:14,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  39%|      | 172/440 [07:09<11:11,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  39%|      | 173/440 [07:12<11:09,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  40%|      | 174/440 [07:14<11:05,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  40%|      | 175/440 [07:17<11:06,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  40%|      | 176/440 [07:19<11:06,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  40%|      | 177/440 [07:22<11:00,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  40%|      | 178/440 [07:24<10:56,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  41%|      | 179/440 [07:27<10:55,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  41%|      | 180/440 [07:29<10:53,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  41%|      | 181/440 [07:32<10:52,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  41%|     | 182/440 [07:34<10:49,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  42%|     | 183/440 [07:37<10:46,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  42%|     | 184/440 [07:39<10:42,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  42%|     | 185/440 [07:42<10:37,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  42%|     | 186/440 [07:44<10:36,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  42%|     | 187/440 [07:47<10:31,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  43%|     | 188/440 [07:49<10:26,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  43%|     | 189/440 [07:52<10:26,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  43%|     | 190/440 [07:54<10:24,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  43%|     | 191/440 [07:57<10:19,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  44%|     | 192/440 [07:59<10:16,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  44%|     | 193/440 [08:02<10:14,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  44%|     | 194/440 [08:04<10:10,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  44%|     | 195/440 [08:07<10:08,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  45%|     | 196/440 [08:09<10:07,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  45%|     | 197/440 [08:12<10:04,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  45%|     | 198/440 [08:14<10:23,  2.58s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  45%|     | 199/440 [08:17<10:13,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  45%|     | 200/440 [08:19<10:10,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  46%|     | 201/440 [08:22<10:06,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  46%|     | 202/440 [08:24<10:00,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  46%|     | 203/440 [08:27<09:55,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  46%|     | 204/440 [08:29<09:53,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  47%|     | 205/440 [08:32<09:53,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  47%|     | 206/440 [08:34<09:53,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  47%|     | 207/440 [08:37<09:46,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  47%|     | 208/440 [08:39<09:42,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  48%|     | 209/440 [08:42<09:38,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  48%|     | 210/440 [08:44<09:34,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  48%|     | 211/440 [08:47<09:32,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  48%|     | 212/440 [08:49<09:30,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  48%|     | 213/440 [08:52<09:26,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  49%|     | 214/440 [08:54<09:26,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  49%|     | 215/440 [08:57<09:25,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  49%|     | 216/440 [08:59<09:26,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  49%|     | 217/440 [09:02<09:21,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  50%|     | 218/440 [09:04<09:17,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  50%|     | 219/440 [09:07<09:16,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  50%|     | 220/440 [09:10<09:15,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  50%|     | 221/440 [09:12<09:11,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  50%|     | 222/440 [09:15<09:08,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  51%|     | 223/440 [09:17<09:04,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  51%|     | 224/440 [09:20<09:01,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  51%|     | 225/440 [09:22<08:58,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  51%|    | 226/440 [09:25<08:54,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  52%|    | 227/440 [09:27<08:51,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  52%|    | 228/440 [09:30<08:49,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  52%|    | 229/440 [09:32<08:48,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  52%|    | 230/440 [09:35<08:45,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  52%|    | 231/440 [09:37<08:41,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  53%|    | 232/440 [09:40<08:38,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  53%|    | 233/440 [09:42<08:35,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  53%|    | 234/440 [09:44<08:31,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  53%|    | 235/440 [09:47<08:52,  2.60s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  54%|    | 236/440 [09:50<08:42,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  54%|    | 237/440 [09:52<08:35,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  54%|    | 238/440 [09:55<08:29,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  54%|    | 239/440 [09:57<08:24,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  55%|    | 240/440 [10:00<08:24,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  55%|    | 241/440 [10:02<08:20,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  55%|    | 242/440 [10:05<08:15,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  55%|    | 243/440 [10:07<08:11,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  55%|    | 244/440 [10:10<08:09,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  56%|    | 245/440 [10:12<08:08,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  56%|    | 246/440 [10:15<08:03,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  56%|    | 247/440 [10:17<08:00,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  56%|    | 248/440 [10:20<07:57,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  57%|    | 249/440 [10:22<07:56,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  57%|    | 250/440 [10:25<07:54,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  57%|    | 251/440 [10:27<07:51,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  57%|    | 252/440 [10:30<07:48,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  57%|    | 253/440 [10:32<07:46,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  58%|    | 254/440 [10:35<07:43,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  58%|    | 255/440 [10:37<07:42,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  58%|    | 256/440 [10:40<07:38,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  58%|    | 257/440 [10:42<07:35,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  59%|    | 258/440 [10:45<07:32,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  59%|    | 259/440 [10:47<07:29,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  59%|    | 260/440 [10:50<07:29,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  59%|    | 261/440 [10:52<07:25,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  60%|    | 262/440 [10:55<07:22,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  60%|    | 263/440 [10:57<07:20,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  60%|    | 264/440 [11:00<07:18,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  60%|    | 265/440 [11:02<07:18,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  60%|    | 266/440 [11:05<07:15,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  61%|    | 267/440 [11:07<07:11,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  61%|    | 268/440 [11:10<07:08,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  61%|    | 269/440 [11:12<07:06,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  61%|   | 270/440 [11:15<07:04,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  62%|   | 271/440 [11:17<07:02,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  62%|   | 272/440 [11:20<06:58,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  62%|   | 273/440 [11:22<07:12,  2.59s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  62%|   | 274/440 [11:25<07:06,  2.57s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  62%|   | 275/440 [11:27<06:59,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  63%|   | 276/440 [11:30<06:53,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  63%|   | 277/440 [11:32<06:48,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  63%|   | 278/440 [11:35<06:45,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  63%|   | 279/440 [11:37<06:44,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  64%|   | 280/440 [11:40<06:39,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  64%|   | 281/440 [11:42<06:36,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  64%|   | 282/440 [11:45<06:33,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  64%|   | 283/440 [11:47<06:31,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  65%|   | 284/440 [11:50<06:28,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  65%|   | 285/440 [11:52<06:25,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  65%|   | 286/440 [11:55<06:24,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  65%|   | 287/440 [11:57<06:21,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  65%|   | 288/440 [12:00<06:17,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  66%|   | 289/440 [12:02<06:16,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  66%|   | 290/440 [12:05<06:12,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  66%|   | 291/440 [12:07<06:09,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  66%|   | 292/440 [12:10<06:06,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  67%|   | 293/440 [12:12<06:04,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  67%|   | 294/440 [12:15<06:02,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  67%|   | 295/440 [12:17<05:59,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  67%|   | 296/440 [12:20<05:57,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  68%|   | 297/440 [12:22<05:54,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  68%|   | 298/440 [12:25<05:51,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  68%|   | 299/440 [12:27<05:49,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  68%|   | 300/440 [12:30<05:47,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  68%|   | 301/440 [12:32<05:44,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  69%|   | 302/440 [12:34<05:41,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  69%|   | 303/440 [12:37<05:38,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  69%|   | 304/440 [12:39<05:36,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  69%|   | 305/440 [12:42<05:34,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  70%|   | 306/440 [12:44<05:32,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  70%|   | 307/440 [12:47<05:29,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  70%|   | 308/440 [12:49<05:26,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  70%|   | 309/440 [12:52<05:39,  2.59s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  70%|   | 310/440 [12:55<05:32,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  71%|   | 311/440 [12:57<05:26,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  71%|   | 312/440 [13:00<05:22,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  71%|   | 313/440 [13:02<05:18,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  71%|  | 314/440 [13:05<05:16,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  72%|  | 315/440 [13:07<05:12,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  72%|  | 316/440 [13:10<05:09,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  72%|  | 317/440 [13:12<05:06,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  72%|  | 318/440 [13:15<05:04,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  72%|  | 319/440 [13:17<05:03,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  73%|  | 320/440 [13:20<04:58,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  73%|  | 321/440 [13:22<04:56,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  73%|  | 322/440 [13:25<04:53,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  73%|  | 323/440 [13:27<04:51,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  74%|  | 324/440 [13:30<04:48,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  74%|  | 325/440 [13:32<04:46,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  74%|  | 326/440 [13:35<04:44,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  74%|  | 327/440 [13:37<04:42,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  75%|  | 328/440 [13:40<04:40,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  75%|  | 329/440 [13:42<04:37,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  75%|  | 330/440 [13:44<04:33,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  75%|  | 331/440 [13:47<04:31,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  75%|  | 332/440 [13:49<04:29,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  76%|  | 333/440 [13:52<04:27,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  76%|  | 334/440 [13:54<04:24,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  76%|  | 335/440 [13:57<04:21,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  76%|  | 336/440 [13:59<04:18,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  77%|  | 337/440 [14:02<04:15,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  77%|  | 338/440 [14:04<04:14,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  77%|  | 339/440 [14:07<04:11,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  77%|  | 340/440 [14:09<04:09,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  78%|  | 341/440 [14:12<04:07,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  78%|  | 342/440 [14:14<04:04,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  78%|  | 343/440 [14:17<04:01,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  78%|  | 344/440 [14:19<03:59,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  78%|  | 345/440 [14:22<03:56,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  79%|  | 346/440 [14:24<03:54,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  79%|  | 347/440 [14:27<03:51,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  79%|  | 348/440 [14:29<03:50,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  79%|  | 349/440 [14:32<03:48,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  80%|  | 350/440 [14:35<03:54,  2.60s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  80%|  | 351/440 [14:37<03:48,  2.57s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  80%|  | 352/440 [14:40<03:43,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  80%|  | 353/440 [14:42<03:40,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  80%|  | 354/440 [14:45<03:37,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  81%|  | 355/440 [14:47<03:33,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  81%|  | 356/440 [14:50<03:30,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  81%|  | 357/440 [14:52<03:26,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  81%| | 358/440 [14:55<03:25,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  82%| | 359/440 [14:57<03:22,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  82%| | 360/440 [15:00<03:19,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  82%| | 361/440 [15:02<03:17,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  82%| | 362/440 [15:05<03:14,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  82%| | 363/440 [15:07<03:11,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  83%| | 364/440 [15:10<03:09,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  83%| | 365/440 [15:12<03:06,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  83%| | 366/440 [15:15<03:03,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  83%| | 367/440 [15:17<03:01,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  84%| | 368/440 [15:20<03:00,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  84%| | 369/440 [15:22<02:58,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  84%| | 370/440 [15:25<02:55,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  84%| | 371/440 [15:27<02:52,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  85%| | 372/440 [15:30<02:49,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  85%| | 373/440 [15:32<02:46,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  85%| | 374/440 [15:35<02:43,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  85%| | 375/440 [15:37<02:41,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  85%| | 376/440 [15:40<02:39,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  86%| | 377/440 [15:42<02:36,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  86%| | 378/440 [15:45<02:34,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  86%| | 379/440 [15:47<02:31,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  86%| | 380/440 [15:49<02:28,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  87%| | 381/440 [15:52<02:26,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  87%| | 382/440 [15:54<02:24,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  87%| | 383/440 [15:57<02:21,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  87%| | 384/440 [15:59<02:18,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  88%| | 385/440 [16:02<02:16,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  88%| | 386/440 [16:04<02:14,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  88%| | 387/440 [16:07<02:17,  2.59s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  88%| | 388/440 [16:10<02:12,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  88%| | 389/440 [16:12<02:09,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  89%| | 390/440 [16:15<02:05,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  89%| | 391/440 [16:17<02:02,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  89%| | 392/440 [16:20<02:00,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  89%| | 393/440 [16:22<01:58,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  90%| | 394/440 [16:25<01:55,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  90%| | 395/440 [16:27<01:52,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  90%| | 396/440 [16:30<01:49,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  90%| | 397/440 [16:32<01:47,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  90%| | 398/440 [16:35<01:45,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  91%| | 399/440 [16:37<01:42,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  91%| | 400/440 [16:40<01:39,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  91%| | 401/440 [16:42<01:37,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  91%|| 402/440 [16:45<01:35,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  92%|| 403/440 [16:47<01:32,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  92%|| 404/440 [16:50<01:29,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  92%|| 405/440 [16:52<01:27,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  92%|| 406/440 [16:55<01:24,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  92%|| 407/440 [16:57<01:22,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  93%|| 408/440 [17:00<01:19,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  93%|| 409/440 [17:02<01:17,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  93%|| 410/440 [17:04<01:14,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  93%|| 411/440 [17:07<01:12,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  94%|| 412/440 [17:09<01:09,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  94%|| 413/440 [17:12<01:07,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  94%|| 414/440 [17:14<01:04,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  94%|| 415/440 [17:17<01:02,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  95%|| 416/440 [17:19<00:59,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  95%|| 417/440 [17:22<00:57,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  95%|| 418/440 [17:24<00:54,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  95%|| 419/440 [17:27<00:52,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  95%|| 420/440 [17:29<00:49,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  96%|| 421/440 [17:32<00:47,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  96%|| 422/440 [17:34<00:44,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  96%|| 423/440 [17:37<00:42,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  96%|| 424/440 [17:39<00:39,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  97%|| 425/440 [17:42<00:37,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  97%|| 426/440 [17:44<00:35,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  97%|| 427/440 [17:47<00:32,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  97%|| 428/440 [17:50<00:31,  2.59s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  98%|| 429/440 [17:52<00:28,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  98%|| 430/440 [17:55<00:25,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  98%|| 431/440 [17:57<00:22,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  98%|| 432/440 [18:00<00:20,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  98%|| 433/440 [18:02<00:17,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  99%|| 434/440 [18:05<00:14,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  99%|| 435/440 [18:07<00:12,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  99%|| 436/440 [18:10<00:09,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  99%|| 437/440 [18:12<00:07,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress: 100%|| 438/440 [18:15<00:04,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress: 100%|| 439/440 [18:17<00:02,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress: 100%|| 440/440 [18:19<00:00,  2.50s/it]\n",
            "\n",
            "Validation Progress:   0%|          | 0/110 [00:00<?, ?it/s]\u001b[A\n",
            "Validation Progress:   1%|          | 1/110 [00:00<01:24,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   2%|         | 2/110 [00:01<01:25,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   3%|         | 3/110 [00:02<01:24,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   4%|         | 4/110 [00:03<01:23,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   5%|         | 5/110 [00:03<01:22,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   5%|         | 6/110 [00:04<01:22,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   6%|         | 7/110 [00:05<01:21,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   7%|         | 8/110 [00:06<01:20,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   8%|         | 9/110 [00:07<01:18,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   9%|         | 10/110 [00:07<01:18,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  10%|         | 11/110 [00:08<01:17,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  11%|         | 12/110 [00:09<01:17,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  12%|        | 13/110 [00:10<01:16,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  13%|        | 14/110 [00:10<01:14,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  14%|        | 15/110 [00:11<01:13,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  15%|        | 16/110 [00:12<01:14,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  15%|        | 17/110 [00:13<01:13,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  16%|        | 18/110 [00:14<01:12,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  17%|        | 19/110 [00:14<01:11,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  18%|        | 20/110 [00:15<01:10,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  19%|        | 21/110 [00:16<01:10,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  20%|        | 22/110 [00:17<01:09,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  21%|        | 23/110 [00:18<01:08,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  22%|       | 24/110 [00:18<01:07,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  23%|       | 25/110 [00:19<01:07,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  24%|       | 26/110 [00:20<01:06,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  25%|       | 27/110 [00:21<01:05,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  25%|       | 28/110 [00:22<01:04,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  26%|       | 29/110 [00:22<01:03,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  27%|       | 30/110 [00:23<01:02,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  28%|       | 31/110 [00:24<01:01,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  29%|       | 32/110 [00:25<01:01,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  30%|       | 33/110 [00:25<01:00,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  31%|       | 34/110 [00:26<01:00,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  32%|      | 35/110 [00:27<00:59,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  33%|      | 36/110 [00:28<00:58,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  34%|      | 37/110 [00:29<00:57,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  35%|      | 38/110 [00:29<00:56,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  35%|      | 39/110 [00:30<00:55,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  36%|      | 40/110 [00:31<00:54,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  37%|      | 41/110 [00:32<00:54,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  38%|      | 42/110 [00:33<00:53,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  39%|      | 43/110 [00:33<00:52,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  40%|      | 44/110 [00:34<00:51,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  41%|      | 45/110 [00:35<00:50,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  42%|     | 46/110 [00:36<00:50,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  43%|     | 47/110 [00:36<00:49,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  44%|     | 48/110 [00:37<00:48,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  45%|     | 49/110 [00:38<00:48,  1.25it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  45%|     | 50/110 [00:39<00:47,  1.25it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  46%|     | 51/110 [00:40<00:46,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  47%|     | 52/110 [00:40<00:45,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  48%|     | 53/110 [00:41<00:44,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  49%|     | 54/110 [00:42<00:43,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  50%|     | 55/110 [00:43<00:43,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  51%|     | 56/110 [00:44<00:42,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  52%|    | 57/110 [00:44<00:41,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  53%|    | 58/110 [00:45<00:41,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  54%|    | 59/110 [00:46<00:40,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  55%|    | 60/110 [00:47<00:39,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  55%|    | 61/110 [00:48<00:38,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  56%|    | 62/110 [00:48<00:37,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  57%|    | 63/110 [00:49<00:37,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  58%|    | 64/110 [00:50<00:36,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  59%|    | 65/110 [00:51<00:35,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  60%|    | 66/110 [00:51<00:34,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  61%|    | 67/110 [00:52<00:33,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  62%|   | 68/110 [00:53<00:32,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  63%|   | 69/110 [00:54<00:32,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  64%|   | 70/110 [00:55<00:31,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  65%|   | 71/110 [00:55<00:30,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  65%|   | 72/110 [00:56<00:29,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  66%|   | 73/110 [00:57<00:28,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  67%|   | 74/110 [00:58<00:28,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  68%|   | 75/110 [00:58<00:27,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  69%|   | 76/110 [00:59<00:26,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  70%|   | 77/110 [01:00<00:25,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  71%|   | 78/110 [01:01<00:25,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  72%|  | 79/110 [01:02<00:24,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  73%|  | 80/110 [01:02<00:24,  1.24it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  74%|  | 81/110 [01:03<00:23,  1.25it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  75%|  | 82/110 [01:04<00:22,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  75%|  | 83/110 [01:05<00:21,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  76%|  | 84/110 [01:06<00:20,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  77%|  | 85/110 [01:06<00:19,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  78%|  | 86/110 [01:07<00:18,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  79%|  | 87/110 [01:08<00:18,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  80%|  | 88/110 [01:09<00:17,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  81%|  | 89/110 [01:10<00:16,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  82%| | 90/110 [01:10<00:15,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  83%| | 91/110 [01:11<00:14,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  84%| | 92/110 [01:12<00:14,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  85%| | 93/110 [01:13<00:13,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  85%| | 94/110 [01:13<00:12,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  86%| | 95/110 [01:14<00:11,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  87%| | 96/110 [01:15<00:11,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  88%| | 97/110 [01:16<00:10,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  89%| | 98/110 [01:17<00:09,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  90%| | 99/110 [01:17<00:08,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  91%| | 100/110 [01:18<00:07,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  92%|| 101/110 [01:19<00:07,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  93%|| 102/110 [01:20<00:06,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  94%|| 103/110 [01:21<00:05,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  95%|| 104/110 [01:21<00:04,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  95%|| 105/110 [01:22<00:03,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  96%|| 106/110 [01:23<00:03,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  97%|| 107/110 [01:24<00:02,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  98%|| 108/110 [01:24<00:01,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  99%|| 109/110 [01:25<00:00,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress: 100%|| 110/110 [01:26<00:00,  1.27it/s]\n",
            "Epochs:  20%|        | 5/25 [1:39:05<6:35:53, 1187.66s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n",
            "[Epoch 5]\n",
            "  [Training] Loss: 0.3672, Accuracy: 88.86%\n",
            "  [Validation] Loss: 0.5849, Accuracy: 71.36%\n",
            "  [INFO] No improvement in validation loss. Patience counter: 2/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   0%|          | 0/440 [00:00<?, ?it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   0%|          | 1/440 [00:02<18:19,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   0%|          | 2/440 [00:04<18:04,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   1%|          | 3/440 [00:07<18:10,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   1%|          | 4/440 [00:09<18:08,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   1%|          | 5/440 [00:12<18:03,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   1%|         | 6/440 [00:15<18:08,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   2%|         | 7/440 [00:17<18:02,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   2%|         | 8/440 [00:19<17:57,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   2%|         | 9/440 [00:22<17:53,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   2%|         | 10/440 [00:24<17:50,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   2%|         | 11/440 [00:27<17:51,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   3%|         | 12/440 [00:29<17:43,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   3%|         | 13/440 [00:32<17:37,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   3%|         | 14/440 [00:34<17:35,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   3%|         | 15/440 [00:37<17:39,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   4%|         | 16/440 [00:39<17:37,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   4%|         | 17/440 [00:42<17:33,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   4%|         | 18/440 [00:44<17:31,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   4%|         | 19/440 [00:47<17:27,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   5%|         | 20/440 [00:49<17:23,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   5%|         | 21/440 [00:52<17:24,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   5%|         | 22/440 [00:54<17:18,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   5%|         | 23/440 [00:57<17:12,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   5%|         | 24/440 [00:59<17:11,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   6%|         | 25/440 [01:02<17:11,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   6%|         | 26/440 [01:04<17:10,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   6%|         | 27/440 [01:07<17:09,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   6%|         | 28/440 [01:10<17:47,  2.59s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   7%|         | 29/440 [01:12<17:31,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   7%|         | 30/440 [01:15<17:24,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   7%|         | 31/440 [01:17<17:13,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   7%|         | 32/440 [01:20<17:05,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   8%|         | 33/440 [01:22<17:00,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   8%|         | 34/440 [01:24<16:56,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   8%|         | 35/440 [01:27<16:54,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   8%|         | 36/440 [01:29<16:48,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   8%|         | 37/440 [01:32<16:42,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   9%|         | 38/440 [01:34<16:38,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   9%|         | 39/440 [01:37<16:37,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   9%|         | 40/440 [01:39<16:35,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   9%|         | 41/440 [01:42<16:32,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  10%|         | 42/440 [01:44<16:29,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  10%|         | 43/440 [01:47<16:25,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  10%|         | 44/440 [01:49<16:23,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  10%|         | 45/440 [01:52<16:25,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  10%|         | 46/440 [01:54<16:21,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  11%|         | 47/440 [01:57<16:17,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  11%|         | 48/440 [01:59<16:12,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  11%|         | 49/440 [02:02<16:11,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  11%|        | 50/440 [02:04<16:11,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  12%|        | 51/440 [02:07<16:12,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  12%|        | 52/440 [02:09<16:07,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  12%|        | 53/440 [02:12<16:03,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  12%|        | 54/440 [02:14<16:00,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  12%|        | 55/440 [02:17<16:04,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  13%|        | 56/440 [02:19<15:58,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  13%|        | 57/440 [02:22<15:54,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  13%|        | 58/440 [02:24<15:51,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  13%|        | 59/440 [02:27<15:46,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  14%|        | 60/440 [02:29<15:55,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  14%|        | 61/440 [02:32<15:49,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  14%|        | 62/440 [02:34<15:41,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  14%|        | 63/440 [02:37<15:38,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  15%|        | 64/440 [02:39<15:34,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  15%|        | 65/440 [02:42<15:34,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  15%|        | 66/440 [02:45<16:07,  2.59s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  15%|        | 67/440 [02:47<15:54,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  15%|        | 68/440 [02:49<15:44,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  16%|        | 69/440 [02:52<15:39,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  16%|        | 70/440 [02:54<15:30,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  16%|        | 71/440 [02:57<15:24,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  16%|        | 72/440 [02:59<15:18,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  17%|        | 73/440 [03:02<15:12,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  17%|        | 74/440 [03:04<15:11,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  17%|        | 75/440 [03:07<15:12,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  17%|        | 76/440 [03:09<15:07,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  18%|        | 77/440 [03:12<15:03,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  18%|        | 78/440 [03:14<15:02,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  18%|        | 79/440 [03:17<15:01,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  18%|        | 80/440 [03:19<14:55,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  18%|        | 81/440 [03:22<14:54,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  19%|        | 82/440 [03:24<14:51,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  19%|        | 83/440 [03:27<14:48,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  19%|        | 84/440 [03:29<14:52,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  19%|        | 85/440 [03:32<14:48,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  20%|        | 86/440 [03:34<14:43,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  20%|        | 87/440 [03:37<14:42,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  20%|        | 88/440 [03:39<14:37,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  20%|        | 89/440 [03:42<14:38,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  20%|        | 90/440 [03:44<14:35,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  21%|        | 91/440 [03:47<14:31,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  21%|        | 92/440 [03:49<14:27,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  21%|        | 93/440 [03:52<14:27,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  21%|       | 94/440 [03:54<14:29,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  22%|       | 95/440 [03:57<14:23,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  22%|       | 96/440 [03:59<14:16,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  22%|       | 97/440 [04:02<14:13,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  22%|       | 98/440 [04:04<14:14,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  22%|       | 99/440 [04:07<14:19,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  23%|       | 100/440 [04:09<14:14,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  23%|       | 101/440 [04:12<14:09,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  23%|       | 102/440 [04:15<14:39,  2.60s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  23%|       | 103/440 [04:17<14:23,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  24%|       | 104/440 [04:20<14:14,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  24%|       | 105/440 [04:22<14:06,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  24%|       | 106/440 [04:25<13:59,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  24%|       | 107/440 [04:27<13:55,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  25%|       | 108/440 [04:30<13:52,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  25%|       | 109/440 [04:32<13:49,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  25%|       | 110/440 [04:35<13:46,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  25%|       | 111/440 [04:37<13:44,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  25%|       | 112/440 [04:40<13:39,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  26%|       | 113/440 [04:42<13:36,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  26%|       | 114/440 [04:45<13:35,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  26%|       | 115/440 [04:47<13:34,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  26%|       | 116/440 [04:50<13:29,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  27%|       | 117/440 [04:52<13:25,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  27%|       | 118/440 [04:55<13:23,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  27%|       | 119/440 [04:57<13:20,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  27%|       | 120/440 [05:00<13:15,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  28%|       | 121/440 [05:02<13:09,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  28%|       | 122/440 [05:05<13:10,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  28%|       | 123/440 [05:07<13:13,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  28%|       | 124/440 [05:10<13:08,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  28%|       | 125/440 [05:12<13:02,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  29%|       | 126/440 [05:15<12:58,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  29%|       | 127/440 [05:17<12:57,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  29%|       | 128/440 [05:20<12:59,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  29%|       | 129/440 [05:22<12:55,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  30%|       | 130/440 [05:25<12:52,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  30%|       | 131/440 [05:27<12:50,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  30%|       | 132/440 [05:30<12:48,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  30%|       | 133/440 [05:32<12:47,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  30%|       | 134/440 [05:34<12:42,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  31%|       | 135/440 [05:37<12:42,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  31%|       | 136/440 [05:40<12:39,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  31%|       | 137/440 [05:42<12:35,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  31%|      | 138/440 [05:44<12:32,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  32%|      | 139/440 [05:47<12:29,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  32%|      | 140/440 [05:49<12:27,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  32%|      | 141/440 [05:52<12:26,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  32%|      | 142/440 [05:54<12:23,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  32%|      | 143/440 [05:57<12:50,  2.59s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  33%|      | 144/440 [06:00<12:37,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  33%|      | 145/440 [06:02<12:28,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  33%|      | 146/440 [06:05<12:20,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  33%|      | 147/440 [06:07<12:15,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  34%|      | 148/440 [06:10<12:11,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  34%|      | 149/440 [06:12<12:06,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  34%|      | 150/440 [06:15<12:03,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  34%|      | 151/440 [06:17<11:58,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  35%|      | 152/440 [06:20<11:55,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  35%|      | 153/440 [06:22<11:56,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  35%|      | 154/440 [06:25<11:52,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  35%|      | 155/440 [06:27<11:49,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  35%|      | 156/440 [06:30<11:45,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  36%|      | 157/440 [06:32<11:42,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  36%|      | 158/440 [06:35<11:43,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  36%|      | 159/440 [06:37<11:43,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  36%|      | 160/440 [06:40<11:38,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  37%|      | 161/440 [06:42<11:34,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  37%|      | 162/440 [06:45<11:31,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  37%|      | 163/440 [06:47<11:29,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  37%|      | 164/440 [06:50<11:25,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  38%|      | 165/440 [06:52<11:22,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  38%|      | 166/440 [06:54<11:22,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  38%|      | 167/440 [06:57<11:21,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  38%|      | 168/440 [06:59<11:17,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  38%|      | 169/440 [07:02<11:13,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  39%|      | 170/440 [07:04<11:09,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  39%|      | 171/440 [07:07<11:08,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  39%|      | 172/440 [07:09<11:07,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  39%|      | 173/440 [07:12<11:03,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  40%|      | 174/440 [07:14<11:00,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  40%|      | 175/440 [07:17<10:58,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  40%|      | 176/440 [07:19<10:55,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  40%|      | 177/440 [07:22<10:56,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  40%|      | 178/440 [07:24<10:51,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  41%|      | 179/440 [07:27<10:49,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  41%|      | 180/440 [07:30<11:11,  2.58s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  41%|      | 181/440 [07:32<11:02,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  41%|     | 182/440 [07:35<10:56,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  42%|     | 183/440 [07:37<10:50,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  42%|     | 184/440 [07:40<10:43,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  42%|     | 185/440 [07:42<10:36,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  42%|     | 186/440 [07:45<10:33,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  42%|     | 187/440 [07:47<10:32,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  43%|     | 188/440 [07:50<10:28,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  43%|     | 189/440 [07:52<10:26,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  43%|     | 190/440 [07:55<10:24,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  43%|     | 191/440 [07:57<10:21,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  44%|     | 192/440 [08:00<10:20,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  44%|     | 193/440 [08:02<10:16,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  44%|     | 194/440 [08:05<10:12,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  44%|     | 195/440 [08:07<10:11,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  45%|     | 196/440 [08:10<10:07,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  45%|     | 197/440 [08:12<10:09,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  45%|     | 198/440 [08:15<10:06,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  45%|     | 199/440 [08:17<10:01,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  45%|     | 200/440 [08:20<09:58,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  46%|     | 201/440 [08:22<09:56,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  46%|     | 202/440 [08:25<09:58,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  46%|     | 203/440 [08:27<09:54,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  46%|     | 204/440 [08:30<09:50,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  47%|     | 205/440 [08:32<09:48,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  47%|     | 206/440 [08:35<09:44,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  47%|     | 207/440 [08:37<09:45,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  47%|     | 208/440 [08:40<09:40,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  48%|     | 209/440 [08:42<09:36,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  48%|     | 210/440 [08:45<09:33,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  48%|     | 211/440 [08:47<09:32,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  48%|     | 212/440 [08:50<09:31,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  48%|     | 213/440 [08:52<09:27,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  49%|     | 214/440 [08:55<09:23,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  49%|     | 215/440 [08:57<09:24,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  49%|     | 216/440 [09:00<09:24,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  49%|     | 217/440 [09:02<09:18,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  50%|     | 218/440 [09:05<09:17,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  50%|     | 219/440 [09:07<09:14,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  50%|     | 220/440 [09:10<09:09,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  50%|     | 221/440 [09:12<09:30,  2.61s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  50%|     | 222/440 [09:15<09:21,  2.58s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  51%|     | 223/440 [09:17<09:13,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  51%|     | 224/440 [09:20<09:04,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  51%|     | 225/440 [09:22<08:59,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  51%|    | 226/440 [09:25<08:55,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  52%|    | 227/440 [09:27<08:51,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  52%|    | 228/440 [09:30<08:46,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  52%|    | 229/440 [09:32<08:46,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  52%|    | 230/440 [09:35<08:45,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  52%|    | 231/440 [09:37<08:44,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  53%|    | 232/440 [09:40<08:39,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  53%|    | 233/440 [09:42<08:37,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  53%|    | 234/440 [09:45<08:34,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  53%|    | 235/440 [09:47<08:31,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  54%|    | 236/440 [09:50<08:30,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  54%|    | 237/440 [09:52<08:27,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  54%|    | 238/440 [09:55<08:24,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  54%|    | 239/440 [09:57<08:20,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  55%|    | 240/440 [10:00<08:17,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  55%|    | 241/440 [10:02<08:17,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  55%|    | 242/440 [10:05<08:12,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  55%|    | 243/440 [10:07<08:10,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  55%|    | 244/440 [10:10<08:06,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  56%|    | 245/440 [10:12<08:04,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  56%|    | 246/440 [10:15<08:01,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  56%|    | 247/440 [10:17<07:57,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  56%|    | 248/440 [10:20<07:55,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  57%|    | 249/440 [10:22<07:55,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  57%|    | 250/440 [10:25<07:52,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  57%|    | 251/440 [10:27<07:51,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  57%|    | 252/440 [10:30<07:47,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  57%|    | 253/440 [10:32<07:44,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  58%|    | 254/440 [10:35<07:41,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  58%|    | 255/440 [10:37<07:40,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  58%|    | 256/440 [10:40<07:39,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  58%|    | 257/440 [10:42<07:35,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  59%|    | 258/440 [10:45<07:51,  2.59s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  59%|    | 259/440 [10:47<07:44,  2.57s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  59%|    | 260/440 [10:50<07:38,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  59%|    | 261/440 [10:52<07:34,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  60%|    | 262/440 [10:55<07:28,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  60%|    | 263/440 [10:57<07:24,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  60%|    | 264/440 [11:00<07:21,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  60%|    | 265/440 [11:02<07:18,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  60%|    | 266/440 [11:05<07:14,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  61%|    | 267/440 [11:07<07:13,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  61%|    | 268/440 [11:10<07:10,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  61%|    | 269/440 [11:12<07:07,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  61%|   | 270/440 [11:15<07:04,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  62%|   | 271/440 [11:17<07:00,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  62%|   | 272/440 [11:20<06:57,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  62%|   | 273/440 [11:22<06:54,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  62%|   | 274/440 [11:25<06:51,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  62%|   | 275/440 [11:27<06:50,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  63%|   | 276/440 [11:30<06:47,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  63%|   | 277/440 [11:32<06:45,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  63%|   | 278/440 [11:35<06:42,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  63%|   | 279/440 [11:37<06:40,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  64%|   | 280/440 [11:40<06:37,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  64%|   | 281/440 [11:42<06:36,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  64%|   | 282/440 [11:45<06:33,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  64%|   | 283/440 [11:47<06:30,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  65%|   | 284/440 [11:50<06:28,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  65%|   | 285/440 [11:52<06:26,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  65%|   | 286/440 [11:55<06:24,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  65%|   | 287/440 [11:57<06:20,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  65%|   | 288/440 [12:00<06:18,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  66%|   | 289/440 [12:02<06:14,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  66%|   | 290/440 [12:05<06:14,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  66%|   | 291/440 [12:07<06:12,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  66%|   | 292/440 [12:10<06:10,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  67%|   | 293/440 [12:12<06:06,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  67%|   | 294/440 [12:15<06:03,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  67%|   | 295/440 [12:17<06:16,  2.59s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  67%|   | 296/440 [12:20<06:08,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  68%|   | 297/440 [12:22<06:03,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  68%|   | 298/440 [12:25<05:58,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  68%|   | 299/440 [12:27<05:55,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  68%|   | 300/440 [12:30<05:51,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  68%|   | 301/440 [12:32<05:47,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  69%|   | 302/440 [12:35<05:44,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  69%|   | 303/440 [12:37<05:42,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  69%|   | 304/440 [12:40<05:40,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  69%|   | 305/440 [12:42<05:36,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  70%|   | 306/440 [12:45<05:33,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  70%|   | 307/440 [12:47<05:30,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  70%|   | 308/440 [12:50<05:27,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  70%|   | 309/440 [12:52<05:26,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  70%|   | 310/440 [12:55<05:23,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  71%|   | 311/440 [12:57<05:20,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  71%|   | 312/440 [13:00<05:18,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  71%|   | 313/440 [13:02<05:16,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  71%|  | 314/440 [13:05<05:14,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  72%|  | 315/440 [13:07<05:12,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  72%|  | 316/440 [13:10<05:08,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  72%|  | 317/440 [13:12<05:06,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  72%|  | 318/440 [13:15<05:04,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  72%|  | 319/440 [13:17<05:01,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  73%|  | 320/440 [13:20<04:58,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  73%|  | 321/440 [13:22<04:56,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  73%|  | 322/440 [13:25<04:53,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  73%|  | 323/440 [13:27<04:50,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  74%|  | 324/440 [13:30<04:49,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  74%|  | 325/440 [13:32<04:46,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  74%|  | 326/440 [13:35<04:43,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  74%|  | 327/440 [13:37<04:40,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  75%|  | 328/440 [13:40<04:38,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  75%|  | 329/440 [13:42<04:36,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  75%|  | 330/440 [13:45<04:34,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  75%|  | 331/440 [13:47<04:43,  2.60s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  75%|  | 332/440 [13:50<04:36,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  76%|  | 333/440 [13:52<04:31,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  76%|  | 334/440 [13:55<04:28,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  76%|  | 335/440 [13:57<04:23,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  76%|  | 336/440 [14:00<04:20,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  77%|  | 337/440 [14:02<04:17,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  77%|  | 338/440 [14:05<04:13,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  77%|  | 339/440 [14:07<04:13,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  77%|  | 340/440 [14:10<04:09,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  78%|  | 341/440 [14:12<04:07,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  78%|  | 342/440 [14:15<04:04,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  78%|  | 343/440 [14:17<04:02,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  78%|  | 344/440 [14:20<03:59,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  78%|  | 345/440 [14:22<03:58,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  79%|  | 346/440 [14:25<03:55,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  79%|  | 347/440 [14:27<03:52,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  79%|  | 348/440 [14:30<03:50,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  79%|  | 349/440 [14:32<03:47,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  80%|  | 350/440 [14:35<03:44,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  80%|  | 351/440 [14:37<03:42,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  80%|  | 352/440 [14:40<03:39,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  80%|  | 353/440 [14:42<03:37,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  80%|  | 354/440 [14:45<03:34,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  81%|  | 355/440 [14:47<03:31,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  81%|  | 356/440 [14:50<03:28,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  81%|  | 357/440 [14:52<03:27,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  81%| | 358/440 [14:55<03:25,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  82%| | 359/440 [14:57<03:22,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  82%| | 360/440 [15:00<03:19,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  82%| | 361/440 [15:02<03:16,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  82%| | 362/440 [15:05<03:14,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  82%| | 363/440 [15:07<03:12,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  83%| | 364/440 [15:10<03:09,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  83%| | 365/440 [15:12<03:08,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  83%| | 366/440 [15:15<03:05,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  83%| | 367/440 [15:17<03:02,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  84%| | 368/440 [15:20<02:59,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  84%| | 369/440 [15:22<02:57,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  84%| | 370/440 [15:25<02:54,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  84%| | 371/440 [15:28<02:59,  2.60s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  85%| | 372/440 [15:30<02:54,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  85%| | 373/440 [15:33<02:51,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  85%| | 374/440 [15:35<02:47,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  85%| | 375/440 [15:38<02:44,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  85%| | 376/440 [15:40<02:41,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  86%| | 377/440 [15:43<02:39,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  86%| | 378/440 [15:45<02:37,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  86%| | 379/440 [15:48<02:34,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  86%| | 380/440 [15:50<02:30,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  87%| | 381/440 [15:53<02:27,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  87%| | 382/440 [15:55<02:25,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  87%| | 383/440 [15:58<02:23,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  87%| | 384/440 [16:00<02:21,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  88%| | 385/440 [16:03<02:18,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  88%| | 386/440 [16:05<02:16,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  88%| | 387/440 [16:08<02:14,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  88%| | 388/440 [16:10<02:11,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  88%| | 389/440 [16:13<02:08,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  89%| | 390/440 [16:15<02:05,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  89%| | 391/440 [16:18<02:02,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  89%| | 392/440 [16:20<02:00,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  89%| | 393/440 [16:23<01:58,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  90%| | 394/440 [16:25<01:55,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  90%| | 395/440 [16:28<01:52,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  90%| | 396/440 [16:30<01:49,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  90%| | 397/440 [16:33<01:47,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  90%| | 398/440 [16:35<01:44,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  91%| | 399/440 [16:38<01:42,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  91%| | 400/440 [16:40<01:39,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  91%| | 401/440 [16:43<01:37,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  91%|| 402/440 [16:45<01:35,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  92%|| 403/440 [16:48<01:32,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  92%|| 404/440 [16:50<01:29,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  92%|| 405/440 [16:53<01:27,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  92%|| 406/440 [16:55<01:24,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  92%|| 407/440 [16:58<01:25,  2.61s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  93%|| 408/440 [17:01<01:22,  2.57s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  93%|| 409/440 [17:03<01:18,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  93%|| 410/440 [17:06<01:15,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  93%|| 411/440 [17:08<01:13,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  94%|| 412/440 [17:11<01:10,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  94%|| 413/440 [17:13<01:07,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  94%|| 414/440 [17:16<01:05,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  94%|| 415/440 [17:18<01:02,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  95%|| 416/440 [17:21<00:59,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  95%|| 417/440 [17:23<00:57,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  95%|| 418/440 [17:26<00:55,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  95%|| 419/440 [17:28<00:52,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  95%|| 420/440 [17:31<00:49,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  96%|| 421/440 [17:33<00:47,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  96%|| 422/440 [17:36<00:45,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  96%|| 423/440 [17:38<00:42,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  96%|| 424/440 [17:41<00:39,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  97%|| 425/440 [17:43<00:37,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  97%|| 426/440 [17:46<00:35,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  97%|| 427/440 [17:48<00:32,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  97%|| 428/440 [17:51<00:30,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  98%|| 429/440 [17:53<00:27,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  98%|| 430/440 [17:56<00:25,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  98%|| 431/440 [17:58<00:22,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  98%|| 432/440 [18:01<00:19,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  98%|| 433/440 [18:03<00:17,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  99%|| 434/440 [18:06<00:14,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  99%|| 435/440 [18:08<00:12,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  99%|| 436/440 [18:11<00:09,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  99%|| 437/440 [18:13<00:07,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress: 100%|| 438/440 [18:16<00:05,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress: 100%|| 439/440 [18:18<00:02,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress: 100%|| 440/440 [18:21<00:00,  2.50s/it]\n",
            "\n",
            "Validation Progress:   0%|          | 0/110 [00:00<?, ?it/s]\u001b[A\n",
            "Validation Progress:   1%|          | 1/110 [00:00<01:26,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   2%|         | 2/110 [00:01<01:25,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   3%|         | 3/110 [00:02<01:24,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   4%|         | 4/110 [00:03<01:23,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   5%|         | 5/110 [00:03<01:22,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   5%|         | 6/110 [00:04<01:21,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   6%|         | 7/110 [00:05<01:20,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   7%|         | 8/110 [00:06<01:20,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   8%|         | 9/110 [00:07<01:19,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   9%|         | 10/110 [00:07<01:18,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  10%|         | 11/110 [00:08<01:17,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  11%|         | 12/110 [00:09<01:16,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  12%|        | 13/110 [00:10<01:15,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  13%|        | 14/110 [00:11<01:15,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  14%|        | 15/110 [00:11<01:14,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  15%|        | 16/110 [00:12<01:14,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  15%|        | 17/110 [00:13<01:13,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  16%|        | 18/110 [00:14<01:12,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  17%|        | 19/110 [00:14<01:12,  1.25it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  18%|        | 20/110 [00:15<01:11,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  19%|        | 21/110 [00:16<01:10,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  20%|        | 22/110 [00:17<01:09,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  21%|        | 23/110 [00:18<01:08,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  22%|       | 24/110 [00:18<01:07,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  23%|       | 25/110 [00:19<01:06,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  24%|       | 26/110 [00:20<01:06,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  25%|       | 27/110 [00:21<01:05,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  25%|       | 28/110 [00:22<01:04,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  26%|       | 29/110 [00:22<01:03,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  27%|       | 30/110 [00:23<01:03,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  28%|       | 31/110 [00:24<01:02,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  29%|       | 32/110 [00:25<01:01,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  30%|       | 33/110 [00:26<01:01,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  31%|       | 34/110 [00:26<01:00,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  32%|      | 35/110 [00:27<00:59,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  33%|      | 36/110 [00:28<00:58,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  34%|      | 37/110 [00:29<00:57,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  35%|      | 38/110 [00:29<00:56,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  35%|      | 39/110 [00:30<00:56,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  36%|      | 40/110 [00:31<00:55,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  37%|      | 41/110 [00:32<00:54,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  38%|      | 42/110 [00:33<00:53,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  39%|      | 43/110 [00:33<00:52,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  40%|      | 44/110 [00:34<00:51,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  41%|      | 45/110 [00:35<00:51,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  42%|     | 46/110 [00:36<00:50,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  43%|     | 47/110 [00:37<00:49,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  44%|     | 48/110 [00:37<00:48,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  45%|     | 49/110 [00:38<00:48,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  45%|     | 50/110 [00:39<00:47,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  46%|     | 51/110 [00:40<00:46,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  47%|     | 52/110 [00:40<00:45,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  48%|     | 53/110 [00:41<00:44,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  49%|     | 54/110 [00:42<00:43,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  50%|     | 55/110 [00:43<00:42,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  51%|     | 56/110 [00:44<00:42,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  52%|    | 57/110 [00:44<00:41,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  53%|    | 58/110 [00:45<00:40,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  54%|    | 59/110 [00:46<00:39,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  55%|    | 60/110 [00:47<00:39,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  55%|    | 61/110 [00:47<00:38,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  56%|    | 62/110 [00:48<00:37,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  57%|    | 63/110 [00:49<00:36,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  58%|    | 64/110 [00:50<00:35,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  59%|    | 65/110 [00:51<00:35,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  60%|    | 66/110 [00:51<00:34,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  61%|    | 67/110 [00:52<00:33,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  62%|   | 68/110 [00:53<00:32,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  63%|   | 69/110 [00:54<00:32,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  64%|   | 70/110 [00:55<00:31,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  65%|   | 71/110 [00:55<00:30,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  65%|   | 72/110 [00:56<00:29,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  66%|   | 73/110 [00:57<00:29,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  67%|   | 74/110 [00:58<00:28,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  68%|   | 75/110 [00:58<00:27,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  69%|   | 76/110 [00:59<00:26,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  70%|   | 77/110 [01:00<00:25,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  71%|   | 78/110 [01:01<00:25,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  72%|  | 79/110 [01:02<00:24,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  73%|  | 80/110 [01:02<00:23,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  74%|  | 81/110 [01:03<00:22,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  75%|  | 82/110 [01:04<00:22,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  75%|  | 83/110 [01:05<00:21,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  76%|  | 84/110 [01:06<00:20,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  77%|  | 85/110 [01:06<00:19,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  78%|  | 86/110 [01:07<00:18,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  79%|  | 87/110 [01:08<00:18,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  80%|  | 88/110 [01:09<00:17,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  81%|  | 89/110 [01:09<00:16,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  82%| | 90/110 [01:10<00:15,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  83%| | 91/110 [01:11<00:14,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  84%| | 92/110 [01:12<00:14,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  85%| | 93/110 [01:13<00:13,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  85%| | 94/110 [01:13<00:12,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  86%| | 95/110 [01:14<00:11,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  87%| | 96/110 [01:15<00:11,  1.25it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  88%| | 97/110 [01:16<00:10,  1.25it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  89%| | 98/110 [01:17<00:09,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  90%| | 99/110 [01:17<00:08,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  91%| | 100/110 [01:18<00:07,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  92%|| 101/110 [01:19<00:07,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  93%|| 102/110 [01:20<00:06,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  94%|| 103/110 [01:21<00:05,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  95%|| 104/110 [01:21<00:04,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  95%|| 105/110 [01:22<00:03,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  96%|| 106/110 [01:23<00:03,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  97%|| 107/110 [01:24<00:02,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  98%|| 108/110 [01:24<00:01,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  99%|| 109/110 [01:25<00:00,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress: 100%|| 110/110 [01:26<00:00,  1.27it/s]\n",
            "Epochs:  24%|       | 6/25 [1:58:52<6:16:06, 1187.69s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n",
            "[Epoch 6]\n",
            "  [Training] Loss: 0.2938, Accuracy: 92.47%\n",
            "  [Validation] Loss: 0.5811, Accuracy: 72.95%\n",
            "  [INFO] No improvement in validation loss. Patience counter: 3/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   0%|          | 0/440 [00:00<?, ?it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   0%|          | 1/440 [00:02<18:19,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   0%|          | 2/440 [00:05<18:15,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   1%|          | 3/440 [00:07<18:07,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   1%|          | 4/440 [00:09<18:02,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   1%|          | 5/440 [00:12<17:55,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   1%|         | 6/440 [00:14<17:59,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   2%|         | 7/440 [00:17<18:42,  2.59s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   2%|         | 8/440 [00:20<18:24,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   2%|         | 9/440 [00:22<18:12,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   2%|         | 10/440 [00:25<18:03,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   2%|         | 11/440 [00:27<18:05,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   3%|         | 12/440 [00:30<17:55,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   3%|         | 13/440 [00:32<17:50,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   3%|         | 14/440 [00:35<17:45,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   3%|         | 15/440 [00:37<17:44,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   4%|         | 16/440 [00:40<17:50,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   4%|         | 17/440 [00:42<17:44,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   4%|         | 18/440 [00:45<17:41,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   4%|         | 19/440 [00:47<17:38,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   5%|         | 20/440 [00:50<17:44,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   5%|         | 21/440 [00:52<17:45,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   5%|         | 22/440 [00:55<17:38,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   5%|         | 23/440 [00:57<17:32,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   5%|         | 24/440 [01:00<17:29,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   6%|         | 25/440 [01:03<17:27,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   6%|         | 26/440 [01:05<17:21,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   6%|         | 27/440 [01:07<17:14,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   6%|         | 28/440 [01:10<17:10,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   7%|         | 29/440 [01:13<17:11,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   7%|         | 30/440 [01:15<17:10,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   7%|         | 31/440 [01:18<17:08,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   7%|         | 32/440 [01:20<17:07,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   8%|         | 33/440 [01:23<17:02,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   8%|         | 34/440 [01:25<16:58,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   8%|         | 35/440 [01:28<16:54,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   8%|         | 36/440 [01:30<16:50,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   8%|         | 37/440 [01:33<16:47,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   9%|         | 38/440 [01:35<16:44,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   9%|         | 39/440 [01:38<16:42,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   9%|         | 40/440 [01:40<16:47,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   9%|         | 41/440 [01:43<16:46,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  10%|         | 42/440 [01:45<16:40,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  10%|         | 43/440 [01:48<16:33,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  10%|         | 44/440 [01:50<16:37,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  10%|         | 45/440 [01:53<16:31,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  10%|         | 46/440 [01:55<16:28,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  11%|         | 47/440 [01:58<17:02,  2.60s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  11%|         | 48/440 [02:01<16:52,  2.58s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  11%|         | 49/440 [02:03<16:39,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  11%|        | 50/440 [02:06<16:34,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  12%|        | 51/440 [02:08<16:24,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  12%|        | 52/440 [02:11<16:14,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  12%|        | 53/440 [02:13<16:09,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  12%|        | 54/440 [02:15<16:04,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  12%|        | 55/440 [02:18<16:07,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  13%|        | 56/440 [02:21<16:04,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  13%|        | 57/440 [02:23<15:59,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  13%|        | 58/440 [02:26<16:02,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  13%|        | 59/440 [02:28<15:58,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  14%|        | 60/440 [02:31<15:53,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  14%|        | 61/440 [02:33<15:50,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  14%|        | 62/440 [02:36<15:50,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  14%|        | 63/440 [02:38<15:44,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  15%|        | 64/440 [02:41<15:42,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  15%|        | 65/440 [02:43<15:39,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  15%|        | 66/440 [02:46<15:41,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  15%|        | 67/440 [02:48<15:37,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  15%|        | 68/440 [02:51<15:32,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  16%|        | 69/440 [02:53<15:34,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  16%|        | 70/440 [02:56<15:30,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  16%|        | 71/440 [02:58<15:25,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  16%|        | 72/440 [03:01<15:22,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  17%|        | 73/440 [03:03<15:21,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  17%|        | 74/440 [03:06<15:17,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  17%|        | 75/440 [03:08<15:14,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  17%|        | 76/440 [03:11<15:07,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  18%|        | 77/440 [03:13<15:04,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  18%|        | 78/440 [03:16<15:01,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  18%|        | 79/440 [03:18<15:01,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  18%|        | 80/440 [03:21<14:58,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  18%|        | 81/440 [03:23<14:56,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  19%|        | 82/440 [03:26<14:53,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  19%|        | 83/440 [03:28<15:28,  2.60s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  19%|        | 84/440 [03:31<15:17,  2.58s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  19%|        | 85/440 [03:33<15:04,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  20%|        | 86/440 [03:36<14:55,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  20%|        | 87/440 [03:38<14:51,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  20%|        | 88/440 [03:41<14:45,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  20%|        | 89/440 [03:44<14:43,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  20%|        | 90/440 [03:46<14:38,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  21%|        | 91/440 [03:48<14:33,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  21%|        | 92/440 [03:51<14:30,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  21%|        | 93/440 [03:53<14:28,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  21%|       | 94/440 [03:56<14:32,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  22%|       | 95/440 [03:59<14:27,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  22%|       | 96/440 [04:01<14:22,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  22%|       | 97/440 [04:04<14:20,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  22%|       | 98/440 [04:06<14:25,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  22%|       | 99/440 [04:09<14:19,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  23%|       | 100/440 [04:11<14:14,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  23%|       | 101/440 [04:14<14:07,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  23%|       | 102/440 [04:16<14:05,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  23%|       | 103/440 [04:19<14:04,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  24%|       | 104/440 [04:21<14:00,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  24%|       | 105/440 [04:24<14:03,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  24%|       | 106/440 [04:26<14:02,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  24%|       | 107/440 [04:29<13:57,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  25%|       | 108/440 [04:31<13:53,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  25%|       | 109/440 [04:34<13:53,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  25%|       | 110/440 [04:36<13:48,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  25%|       | 111/440 [04:39<13:42,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  25%|       | 112/440 [04:41<13:38,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  26%|       | 113/440 [04:44<13:36,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  26%|       | 114/440 [04:46<13:34,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  26%|       | 115/440 [04:49<13:30,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  26%|       | 116/440 [04:51<13:26,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  27%|       | 117/440 [04:54<13:22,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  27%|       | 118/440 [04:56<13:25,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  27%|       | 119/440 [04:59<13:21,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  27%|       | 120/440 [05:01<13:19,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  28%|       | 121/440 [05:04<13:18,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  28%|       | 122/440 [05:06<13:13,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  28%|       | 123/440 [05:09<13:13,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  28%|       | 124/440 [05:12<13:41,  2.60s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  28%|       | 125/440 [05:14<13:27,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  29%|       | 126/440 [05:16<13:18,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  29%|       | 127/440 [05:19<13:12,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  29%|       | 128/440 [05:21<13:04,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  29%|       | 129/440 [05:24<13:00,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  30%|       | 130/440 [05:26<12:55,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  30%|       | 131/440 [05:29<12:51,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  30%|       | 132/440 [05:31<12:51,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  30%|       | 133/440 [05:34<12:50,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  30%|       | 134/440 [05:36<12:44,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  31%|       | 135/440 [05:39<12:40,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  31%|       | 136/440 [05:41<12:36,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  31%|       | 137/440 [05:44<12:34,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  31%|      | 138/440 [05:46<12:34,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  32%|      | 139/440 [05:49<12:39,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  32%|      | 140/440 [05:51<12:33,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  32%|      | 141/440 [05:54<12:25,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  32%|      | 142/440 [05:56<12:25,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  32%|      | 143/440 [05:59<12:20,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  33%|      | 144/440 [06:01<12:17,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  33%|      | 145/440 [06:04<12:15,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  33%|      | 146/440 [06:06<12:13,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  33%|      | 147/440 [06:09<12:11,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  34%|      | 148/440 [06:11<12:07,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  34%|      | 149/440 [06:14<12:05,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  34%|      | 150/440 [06:16<12:01,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  34%|      | 151/440 [06:19<12:00,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  35%|      | 152/440 [06:21<12:03,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  35%|      | 153/440 [06:24<11:58,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  35%|      | 154/440 [06:26<11:57,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  35%|      | 155/440 [06:29<11:54,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  35%|      | 156/440 [06:31<11:50,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  36%|      | 157/440 [06:34<11:48,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  36%|      | 158/440 [06:36<11:45,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  36%|      | 159/440 [06:39<11:41,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  36%|      | 160/440 [06:41<11:40,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  37%|      | 161/440 [06:44<12:07,  2.61s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  37%|      | 162/440 [06:47<12:00,  2.59s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  37%|      | 163/440 [06:49<11:50,  2.57s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  37%|      | 164/440 [06:52<11:41,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  38%|      | 165/440 [06:54<11:36,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  38%|      | 166/440 [06:57<11:38,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  38%|      | 167/440 [06:59<11:31,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  38%|      | 168/440 [07:02<11:25,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  38%|      | 169/440 [07:04<11:21,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  39%|      | 170/440 [07:07<11:17,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  39%|      | 171/440 [07:09<11:12,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  39%|      | 172/440 [07:12<11:07,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  39%|      | 173/440 [07:14<11:03,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  40%|      | 174/440 [07:17<11:01,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  40%|      | 175/440 [07:19<11:01,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  40%|      | 176/440 [07:22<10:59,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  40%|      | 177/440 [07:24<11:00,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  40%|      | 178/440 [07:27<10:55,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  41%|      | 179/440 [07:29<10:51,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  41%|      | 180/440 [07:32<10:51,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  41%|      | 181/440 [07:34<10:50,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  41%|     | 182/440 [07:37<10:43,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  42%|     | 183/440 [07:39<10:39,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  42%|     | 184/440 [07:42<10:36,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  42%|     | 185/440 [07:44<10:36,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  42%|     | 186/440 [07:47<10:37,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  42%|     | 187/440 [07:49<10:33,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  43%|     | 188/440 [07:52<10:28,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  43%|     | 189/440 [07:54<10:23,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  43%|     | 190/440 [07:57<10:21,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  43%|     | 191/440 [07:59<10:20,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  44%|     | 192/440 [08:02<10:16,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  44%|     | 193/440 [08:04<10:17,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  44%|     | 194/440 [08:07<10:13,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  44%|     | 195/440 [08:09<10:10,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  45%|     | 196/440 [08:12<10:11,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  45%|     | 197/440 [08:14<10:06,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  45%|     | 198/440 [08:17<10:02,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  45%|     | 199/440 [08:19<10:03,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  45%|     | 200/440 [08:22<10:01,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  46%|     | 201/440 [08:24<09:58,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  46%|     | 202/440 [08:27<10:18,  2.60s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  46%|     | 203/440 [08:30<10:06,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  46%|     | 204/440 [08:32<09:58,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  47%|     | 205/440 [08:35<09:55,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  47%|     | 206/440 [08:37<09:49,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  47%|     | 207/440 [08:40<09:44,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  47%|     | 208/440 [08:42<09:41,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  48%|     | 209/440 [08:45<09:36,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  48%|     | 210/440 [08:47<09:36,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  48%|     | 211/440 [08:50<09:33,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  48%|     | 212/440 [08:52<09:29,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  48%|     | 213/440 [08:55<09:27,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  49%|     | 214/440 [08:57<09:23,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  49%|     | 215/440 [09:00<09:23,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  49%|     | 216/440 [09:02<09:19,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  49%|     | 217/440 [09:05<09:16,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  50%|     | 218/440 [09:07<09:14,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  50%|     | 219/440 [09:10<09:12,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  50%|     | 220/440 [09:12<09:07,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  50%|     | 221/440 [09:14<09:04,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  50%|     | 222/440 [09:17<09:02,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  51%|     | 223/440 [09:19<09:01,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  51%|     | 224/440 [09:22<09:00,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  51%|     | 225/440 [09:25<08:57,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  51%|    | 226/440 [09:27<08:53,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  52%|    | 227/440 [09:29<08:50,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  52%|    | 228/440 [09:32<08:47,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  52%|    | 229/440 [09:34<08:48,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  52%|    | 230/440 [09:37<08:43,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  52%|    | 231/440 [09:39<08:41,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  53%|    | 232/440 [09:42<08:37,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  53%|    | 233/440 [09:44<08:35,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  53%|    | 234/440 [09:47<08:33,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  53%|    | 235/440 [09:49<08:32,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  54%|    | 236/440 [09:52<08:27,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  54%|    | 237/440 [09:54<08:24,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  54%|    | 238/440 [09:57<08:43,  2.59s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  54%|    | 239/440 [10:00<08:35,  2.57s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  55%|    | 240/440 [10:02<08:30,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  55%|    | 241/440 [10:05<08:24,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  55%|    | 242/440 [10:07<08:20,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  55%|    | 243/440 [10:10<08:16,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  55%|    | 244/440 [10:12<08:12,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  56%|    | 245/440 [10:15<08:09,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  56%|    | 246/440 [10:17<08:06,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  56%|    | 247/440 [10:20<08:05,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  56%|    | 248/440 [10:22<08:03,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  57%|    | 249/440 [10:25<07:59,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  57%|    | 250/440 [10:27<07:56,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  57%|    | 251/440 [10:30<07:55,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  57%|    | 252/440 [10:32<07:50,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  57%|    | 253/440 [10:35<07:48,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  58%|    | 254/440 [10:37<07:44,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  58%|    | 255/440 [10:40<07:41,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  58%|    | 256/440 [10:42<07:37,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  58%|    | 257/440 [10:45<07:35,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  59%|    | 258/440 [10:47<07:37,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  59%|    | 259/440 [10:50<07:34,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  59%|    | 260/440 [10:52<07:29,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  59%|    | 261/440 [10:55<07:26,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  60%|    | 262/440 [10:57<07:24,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  60%|    | 263/440 [11:00<07:21,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  60%|    | 264/440 [11:02<07:20,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  60%|    | 265/440 [11:05<07:16,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  60%|    | 266/440 [11:07<07:14,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  61%|    | 267/440 [11:10<07:11,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  61%|    | 268/440 [11:12<07:09,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  61%|    | 269/440 [11:15<07:06,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  61%|   | 270/440 [11:17<07:05,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  62%|   | 271/440 [11:20<07:02,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  62%|   | 272/440 [11:22<06:59,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  62%|   | 273/440 [11:25<06:58,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  62%|   | 274/440 [11:27<06:55,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  62%|   | 275/440 [11:30<06:52,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  63%|   | 276/440 [11:32<06:48,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  63%|   | 277/440 [11:35<06:46,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  63%|   | 278/440 [11:38<07:01,  2.60s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  63%|   | 279/440 [11:40<06:54,  2.57s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  64%|   | 280/440 [11:43<06:47,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  64%|   | 281/440 [11:45<06:43,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  64%|   | 282/440 [11:48<06:41,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  64%|   | 283/440 [11:50<06:39,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  65%|   | 284/440 [11:53<06:34,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  65%|   | 285/440 [11:55<06:31,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  65%|   | 286/440 [11:58<06:27,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  65%|   | 287/440 [12:00<06:25,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  65%|   | 288/440 [12:03<06:23,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  66%|   | 289/440 [12:05<06:21,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  66%|   | 290/440 [12:08<06:16,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  66%|   | 291/440 [12:10<06:13,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  66%|   | 292/440 [12:13<06:12,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  67%|   | 293/440 [12:15<06:11,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  67%|   | 294/440 [12:18<06:08,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  67%|   | 295/440 [12:20<06:05,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  67%|   | 296/440 [12:23<06:00,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  68%|   | 297/440 [12:25<05:58,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  68%|   | 298/440 [12:28<05:54,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  68%|   | 299/440 [12:30<05:50,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  68%|   | 300/440 [12:33<05:47,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  68%|   | 301/440 [12:35<05:44,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  69%|   | 302/440 [12:38<05:43,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  69%|   | 303/440 [12:40<05:41,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  69%|   | 304/440 [12:43<05:38,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  69%|   | 305/440 [12:45<05:36,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  70%|   | 306/440 [12:48<05:33,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  70%|   | 307/440 [12:50<05:33,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  70%|   | 308/440 [12:53<05:29,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  70%|   | 309/440 [12:55<05:27,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  70%|   | 310/440 [12:58<05:24,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  71%|   | 311/440 [13:00<05:23,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  71%|   | 312/440 [13:03<05:20,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  71%|   | 313/440 [13:05<05:16,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  71%|  | 314/440 [13:08<05:27,  2.60s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  72%|  | 315/440 [13:11<05:20,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  72%|  | 316/440 [13:13<05:15,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  72%|  | 317/440 [13:16<05:11,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  72%|  | 318/440 [13:18<05:07,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  72%|  | 319/440 [13:21<05:04,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  73%|  | 320/440 [13:23<05:01,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  73%|  | 321/440 [13:26<04:58,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  73%|  | 322/440 [13:28<04:55,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  73%|  | 323/440 [13:31<04:52,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  74%|  | 324/440 [13:33<04:49,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  74%|  | 325/440 [13:36<04:46,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  74%|  | 326/440 [13:38<04:45,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  74%|  | 327/440 [13:41<04:42,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  75%|  | 328/440 [13:43<04:39,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  75%|  | 329/440 [13:46<04:36,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  75%|  | 330/440 [13:48<04:34,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  75%|  | 331/440 [13:51<04:34,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  75%|  | 332/440 [13:53<04:30,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  76%|  | 333/440 [13:56<04:27,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  76%|  | 334/440 [13:58<04:24,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  76%|  | 335/440 [14:01<04:21,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  76%|  | 336/440 [14:03<04:19,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  77%|  | 337/440 [14:06<04:16,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  77%|  | 338/440 [14:08<04:14,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  77%|  | 339/440 [14:10<04:11,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  77%|  | 340/440 [14:13<04:09,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  78%|  | 341/440 [14:15<04:07,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  78%|  | 342/440 [14:18<04:04,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  78%|  | 343/440 [14:20<04:01,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  78%|  | 344/440 [14:23<03:58,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  78%|  | 345/440 [14:25<03:57,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  79%|  | 346/440 [14:28<03:55,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  79%|  | 347/440 [14:30<03:52,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  79%|  | 348/440 [14:33<03:48,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  79%|  | 349/440 [14:35<03:46,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  80%|  | 350/440 [14:38<03:45,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  80%|  | 351/440 [14:40<03:42,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  80%|  | 352/440 [14:43<03:39,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  80%|  | 353/440 [14:45<03:36,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  80%|  | 354/440 [14:48<03:35,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  81%|  | 355/440 [14:51<03:41,  2.60s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  81%|  | 356/440 [14:53<03:35,  2.57s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  81%|  | 357/440 [14:56<03:31,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  81%| | 358/440 [14:58<03:27,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  82%| | 359/440 [15:01<03:24,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  82%| | 360/440 [15:03<03:22,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  82%| | 361/440 [15:06<03:18,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  82%| | 362/440 [15:08<03:15,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  82%| | 363/440 [15:11<03:12,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  83%| | 364/440 [15:13<03:09,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  83%| | 365/440 [15:16<03:07,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  83%| | 366/440 [15:18<03:05,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  83%| | 367/440 [15:21<03:03,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  84%| | 368/440 [15:23<03:00,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  84%| | 369/440 [15:26<02:57,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  84%| | 370/440 [15:28<02:55,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  84%| | 371/440 [15:31<02:52,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  85%| | 372/440 [15:33<02:49,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  85%| | 373/440 [15:36<02:46,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  85%| | 374/440 [15:38<02:45,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  85%| | 375/440 [15:41<02:42,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  85%| | 376/440 [15:43<02:40,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  86%| | 377/440 [15:46<02:37,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  86%| | 378/440 [15:48<02:34,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  86%| | 379/440 [15:51<02:32,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  86%| | 380/440 [15:53<02:29,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  87%| | 381/440 [15:56<02:27,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  87%| | 382/440 [15:58<02:24,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  87%| | 383/440 [16:01<02:22,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  87%| | 384/440 [16:03<02:19,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  88%| | 385/440 [16:06<02:17,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  88%| | 386/440 [16:08<02:14,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  88%| | 387/440 [16:11<02:11,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  88%| | 388/440 [16:13<02:09,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  88%| | 389/440 [16:16<02:07,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  89%| | 390/440 [16:18<02:04,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  89%| | 391/440 [16:21<02:02,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  89%| | 392/440 [16:23<02:04,  2.60s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  89%| | 393/440 [16:26<02:00,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  90%| | 394/440 [16:28<01:57,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  90%| | 395/440 [16:31<01:53,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  90%| | 396/440 [16:33<01:50,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  90%| | 397/440 [16:36<01:47,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  90%| | 398/440 [16:38<01:45,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  91%| | 399/440 [16:41<01:42,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  91%| | 400/440 [16:43<01:39,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  91%| | 401/440 [16:46<01:37,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  91%|| 402/440 [16:48<01:34,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  92%|| 403/440 [16:51<01:32,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  92%|| 404/440 [16:53<01:29,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  92%|| 405/440 [16:56<01:27,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  92%|| 406/440 [16:58<01:24,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  92%|| 407/440 [17:01<01:22,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  93%|| 408/440 [17:03<01:19,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  93%|| 409/440 [17:06<01:16,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  93%|| 410/440 [17:08<01:14,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  93%|| 411/440 [17:11<01:12,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  94%|| 412/440 [17:13<01:09,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  94%|| 413/440 [17:16<01:07,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  94%|| 414/440 [17:18<01:04,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  94%|| 415/440 [17:21<01:02,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  95%|| 416/440 [17:23<00:59,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  95%|| 417/440 [17:26<00:57,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  95%|| 418/440 [17:28<00:54,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  95%|| 419/440 [17:31<00:52,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  95%|| 420/440 [17:33<00:49,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  96%|| 421/440 [17:36<00:47,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  96%|| 422/440 [17:38<00:44,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  96%|| 423/440 [17:41<00:42,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  96%|| 424/440 [17:43<00:39,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  97%|| 425/440 [17:46<00:37,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  97%|| 426/440 [17:48<00:34,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  97%|| 427/440 [17:51<00:32,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  97%|| 428/440 [17:53<00:30,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  98%|| 429/440 [17:56<00:27,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  98%|| 430/440 [17:58<00:24,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  98%|| 431/440 [18:01<00:22,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  98%|| 432/440 [18:03<00:19,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  98%|| 433/440 [18:06<00:18,  2.59s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  99%|| 434/440 [18:08<00:15,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  99%|| 435/440 [18:11<00:12,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  99%|| 436/440 [18:13<00:10,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  99%|| 437/440 [18:16<00:07,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress: 100%|| 438/440 [18:18<00:05,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress: 100%|| 439/440 [18:21<00:02,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress: 100%|| 440/440 [18:23<00:00,  2.51s/it]\n",
            "\n",
            "Validation Progress:   0%|          | 0/110 [00:00<?, ?it/s]\u001b[A\n",
            "Validation Progress:   1%|          | 1/110 [00:00<01:25,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   2%|         | 2/110 [00:01<01:24,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   3%|         | 3/110 [00:02<01:24,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   4%|         | 4/110 [00:03<01:23,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   5%|         | 5/110 [00:03<01:22,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   5%|         | 6/110 [00:04<01:22,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   6%|         | 7/110 [00:05<01:20,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   7%|         | 8/110 [00:06<01:20,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   8%|         | 9/110 [00:07<01:19,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   9%|         | 10/110 [00:07<01:18,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  10%|         | 11/110 [00:08<01:17,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  11%|         | 12/110 [00:09<01:16,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  12%|        | 13/110 [00:10<01:15,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  13%|        | 14/110 [00:10<01:14,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  14%|        | 15/110 [00:11<01:14,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  15%|        | 16/110 [00:12<01:13,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  15%|        | 17/110 [00:13<01:12,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  16%|        | 18/110 [00:14<01:11,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  17%|        | 19/110 [00:14<01:11,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  18%|        | 20/110 [00:15<01:10,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  19%|        | 21/110 [00:16<01:09,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  20%|        | 22/110 [00:17<01:08,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  21%|        | 23/110 [00:17<01:07,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  22%|       | 24/110 [00:18<01:06,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  23%|       | 25/110 [00:19<01:06,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  24%|       | 26/110 [00:20<01:05,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  25%|       | 27/110 [00:21<01:04,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  25%|       | 28/110 [00:21<01:03,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  26%|       | 29/110 [00:22<01:03,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  27%|       | 30/110 [00:23<01:02,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  28%|       | 31/110 [00:24<01:01,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  29%|       | 32/110 [00:25<01:01,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  30%|       | 33/110 [00:25<01:00,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  31%|       | 34/110 [00:26<00:59,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  32%|      | 35/110 [00:27<00:59,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  33%|      | 36/110 [00:28<00:58,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  34%|      | 37/110 [00:28<00:57,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  35%|      | 38/110 [00:29<00:56,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  35%|      | 39/110 [00:30<00:55,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  36%|      | 40/110 [00:31<00:54,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  37%|      | 41/110 [00:32<00:53,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  38%|      | 42/110 [00:32<00:53,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  39%|      | 43/110 [00:33<00:52,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  40%|      | 44/110 [00:34<00:51,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  41%|      | 45/110 [00:35<00:50,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  42%|     | 46/110 [00:35<00:49,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  43%|     | 47/110 [00:36<00:49,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  44%|     | 48/110 [00:37<00:48,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  45%|     | 49/110 [00:38<00:47,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  45%|     | 50/110 [00:39<00:46,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  46%|     | 51/110 [00:39<00:45,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  47%|     | 52/110 [00:40<00:45,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  48%|     | 53/110 [00:41<00:44,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  49%|     | 54/110 [00:42<00:44,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  50%|     | 55/110 [00:43<00:43,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  51%|     | 56/110 [00:43<00:42,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  52%|    | 57/110 [00:44<00:41,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  53%|    | 58/110 [00:45<00:40,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  54%|    | 59/110 [00:46<00:39,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  55%|    | 60/110 [00:46<00:38,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  55%|    | 61/110 [00:47<00:38,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  56%|    | 62/110 [00:48<00:37,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  57%|    | 63/110 [00:49<00:36,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  58%|    | 64/110 [00:50<00:35,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  59%|    | 65/110 [00:50<00:34,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  60%|    | 66/110 [00:51<00:34,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  61%|    | 67/110 [00:52<00:33,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  62%|   | 68/110 [00:53<00:33,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  63%|   | 69/110 [00:53<00:32,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  64%|   | 70/110 [00:54<00:31,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  65%|   | 71/110 [00:55<00:30,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  65%|   | 72/110 [00:56<00:29,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  66%|   | 73/110 [00:57<00:28,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  67%|   | 74/110 [00:57<00:27,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  68%|   | 75/110 [00:58<00:27,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  69%|   | 76/110 [00:59<00:26,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  70%|   | 77/110 [01:00<00:25,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  71%|   | 78/110 [01:00<00:24,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  72%|  | 79/110 [01:01<00:24,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  73%|  | 80/110 [01:02<00:23,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  74%|  | 81/110 [01:03<00:22,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  75%|  | 82/110 [01:04<00:22,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  75%|  | 83/110 [01:04<00:21,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  76%|  | 84/110 [01:05<00:20,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  77%|  | 85/110 [01:06<00:19,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  78%|  | 86/110 [01:07<00:18,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  79%|  | 87/110 [01:08<00:18,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  80%|  | 88/110 [01:08<00:17,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  81%|  | 89/110 [01:09<00:16,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  82%| | 90/110 [01:10<00:15,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  83%| | 91/110 [01:11<00:14,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  84%| | 92/110 [01:11<00:14,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  85%| | 93/110 [01:12<00:13,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  85%| | 94/110 [01:13<00:12,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  86%| | 95/110 [01:14<00:11,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  87%| | 96/110 [01:15<00:10,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  88%| | 97/110 [01:15<00:10,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  89%| | 98/110 [01:16<00:09,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  90%| | 99/110 [01:17<00:08,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  91%| | 100/110 [01:18<00:07,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  92%|| 101/110 [01:19<00:07,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  93%|| 102/110 [01:19<00:06,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  94%|| 103/110 [01:20<00:05,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  95%|| 104/110 [01:21<00:04,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  95%|| 105/110 [01:22<00:03,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  96%|| 106/110 [01:22<00:03,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  97%|| 107/110 [01:23<00:02,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  98%|| 108/110 [01:24<00:01,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  99%|| 109/110 [01:25<00:00,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress: 100%|| 110/110 [01:26<00:00,  1.28it/s]\n",
            "Epochs:  28%|       | 7/25 [2:18:43<5:56:32, 1188.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n",
            "[Epoch 7]\n",
            "  [Training] Loss: 0.2324, Accuracy: 94.20%\n",
            "  [Validation] Loss: 0.6120, Accuracy: 72.50%\n",
            "  [INFO] No improvement in validation loss. Patience counter: 4/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   0%|          | 0/440 [00:00<?, ?it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   0%|          | 1/440 [00:02<18:06,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   0%|          | 2/440 [00:05<18:18,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   1%|          | 3/440 [00:07<18:13,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   1%|          | 4/440 [00:09<18:09,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   1%|          | 5/440 [00:12<18:06,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   1%|         | 6/440 [00:15<18:13,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   2%|         | 7/440 [00:17<18:08,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   2%|         | 8/440 [00:20<18:09,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   2%|         | 9/440 [00:22<18:01,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   2%|         | 10/440 [00:25<17:55,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   2%|         | 11/440 [00:27<17:55,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   3%|         | 12/440 [00:30<17:54,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   3%|         | 13/440 [00:32<17:48,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   3%|         | 14/440 [00:35<17:43,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   3%|         | 15/440 [00:37<17:39,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   4%|         | 16/440 [00:40<17:37,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   4%|         | 17/440 [00:42<17:32,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   4%|         | 18/440 [00:45<17:32,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   4%|         | 19/440 [00:47<17:27,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   5%|         | 20/440 [00:49<17:25,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   5%|         | 21/440 [00:52<17:23,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   5%|         | 22/440 [00:54<17:19,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   5%|         | 23/440 [00:57<17:15,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   5%|         | 24/440 [00:59<17:14,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   6%|         | 25/440 [01:02<17:12,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   6%|         | 26/440 [01:04<17:11,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   6%|         | 27/440 [01:07<17:52,  2.60s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   6%|         | 28/440 [01:10<17:34,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   7%|         | 29/440 [01:12<17:21,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   7%|         | 30/440 [01:15<17:15,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   7%|         | 31/440 [01:17<17:16,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   7%|         | 32/440 [01:20<17:06,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   8%|         | 33/440 [01:22<16:58,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   8%|         | 34/440 [01:25<16:54,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   8%|         | 35/440 [01:27<16:50,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   8%|         | 36/440 [01:30<16:50,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   8%|         | 37/440 [01:32<16:43,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   9%|         | 38/440 [01:35<16:43,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   9%|         | 39/440 [01:37<16:42,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   9%|         | 40/440 [01:40<16:40,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:   9%|         | 41/440 [01:42<16:33,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  10%|         | 42/440 [01:45<16:31,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  10%|         | 43/440 [01:47<16:28,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  10%|         | 44/440 [01:50<16:25,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  10%|         | 45/440 [01:52<16:23,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  10%|         | 46/440 [01:55<16:19,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  11%|         | 47/440 [01:57<16:17,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  11%|         | 48/440 [02:00<16:15,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  11%|         | 49/440 [02:02<16:13,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  11%|        | 50/440 [02:05<16:17,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  12%|        | 51/440 [02:07<16:13,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  12%|        | 52/440 [02:10<16:09,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  12%|        | 53/440 [02:12<16:03,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  12%|        | 54/440 [02:15<16:00,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  12%|        | 55/440 [02:17<15:58,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  13%|        | 56/440 [02:20<15:55,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  13%|        | 57/440 [02:22<15:54,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  13%|        | 58/440 [02:25<15:51,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  13%|        | 59/440 [02:27<15:49,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  14%|        | 60/440 [02:30<15:50,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  14%|        | 61/440 [02:32<15:47,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  14%|        | 62/440 [02:35<15:43,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  14%|        | 63/440 [02:37<15:39,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  15%|        | 64/440 [02:39<15:35,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  15%|        | 65/440 [02:42<15:38,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  15%|        | 66/440 [02:45<15:36,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  15%|        | 67/440 [02:47<15:31,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  15%|        | 68/440 [02:50<16:05,  2.59s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  16%|        | 69/440 [02:52<15:50,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  16%|        | 70/440 [02:55<15:38,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  16%|        | 71/440 [02:57<15:29,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  16%|        | 72/440 [03:00<15:24,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  17%|        | 73/440 [03:02<15:19,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  17%|        | 74/440 [03:05<15:18,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  17%|        | 75/440 [03:07<15:11,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  17%|        | 76/440 [03:10<15:07,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  18%|        | 77/440 [03:12<15:01,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  18%|        | 78/440 [03:15<14:58,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  18%|        | 79/440 [03:17<15:01,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  18%|        | 80/440 [03:20<15:01,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  18%|        | 81/440 [03:22<14:55,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  19%|        | 82/440 [03:25<14:55,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  19%|        | 83/440 [03:27<14:50,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  19%|        | 84/440 [03:30<14:48,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  19%|        | 85/440 [03:32<14:42,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  20%|        | 86/440 [03:35<14:38,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  20%|        | 87/440 [03:37<14:33,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  20%|        | 88/440 [03:40<14:31,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  20%|        | 89/440 [03:42<14:33,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  20%|        | 90/440 [03:45<14:31,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  21%|        | 91/440 [03:47<14:27,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  21%|        | 92/440 [03:50<14:23,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  21%|        | 93/440 [03:52<14:19,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  21%|       | 94/440 [03:55<14:21,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  22%|       | 95/440 [03:57<14:17,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  22%|       | 96/440 [04:00<14:18,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  22%|       | 97/440 [04:02<14:14,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  22%|       | 98/440 [04:04<14:11,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  22%|       | 99/440 [04:07<14:11,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  23%|       | 100/440 [04:09<14:08,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  23%|       | 101/440 [04:12<14:05,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  23%|       | 102/440 [04:14<14:03,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  23%|       | 103/440 [04:17<14:00,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  24%|       | 104/440 [04:20<14:02,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  24%|       | 105/440 [04:22<14:30,  2.60s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  24%|       | 106/440 [04:25<14:14,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  24%|       | 107/440 [04:27<14:04,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  25%|       | 108/440 [04:30<13:59,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  25%|       | 109/440 [04:32<13:54,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  25%|       | 110/440 [04:35<13:46,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  25%|       | 111/440 [04:37<13:41,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  25%|       | 112/440 [04:40<13:37,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  26%|       | 113/440 [04:42<13:38,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  26%|       | 114/440 [04:45<13:35,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  26%|       | 115/440 [04:47<13:30,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  26%|       | 116/440 [04:50<13:27,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  27%|       | 117/440 [04:52<13:24,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  27%|       | 118/440 [04:55<13:26,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  27%|       | 119/440 [04:57<13:21,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  27%|       | 120/440 [05:00<13:19,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  28%|       | 121/440 [05:02<13:14,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  28%|       | 122/440 [05:05<13:10,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  28%|       | 123/440 [05:07<13:06,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  28%|       | 124/440 [05:10<13:04,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  28%|       | 125/440 [05:12<13:06,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  29%|       | 126/440 [05:15<13:01,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  29%|       | 127/440 [05:17<12:56,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  29%|       | 128/440 [05:20<12:57,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  29%|       | 129/440 [05:22<12:52,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  30%|       | 130/440 [05:25<12:49,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  30%|       | 131/440 [05:27<12:47,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  30%|       | 132/440 [05:30<12:46,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  30%|       | 133/440 [05:32<12:44,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  30%|       | 134/440 [05:35<12:43,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  31%|       | 135/440 [05:37<12:39,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  31%|       | 136/440 [05:39<12:34,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  31%|       | 137/440 [05:42<12:33,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  31%|      | 138/440 [05:44<12:32,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  32%|      | 139/440 [05:47<12:28,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  32%|      | 140/440 [05:49<12:24,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  32%|      | 141/440 [05:52<12:21,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  32%|      | 142/440 [05:54<12:20,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  32%|      | 143/440 [05:57<12:17,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  33%|      | 144/440 [05:59<12:15,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  33%|      | 145/440 [06:02<12:13,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  33%|      | 146/440 [06:05<12:38,  2.58s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  33%|      | 147/440 [06:07<12:28,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  34%|      | 148/440 [06:10<12:19,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  34%|      | 149/440 [06:12<12:14,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  34%|      | 150/440 [06:15<12:09,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  34%|      | 151/440 [06:17<12:02,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  35%|      | 152/440 [06:20<11:59,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  35%|      | 153/440 [06:22<11:54,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  35%|      | 154/440 [06:25<11:50,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  35%|      | 155/440 [06:27<11:50,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  35%|      | 156/440 [06:30<11:48,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  36%|      | 157/440 [06:32<11:46,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  36%|      | 158/440 [06:34<11:42,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  36%|      | 159/440 [06:37<11:42,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  36%|      | 160/440 [06:39<11:37,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  37%|      | 161/440 [06:42<11:34,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  37%|      | 162/440 [06:44<11:33,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  37%|      | 163/440 [06:47<11:30,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  37%|      | 164/440 [06:49<11:27,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  38%|      | 165/440 [06:52<11:24,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  38%|      | 166/440 [06:54<11:20,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  38%|      | 167/440 [06:57<11:22,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  38%|      | 168/440 [06:59<11:20,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  38%|      | 169/440 [07:02<11:14,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  39%|      | 170/440 [07:04<11:17,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  39%|      | 171/440 [07:07<11:12,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  39%|      | 172/440 [07:09<11:07,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  39%|      | 173/440 [07:12<11:04,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  40%|      | 174/440 [07:14<11:03,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  40%|      | 175/440 [07:17<10:58,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  40%|      | 176/440 [07:19<11:01,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  40%|      | 177/440 [07:22<10:56,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  40%|      | 178/440 [07:24<10:52,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  41%|      | 179/440 [07:27<10:47,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  41%|      | 180/440 [07:29<10:46,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  41%|      | 181/440 [07:32<10:42,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  41%|     | 182/440 [07:35<11:04,  2.57s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  42%|     | 183/440 [07:37<10:53,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  42%|     | 184/440 [07:40<10:45,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  42%|     | 185/440 [07:42<10:38,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  42%|     | 186/440 [07:44<10:34,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  42%|     | 187/440 [07:47<10:29,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  43%|     | 188/440 [07:49<10:25,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  43%|     | 189/440 [07:52<10:23,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  43%|     | 190/440 [07:54<10:20,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  43%|     | 191/440 [07:57<10:18,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  44%|     | 192/440 [07:59<10:18,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  44%|     | 193/440 [08:02<10:16,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  44%|     | 194/440 [08:04<10:11,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  44%|     | 195/440 [08:07<10:07,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  45%|     | 196/440 [08:09<10:06,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  45%|     | 197/440 [08:12<10:02,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  45%|     | 198/440 [08:14<10:02,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  45%|     | 199/440 [08:17<09:57,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  45%|     | 200/440 [08:19<09:53,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  46%|     | 201/440 [08:22<09:52,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  46%|     | 202/440 [08:24<09:50,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  46%|     | 203/440 [08:27<09:48,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  46%|     | 204/440 [08:29<09:46,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  47%|     | 205/440 [08:32<09:44,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  47%|     | 206/440 [08:34<09:41,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  47%|     | 207/440 [08:37<09:39,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  47%|     | 208/440 [08:39<09:38,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  48%|     | 209/440 [08:42<09:34,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  48%|     | 210/440 [08:44<09:32,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  48%|     | 211/440 [08:47<09:28,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  48%|     | 212/440 [08:49<09:25,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  48%|     | 213/440 [08:52<09:22,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  49%|     | 214/440 [08:54<09:20,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  49%|     | 215/440 [08:56<09:17,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  49%|     | 216/440 [08:59<09:15,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  49%|     | 217/440 [09:01<09:12,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  50%|     | 218/440 [09:04<09:10,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  50%|     | 219/440 [09:06<09:07,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  50%|     | 220/440 [09:09<09:06,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  50%|     | 221/440 [09:11<09:03,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  50%|     | 222/440 [09:14<09:26,  2.60s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  51%|     | 223/440 [09:17<09:16,  2.57s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  51%|     | 224/440 [09:19<09:08,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  51%|     | 225/440 [09:22<09:01,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  51%|    | 226/440 [09:24<08:55,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  52%|    | 227/440 [09:27<08:51,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  52%|    | 228/440 [09:29<08:48,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  52%|    | 229/440 [09:32<08:45,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  52%|    | 230/440 [09:34<08:44,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  52%|    | 231/440 [09:37<08:39,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  53%|    | 232/440 [09:39<08:37,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  53%|    | 233/440 [09:42<08:33,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  53%|    | 234/440 [09:44<08:32,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  53%|    | 235/440 [09:47<08:31,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  54%|    | 236/440 [09:49<08:28,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  54%|    | 237/440 [09:51<08:23,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  54%|    | 238/440 [09:54<08:20,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  54%|    | 239/440 [09:56<08:17,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  55%|    | 240/440 [09:59<08:15,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  55%|    | 241/440 [10:01<08:12,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  55%|    | 242/440 [10:04<08:09,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  55%|    | 243/440 [10:06<08:07,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  55%|    | 244/440 [10:09<08:06,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  56%|    | 245/440 [10:11<08:02,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  56%|    | 246/440 [10:14<08:00,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  56%|    | 247/440 [10:16<07:58,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  56%|    | 248/440 [10:19<07:54,  2.47s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  57%|    | 249/440 [10:21<07:53,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  57%|    | 250/440 [10:24<07:52,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  57%|    | 251/440 [10:26<07:49,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  57%|    | 252/440 [10:29<07:47,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  57%|    | 253/440 [10:31<07:44,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  58%|    | 254/440 [10:34<07:41,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  58%|    | 255/440 [10:36<07:39,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  58%|    | 256/440 [10:39<07:36,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  58%|    | 257/440 [10:41<07:33,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  59%|    | 258/440 [10:44<07:50,  2.59s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  59%|    | 259/440 [10:46<07:43,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  59%|    | 260/440 [10:49<07:35,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  59%|    | 261/440 [10:51<07:30,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  60%|    | 262/440 [10:54<07:25,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  60%|    | 263/440 [10:56<07:21,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  60%|    | 264/440 [10:59<07:20,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  60%|    | 265/440 [11:01<07:16,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  60%|    | 266/440 [11:04<07:13,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  61%|    | 267/440 [11:06<07:09,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  61%|    | 268/440 [11:09<07:06,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  61%|    | 269/440 [11:11<07:06,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  61%|   | 270/440 [11:14<07:03,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  62%|   | 271/440 [11:16<07:00,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  62%|   | 272/440 [11:19<06:57,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  62%|   | 273/440 [11:21<06:55,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  62%|   | 274/440 [11:24<06:53,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  62%|   | 275/440 [11:26<06:50,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  63%|   | 276/440 [11:29<06:49,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  63%|   | 277/440 [11:31<06:47,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  63%|   | 278/440 [11:34<06:44,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  63%|   | 279/440 [11:36<06:43,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  64%|   | 280/440 [11:39<06:39,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  64%|   | 281/440 [11:41<06:36,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  64%|   | 282/440 [11:44<06:34,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  64%|   | 283/440 [11:46<06:34,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  65%|   | 284/440 [11:49<06:30,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  65%|   | 285/440 [11:51<06:27,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  65%|   | 286/440 [11:54<06:23,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  65%|   | 287/440 [11:56<06:21,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  65%|   | 288/440 [11:59<06:20,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  66%|   | 289/440 [12:01<06:17,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  66%|   | 290/440 [12:04<06:13,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  66%|   | 291/440 [12:06<06:11,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  66%|   | 292/440 [12:09<06:08,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  67%|   | 293/440 [12:11<06:05,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  67%|   | 294/440 [12:14<06:03,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  67%|   | 295/440 [12:16<06:00,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  67%|   | 296/440 [12:19<05:58,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  68%|   | 297/440 [12:21<05:55,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  68%|   | 298/440 [12:24<05:54,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  68%|   | 299/440 [12:26<06:06,  2.60s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  68%|   | 300/440 [12:29<06:00,  2.57s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  68%|   | 301/440 [12:31<05:54,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  69%|   | 302/440 [12:34<05:50,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  69%|   | 303/440 [12:36<05:45,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  69%|   | 304/440 [12:39<05:42,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  69%|   | 305/440 [12:41<05:38,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  70%|   | 306/440 [12:44<05:38,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  70%|   | 307/440 [12:46<05:35,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  70%|   | 308/440 [12:49<05:33,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  70%|   | 309/440 [12:51<05:29,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  70%|   | 310/440 [12:54<05:26,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  71%|   | 311/440 [12:56<05:23,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  71%|   | 312/440 [12:59<05:21,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  71%|   | 313/440 [13:02<05:18,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  71%|  | 314/440 [13:04<05:14,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  72%|  | 315/440 [13:06<05:12,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  72%|  | 316/440 [13:09<05:10,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  72%|  | 317/440 [13:12<05:07,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  72%|  | 318/440 [13:14<05:06,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  72%|  | 319/440 [13:17<05:02,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  73%|  | 320/440 [13:19<04:58,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  73%|  | 321/440 [13:21<04:56,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  73%|  | 322/440 [13:24<04:55,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  73%|  | 323/440 [13:27<04:54,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  74%|  | 324/440 [13:29<04:51,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  74%|  | 325/440 [13:32<04:48,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  74%|  | 326/440 [13:34<04:45,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  74%|  | 327/440 [13:37<04:43,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  75%|  | 328/440 [13:39<04:40,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  75%|  | 329/440 [13:42<04:38,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  75%|  | 330/440 [13:44<04:35,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  75%|  | 331/440 [13:47<04:33,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  75%|  | 332/440 [13:49<04:32,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  76%|  | 333/440 [13:52<04:29,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  76%|  | 334/440 [13:54<04:26,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  76%|  | 335/440 [13:57<04:23,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  76%|  | 336/440 [14:00<04:32,  2.62s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  77%|  | 337/440 [14:02<04:25,  2.58s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  77%|  | 338/440 [14:04<04:19,  2.55s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  77%|  | 339/440 [14:07<04:15,  2.53s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  77%|  | 340/440 [14:09<04:11,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  78%|  | 341/440 [14:12<04:08,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  78%|  | 342/440 [14:14<04:04,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  78%|  | 343/440 [14:17<04:01,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  78%|  | 344/440 [14:19<03:58,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  78%|  | 345/440 [14:22<03:55,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  79%|  | 346/440 [14:24<03:54,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  79%|  | 347/440 [14:27<03:51,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  79%|  | 348/440 [14:29<03:49,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  79%|  | 349/440 [14:32<03:46,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  80%|  | 350/440 [14:34<03:44,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  80%|  | 351/440 [14:37<03:42,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  80%|  | 352/440 [14:39<03:39,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  80%|  | 353/440 [14:42<03:36,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  80%|  | 354/440 [14:44<03:33,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  81%|  | 355/440 [14:47<03:31,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  81%|  | 356/440 [14:49<03:29,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  81%|  | 357/440 [14:52<03:26,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  81%| | 358/440 [14:54<03:23,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  82%| | 359/440 [14:57<03:21,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  82%| | 360/440 [14:59<03:19,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  82%| | 361/440 [15:02<03:16,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  82%| | 362/440 [15:04<03:14,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  82%| | 363/440 [15:07<03:11,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  83%| | 364/440 [15:09<03:09,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  83%| | 365/440 [15:12<03:07,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  83%| | 366/440 [15:14<03:05,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  83%| | 367/440 [15:17<03:02,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  84%| | 368/440 [15:19<02:59,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  84%| | 369/440 [15:22<02:56,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  84%| | 370/440 [15:24<02:55,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  84%| | 371/440 [15:27<02:52,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  85%| | 372/440 [15:29<02:49,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  85%| | 373/440 [15:32<02:46,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  85%| | 374/440 [15:34<02:44,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  85%| | 375/440 [15:37<02:42,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  85%| | 376/440 [15:39<02:39,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  86%| | 377/440 [15:42<02:43,  2.59s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  86%| | 378/440 [15:44<02:38,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  86%| | 379/440 [15:47<02:34,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  86%| | 380/440 [15:49<02:31,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  87%| | 381/440 [15:52<02:27,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  87%| | 382/440 [15:54<02:24,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  87%| | 383/440 [15:57<02:22,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  87%| | 384/440 [15:59<02:19,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  88%| | 385/440 [16:02<02:17,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  88%| | 386/440 [16:04<02:14,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  88%| | 387/440 [16:07<02:11,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  88%| | 388/440 [16:09<02:09,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  88%| | 389/440 [16:12<02:06,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  89%| | 390/440 [16:14<02:05,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  89%| | 391/440 [16:17<02:02,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  89%| | 392/440 [16:19<01:59,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  89%| | 393/440 [16:22<01:56,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  90%| | 394/440 [16:24<01:54,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  90%| | 395/440 [16:27<01:51,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  90%| | 396/440 [16:29<01:49,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  90%| | 397/440 [16:32<01:47,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  90%| | 398/440 [16:34<01:44,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  91%| | 399/440 [16:37<01:42,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  91%| | 400/440 [16:39<01:39,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  91%| | 401/440 [16:42<01:36,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  91%|| 402/440 [16:44<01:34,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  92%|| 403/440 [16:47<01:32,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  92%|| 404/440 [16:49<01:29,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  92%|| 405/440 [16:52<01:27,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  92%|| 406/440 [16:54<01:24,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  92%|| 407/440 [16:57<01:21,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  93%|| 408/440 [16:59<01:19,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  93%|| 409/440 [17:02<01:17,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  93%|| 410/440 [17:04<01:14,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  93%|| 411/440 [17:06<01:11,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  94%|| 412/440 [17:09<01:09,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  94%|| 413/440 [17:12<01:09,  2.58s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  94%|| 414/440 [17:14<01:06,  2.56s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  94%|| 415/440 [17:17<01:03,  2.54s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  95%|| 416/440 [17:19<01:00,  2.52s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  95%|| 417/440 [17:22<00:57,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  95%|| 418/440 [17:24<00:54,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  95%|| 419/440 [17:27<00:52,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  95%|| 420/440 [17:29<00:50,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  96%|| 421/440 [17:32<00:47,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  96%|| 422/440 [17:34<00:44,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  96%|| 423/440 [17:37<00:42,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  96%|| 424/440 [17:39<00:39,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  97%|| 425/440 [17:42<00:37,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  97%|| 426/440 [17:44<00:34,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  97%|| 427/440 [17:47<00:32,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  97%|| 428/440 [17:49<00:29,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  98%|| 429/440 [17:52<00:27,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  98%|| 430/440 [17:54<00:24,  2.49s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  98%|| 431/440 [17:57<00:22,  2.48s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  98%|| 432/440 [17:59<00:20,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  98%|| 433/440 [18:02<00:17,  2.51s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  99%|| 434/440 [18:04<00:15,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  99%|| 435/440 [18:07<00:12,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  99%|| 436/440 [18:09<00:09,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress:  99%|| 437/440 [18:12<00:07,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress: 100%|| 438/440 [18:14<00:05,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress: 100%|| 439/440 [18:17<00:02,  2.50s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Progress: 100%|| 440/440 [18:19<00:00,  2.50s/it]\n",
            "\n",
            "Validation Progress:   0%|          | 0/110 [00:00<?, ?it/s]\u001b[A\n",
            "Validation Progress:   1%|          | 1/110 [00:00<01:25,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   2%|         | 2/110 [00:01<01:25,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   3%|         | 3/110 [00:02<01:23,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   4%|         | 4/110 [00:03<01:22,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   5%|         | 5/110 [00:03<01:21,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   5%|         | 6/110 [00:04<01:21,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   6%|         | 7/110 [00:05<01:21,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   7%|         | 8/110 [00:06<01:20,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   8%|         | 9/110 [00:07<01:19,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:   9%|         | 10/110 [00:07<01:19,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  10%|         | 11/110 [00:08<01:17,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  11%|         | 12/110 [00:09<01:17,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  12%|        | 13/110 [00:10<01:16,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  13%|        | 14/110 [00:11<01:15,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  14%|        | 15/110 [00:11<01:14,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  15%|        | 16/110 [00:12<01:13,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  15%|        | 17/110 [00:13<01:12,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  16%|        | 18/110 [00:14<01:11,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  17%|        | 19/110 [00:14<01:10,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  18%|        | 20/110 [00:15<01:10,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  19%|        | 21/110 [00:16<01:09,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  20%|        | 22/110 [00:17<01:08,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  21%|        | 23/110 [00:18<01:08,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  22%|       | 24/110 [00:18<01:07,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  23%|       | 25/110 [00:19<01:06,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  24%|       | 26/110 [00:20<01:05,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  25%|       | 27/110 [00:21<01:04,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  25%|       | 28/110 [00:21<01:03,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  26%|       | 29/110 [00:22<01:03,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  27%|       | 30/110 [00:23<01:02,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  28%|       | 31/110 [00:24<01:01,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  29%|       | 32/110 [00:25<01:01,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  30%|       | 33/110 [00:25<00:59,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  31%|       | 34/110 [00:26<00:59,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  32%|      | 35/110 [00:27<00:58,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  33%|      | 36/110 [00:28<00:57,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  34%|      | 37/110 [00:28<00:56,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  35%|      | 38/110 [00:29<00:56,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  35%|      | 39/110 [00:30<00:55,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  36%|      | 40/110 [00:31<00:55,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  37%|      | 41/110 [00:32<00:54,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  38%|      | 42/110 [00:32<00:53,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  39%|      | 43/110 [00:33<00:52,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  40%|      | 44/110 [00:34<00:51,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  41%|      | 45/110 [00:35<00:50,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  42%|     | 46/110 [00:36<00:49,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  43%|     | 47/110 [00:36<00:49,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  44%|     | 48/110 [00:37<00:48,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  45%|     | 49/110 [00:38<00:47,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  45%|     | 50/110 [00:39<00:46,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  46%|     | 51/110 [00:39<00:46,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  47%|     | 52/110 [00:40<00:45,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  48%|     | 53/110 [00:41<00:44,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  49%|     | 54/110 [00:42<00:43,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  50%|     | 55/110 [00:43<00:43,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  51%|     | 56/110 [00:43<00:42,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  52%|    | 57/110 [00:44<00:41,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  53%|    | 58/110 [00:45<00:40,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  54%|    | 59/110 [00:46<00:39,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  55%|    | 60/110 [00:46<00:38,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  55%|    | 61/110 [00:47<00:38,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  56%|    | 62/110 [00:48<00:37,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  57%|    | 63/110 [00:49<00:36,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  58%|    | 64/110 [00:50<00:35,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  59%|    | 65/110 [00:50<00:35,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  60%|    | 66/110 [00:51<00:34,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  61%|    | 67/110 [00:52<00:33,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  62%|   | 68/110 [00:53<00:32,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  63%|   | 69/110 [00:54<00:32,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  64%|   | 70/110 [00:54<00:31,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  65%|   | 71/110 [00:55<00:30,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  65%|   | 72/110 [00:56<00:30,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  66%|   | 73/110 [00:57<00:29,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  67%|   | 74/110 [00:57<00:28,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  68%|   | 75/110 [00:58<00:27,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  69%|   | 76/110 [00:59<00:26,  1.29it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  70%|   | 77/110 [01:00<00:25,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  71%|   | 78/110 [01:01<00:25,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  72%|  | 79/110 [01:01<00:24,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  73%|  | 80/110 [01:02<00:23,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  74%|  | 81/110 [01:03<00:22,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  75%|  | 82/110 [01:04<00:22,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  75%|  | 83/110 [01:04<00:21,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  76%|  | 84/110 [01:05<00:20,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  77%|  | 85/110 [01:06<00:19,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  78%|  | 86/110 [01:07<00:19,  1.25it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  79%|  | 87/110 [01:08<00:18,  1.26it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  80%|  | 88/110 [01:08<00:17,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  81%|  | 89/110 [01:09<00:16,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  82%| | 90/110 [01:10<00:15,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  83%| | 91/110 [01:11<00:14,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  84%| | 92/110 [01:12<00:14,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  85%| | 93/110 [01:12<00:13,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  85%| | 94/110 [01:13<00:12,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  86%| | 95/110 [01:14<00:11,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  87%| | 96/110 [01:15<00:10,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  88%| | 97/110 [01:15<00:10,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  89%| | 98/110 [01:16<00:09,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  90%| | 99/110 [01:17<00:08,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  91%| | 100/110 [01:18<00:07,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  92%|| 101/110 [01:19<00:07,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  93%|| 102/110 [01:19<00:06,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  94%|| 103/110 [01:20<00:05,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  95%|| 104/110 [01:21<00:04,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  95%|| 105/110 [01:22<00:03,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  96%|| 106/110 [01:23<00:03,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  97%|| 107/110 [01:23<00:02,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  98%|| 108/110 [01:24<00:01,  1.28it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress:  99%|| 109/110 [01:25<00:00,  1.27it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Progress: 100%|| 110/110 [01:26<00:00,  1.28it/s]\n",
            "Epochs:  28%|       | 7/25 [2:38:28<6:47:31, 1358.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n",
            "[Epoch 8]\n",
            "  [Training] Loss: 0.1812, Accuracy: 95.23%\n",
            "  [Validation] Loss: 0.6679, Accuracy: 69.77%\n",
            "  [INFO] No improvement in validation loss. Patience counter: 5/5\n",
            "  [INFO] Early stopping triggered at epoch 8.\n",
            "Training completed.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "from datetime import datetime\n",
        "#===================================\n",
        "now = datetime.now()\n",
        "formatted_time = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "\n",
        "\n",
        "MODEL_NAME = f\"best_model_{formatted_time}.pth\"\n",
        "SAVE_MODEL_PATH = f\"/content/drive/MyDrive/FALL2024/comp550/final_project/models/{MODEL_NAME}\"  #\n",
        "#=================================================\n",
        "\n",
        "# Parameters for early stopping\n",
        "patience = 5  # Number of epochs to wait for improvement\n",
        "min_delta = 0.001  # Minimum improvement to reset patience\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "\n",
        "# For storing metrics\n",
        "epoch_train_loss = []\n",
        "epoch_train_accuracy = []\n",
        "epoch_val_loss = []\n",
        "epoch_val_accuracy = []\n",
        "\n",
        "# Paths to save models\n",
        "best_model_path = SAVE_MODEL_PATH\n",
        "last_model_path = SAVE_MODEL_PATH\n",
        "\n",
        "# Training loop with early stopping and keyboard interrupt handling\n",
        "num_epochs = 25\n",
        "progress_bar = tqdm(range(num_epochs), desc=\"Epochs\")\n",
        "\n",
        "try:\n",
        "    for epoch in progress_bar:\n",
        "        # Training\n",
        "        train_loss, train_accuracy = train_one_epoch(\n",
        "            model=model,\n",
        "            data_loader=reddit_training_data_loader,\n",
        "            optimizer=optimizer,\n",
        "            criterion=criterion,\n",
        "            device=device\n",
        "        )\n",
        "        scheduler.step()\n",
        "        epoch_train_loss.append(train_loss)\n",
        "        epoch_train_accuracy.append(train_accuracy)\n",
        "\n",
        "        # Validation\n",
        "        val_loss, val_accuracy = validation(\n",
        "            model=model,\n",
        "            val_loader=reddit_validation_data_loader,\n",
        "            criterion=criterion,\n",
        "            device=device\n",
        "        )\n",
        "        epoch_val_loss.append(val_loss)\n",
        "        epoch_val_accuracy.append(val_accuracy)\n",
        "\n",
        "        print(f\"[Epoch {epoch + 1}]\")\n",
        "        print(f\"  [Training] Loss: {train_loss:.4f}, Accuracy: {train_accuracy * 100:.2f}%\")\n",
        "        print(f\"  [Validation] Loss: {val_loss:.4f}, Accuracy: {val_accuracy * 100:.2f}%\")\n",
        "\n",
        "        # Check for improvement in validation loss\n",
        "        if val_loss < best_val_loss - min_delta:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            # Save the best model\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            print(f\"  [INFO] Validation loss improved. Model saved to {best_model_path}.\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"  [INFO] No improvement in validation loss. Patience counter: {patience_counter}/{patience}\")\n",
        "\n",
        "        # Early stopping condition\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"  [INFO] Early stopping triggered at epoch {epoch + 1}.\")\n",
        "            break\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n[INFO] Training interrupted by user.\")\n",
        "    torch.save(model.state_dict(), last_model_path)\n",
        "    print(f\"[INFO] Model saved to {last_model_path} on interrupt.\")\n",
        "    disconnect_runtime()\n",
        "\n",
        "print(\"Training completed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Fj22HJ6BAV1"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 957
        },
        "collapsed": true,
        "id": "yYSSd394KI8t",
        "outputId": "346d6f03-8f56-4abe-e8a7-4af8a37dcebe"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1cAAAHWCAYAAACbsXOkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAACmjElEQVR4nOzdd3xN9//A8de9N3uK7EREJLFnjZSaLYLSUq3R1mhVW7/qUqVqq1ZbraoOOlCqWqX4UsRqjaIotWuEmImEhOxxc+/5/XHkciUhIcm9kvfz8TiS+7lnvO/93Cv3fT9LoyiKghBCCCGEEEKIe6K1dABCCCGEEEIIUR5IciWEEEIIIYQQJUCSKyGEEEIIIYQoAZJcCSGEEEIIIUQJkORKCCGEEEIIIUqAJFdCCCGEEEIIUQIkuRJCCCGEEEKIEiDJlRBCCCGEEEKUAEmuhBBCCCGEEKIESHIlhBBlZNCgQVSrVu2ujp04cSIajaZkA7IyZ86cQaPR8MMPP5T5tTUaDRMnTjTd/uGHH9BoNJw5c+aOx1arVo1BgwaVaDz38loRQghhOZJcCSEqPI1GU6Rt8+bNlg61wnvttdfQaDRER0cXus+YMWPQaDQcPHiwDCMrvtjYWCZOnMj+/fstHUqB/vvvPzQaDQ4ODly7ds3S4QghxH1BkishRIX3448/mm0dO3YssLx27dr3dJ3vvvuO48eP39WxY8eOJTMz856uXx4888wzACxatKjQfX7++Wfq169PgwYN7vo6/fv3JzMzk+Dg4Ls+x53ExsYyadKkApOre3mtlJSFCxfi5+cHwNKlSy0aixBC3C9sLB2AEEJY2rPPPmt2+++//2bDhg35ym+VkZGBk5NTka9ja2t7V/EB2NjYYGMj/2VHREQQFhbGzz//zPjx4/Pdv3PnTmJiYvjwww/v6To6nQ6dTndP57gX9/JaKQmKorBo0SKefvppYmJi+Omnn3jhhRcsGlNh0tPTcXZ2tnQYQggBSMuVEEIUSbt27ahXrx579+6lTZs2ODk58e677wLwv//9j0cffZSAgADs7e0JDQ3lvffew2AwmJ3j1nE0eWOMPvnkE7799ltCQ0Oxt7enWbNm7Nmzx+zYgsZcaTQahg0bxooVK6hXrx729vbUrVuXqKiofPFv3ryZpk2b4uDgQGhoKN98802Rx3Ft27aNp556iqpVq2Jvb09QUBBvvvlmvpa0QYMG4eLiwsWLF+nRowcuLi54e3szYsSIfM/FtWvXGDRoEO7u7lSqVImBAwcWuevZM888w7Fjx9i3b1+++xYtWoRGo6Ffv37k5OQwfvx4mjRpgru7O87OzrRu3Zo///zzjtcoaMyVoihMmTKFKlWq4OTkRPv27Tly5Ei+Y5OSkhgxYgT169fHxcUFNzc3unTpwoEDB0z7bN68mWbNmgHw3HPPmbqe5o03K2jMVXp6Om+99RZBQUHY29tTs2ZNPvnkExRFMduvOK+Lwmzfvp0zZ87Qt29f+vbty9atW7lw4UK+/YxGI59//jn169fHwcEBb29vOnfuzD///GO238KFC2nevDlOTk54eHjQpk0b1q9fbxbzzWPe8tw6ni2vXrZs2cL//d//4ePjQ5UqVQA4e/Ys//d//0fNmjVxdHTE09OTp556qsBxc9euXePNN9+kWrVq2NvbU6VKFQYMGMCVK1dIS0vD2dmZ119/Pd9xFy5cQKfTMXXq1CI+k0KIika+BhVCiCJKTEykS5cu9O3bl2effRZfX19A/cDn4uLC8OHDcXFx4Y8//mD8+PGkpKQwbdq0O5530aJFpKam8tJLL6HRaPj444954oknOH369B1bMP766y+WLVvG//3f/+Hq6srMmTPp1asX586dw9PTE4B///2Xzp074+/vz6RJkzAYDEyePBlvb+8iPe4lS5aQkZHB0KFD8fT0ZPfu3XzxxRdcuHCBJUuWmO1rMBiIjIwkIiKCTz75hI0bN/Lpp58SGhrK0KFDATVJefzxx/nrr794+eWXqV27NsuXL2fgwIFFiueZZ55h0qRJLFq0iAceeMDs2r/++iutW7ematWqXLlyhe+//55+/foxZMgQUlNTmTNnDpGRkezevZtGjRoV6Xp5xo8fz5QpU+jatStdu3Zl3759dOrUiZycHLP9Tp8+zYoVK3jqqacICQkhPj6eb775hrZt23L06FECAgKoXbs2kydPZvz48bz44ou0bt0agJYtWxZ4bUVReOyxx/jzzz8ZPHgwjRo1Yt26dbz99ttcvHiRzz77zGz/orwubuenn34iNDSUZs2aUa9ePZycnPj55595++23zfYbPHgwP/zwA126dOGFF14gNzeXbdu28ffff9O0aVMAJk2axMSJE2nZsiWTJ0/Gzs6OXbt28ccff9CpU6ciP/83+7//+z+8vb0ZP3486enpAOzZs4cdO3bQt29fqlSpwpkzZ5g1axbt2rXj6NGjplbmtLQ0WrduzX///cfzzz/PAw88wJUrV1i5ciUXLlygUaNG9OzZk8WLFzN9+nSzFsyff/4ZRVFM3VOFECIfRQghhJlXXnlFufW/x7Zt2yqAMnv27Hz7Z2Rk5Ct76aWXFCcnJyUrK8tUNnDgQCU4ONh0OyYmRgEUT09PJSkpyVT+v//9TwGUVatWmcomTJiQLyZAsbOzU6Kjo01lBw4cUADliy++MJV1795dcXJyUi5evGgqO3nypGJjY5PvnAUp6PFNnTpV0Wg0ytmzZ80eH6BMnjzZbN/GjRsrTZo0Md1esWKFAigff/yxqSw3N1dp3bq1Aijz5s27Y0zNmjVTqlSpohgMBlNZVFSUAijffPON6ZzZ2dlmx129elXx9fVVnn/+ebNyQJkwYYLp9rx58xRAiYmJURRFURISEhQ7Ozvl0UcfVYxGo2m/d999VwGUgQMHmsqysrLM4lIUta7t7e3Nnps9e/YU+nhvfa3kPWdTpkwx2+/JJ59UNBqN2WugqK+LwuTk5Cienp7KmDFjTGVPP/200rBhQ7P9/vjjDwVQXnvttXznyHuOTp48qWi1WqVnz575npObn8dbn/88wcHBZs9tXr20atVKyc3NNdu3oNfpzp07FUBZsGCBqWz8+PEKoCxbtqzQuNetW6cAytq1a83ub9CggdK2bdt8xwkhRB7pFiiEEEVkb2/Pc889l6/c0dHR9HtqaipXrlyhdevWZGRkcOzYsTuet0+fPnh4eJhu57VinD59+o7HdujQgdDQUNPtBg0a4ObmZjrWYDCwceNGevToQUBAgGm/sLAwunTpcsfzg/njS09P58qVK7Rs2RJFUfj333/z7f/yyy+b3W7durXZY1mzZg02NjamlixQxzi9+uqrRYoH1HFyFy5cYOvWraayRYsWYWdnx1NPPWU6p52dHaB2X0tKSiI3N5emTZsW2KXwdjZu3EhOTg6vvvqqWVfKN954I9++9vb2aLXqn1eDwUBiYiIuLi7UrFmz2NfNs2bNGnQ6Ha+99ppZ+VtvvYWiKKxdu9as/E6vi9tZu3YtiYmJ9OvXz1TWr18/Dhw4YNYN8rfffkOj0TBhwoR858h7jlasWIHRaGT8+PGm5+TWfe7GkCFD8o2Ju/l1qtfrSUxMJCwsjEqVKpk977/99hsNGzakZ8+ehcbdoUMHAgIC+Omnn0z3HT58mIMHD95xLKYQomKT5EoIIYooMDDQ9GH9ZkeOHKFnz564u7vj5uaGt7e36QNYcnLyHc9btWpVs9t5idbVq1eLfWze8XnHJiQkkJmZSVhYWL79CioryLlz5xg0aBCVK1c2jaNq27YtkP/x5Y27KSweUMfG+Pv74+LiYrZfzZo1ixQPQN++fdHpdKZZA7Oysli+fDldunQxS1Tnz59PgwYNcHBwwNPTE29vb1avXl2kernZ2bNnAQgPDzcr9/b2NrseqIncZ599Rnh4OPb29nh5eeHt7c3BgweLfd2brx8QEICrq6tZed4Mlnnx5bnT6+J2Fi5cSEhICPb29kRHRxMdHU1oaChOTk5mycapU6cICAigcuXKhZ7r1KlTaLVa6tSpc8frFkdISEi+sszMTMaPH28ak5b3vF+7ds3seT916hT16tW77fm1Wi3PPPMMK1asICMjA1C7Sjo4OJiSdyGEKIgkV0IIUUQ3fzOe59q1a7Rt25YDBw4wefJkVq1axYYNG/joo48A9YP2nRQ2K51yy0QFJX1sURgMBjp27Mjq1asZNWoUK1asYMOGDaaJF259fGU1w56Pjw8dO3bkt99+Q6/Xs2rVKlJTU83GwixcuJBBgwYRGhrKnDlziIqKYsOGDTz88MNFqpe79cEHHzB8+HDatGnDwoULWbduHRs2bKBu3bqlet2b3e3rIiUlhVWrVhETE0N4eLhpq1OnDhkZGSxatKjEXltFcetEKHkKei+++uqrvP/++/Tu3Ztff/2V9evXs2HDBjw9Pe/qeR8wYABpaWmsWLHCNHtit27dcHd3L/a5hBAVh0xoIYQQ92Dz5s0kJiaybNky2rRpYyqPiYmxYFQ3+Pj44ODgUOCiu7dbiDfPoUOHOHHiBPPnz2fAgAGm8g0bNtx1TMHBwWzatIm0tDSz1qviruv0zDPPEBUVxdq1a1m0aBFubm50797ddP/SpUupXr06y5YtM+uCVlA3tqLEDHDy5EmqV69uKr98+XK+1qClS5fSvn175syZY1Z+7do1vLy8TLeL0y0uODiYjRs3kpqaatZ6ldfttKTW41q2bBlZWVnMmjXLLFZQ62fs2LFs376dVq1aERoayrp160hKSiq09So0NBSj0cjRo0dvO4GIh4dHvtkic3JyiIuLK3LsS5cuZeDAgXz66aemsqysrHznDQ0N5fDhw3c8X7169WjcuDE//fQTVapU4dy5c3zxxRdFjkcIUTFJy5UQQtyDvBaCm7/Nz8nJ4euvv7ZUSGZ0Oh0dOnRgxYoVxMbGmsqjo6PzjdMp7Hgwf3yKovD555/fdUxdu3YlNzeXWbNmmcoMBkOxP7j26NEDJycnvv76a9auXcsTTzyBg4PDbWPftWsXO3fuLHbMHTp0wNbWli+++MLsfDNmzMi3r06ny9e6s2TJEi5evGhWlrc2U1GmoO/atSsGg4Evv/zSrPyzzz5Do9EUefzcnSxcuJDq1avz8ssv8+STT5ptI0aMwMXFxdQ1sFevXiiKwqRJk/KdJ+/x9+jRA61Wy+TJk/O1Ht38HIWGhpqNnwP49ttvC225KkhBz/sXX3yR7xy9evXiwIEDLF++vNC48/Tv35/169czY8YMPD09S+x5FkKUX9JyJYQQ96Bly5Z4eHgwcOBAXnvtNTQaDT/++GOZdp26k4kTJ7J+/Xoeeughhg4davqQXq9ePfbv33/bY2vVqkVoaCgjRozg4sWLuLm58dtvvxVp7E5hunfvzkMPPcQ777zDmTNnqFOnDsuWLSv2eCQXFxd69OhhGnd16/TY3bp1Y9myZfTs2ZNHH32UmJgYZs+eTZ06dUhLSyvWtfLW65o6dSrdunWja9eu/Pvvv6xduzZfC0+3bt2YPHkyzz33HC1btuTQoUP89NNPZi1eoCYUlSpVYvbs2bi6uuLs7ExERESB44m6d+9O+/btGTNmDGfOnKFhw4asX7+e//3vf7zxxhtmk1fcrdjYWP788898k2bksbe3JzIykiVLljBz5kzat29P//79mTlzJidPnqRz584YjUa2bdtG+/btGTZsGGFhYYwZM4b33nuP1q1b88QTT2Bvb8+ePXsICAgwrRf1wgsv8PLLL9OrVy86duzIgQMHWLduXb7n9na6devGjz/+iLu7O3Xq1GHnzp1s3Lgx39Tzb7/9NkuXLuWpp57i+eefp0mTJiQlJbFy5Upmz55Nw4YNTfs+/fTTjBw5kuXLlzN06FCLL+4shLB+0nIlhBD3wNPTk99//x1/f3/Gjh3LJ598QseOHfn4448tHZpJkyZNWLt2LR4eHowbN445c+YwefJkHnnkEbOWnoLY2tqyatUqGjVqxNSpU5k0aRLh4eEsWLDgruPRarWsXLmSZ555hoULFzJmzBgCAwOZP39+sc+Vl1D5+/vz8MMPm903aNAgPvjgAw4cOMBrr73GunXrWLhwoWn9peKaMmUKkyZN4t9//+Xtt9/m1KlTrF+/3tQClefdd9/lrbfeYt26dbz++uvs27eP1atXExQUZLafra0t8+fPR6fT8fLLL9OvXz+2bNlS4LXznrM33niD33//nTfeeIOjR48ybdo0pk+ffleP51a//PILRqPRrGvlrbp3705iYqKp1XPevHlMmzaNmJgY3n77bT744AMyMzPN1uuaPHkyc+fOJTMzkzFjxjB+/HjOnj3LI488YtpnyJAhjBo1iq1bt/LWW28RExPDhg0b8j23t/P5558zYMAAfvrpJ9566y3i4uLYuHFjvolTXFxc2LZtG0OHDmXNmjW89tprfP3119SsWdO0IHEeX19f01pc/fv3L3IsQoiKS6NY09erQgghykyPHj04cuQIJ0+etHQoQlitnj17cujQoSKNURRCCGm5EkKICiAzM9Ps9smTJ1mzZg3t2rWzTEBC3Afi4uJYvXq1tFoJIYpMWq6EEKIC8Pf3Z9CgQVSvXp2zZ88ya9YssrOz+ffff/Ot3SRERRcTE8P27dv5/vvv2bNnD6dOncLPz8/SYQkh7gMyoYUQQlQAnTt35ueff+bSpUvY29vTokULPvjgA0mshCjAli1beO6556hatSrz58+XxEoIUWTSciWEEEIIIYQQJUDGXAkhhBBCCCFECZDkSgghhBBCCCFKgIy5KoDRaCQ2NhZXV1c0Go2lwxFCCCGEEEJYiKIopKamEhAQgFZ7+7YpSa4KEBsbm2+xRyGEEEIIIUTFdf78+XyLjd9KkqsCuLq6AuoT6ObmZtFY9Ho969evp1OnTtja2lo0FqGSOrE+UifWRerD+kidWB+pE+si9WF9rKlOUlJSCAoKMuUItyPJVQHyugK6ublZRXLl5OSEm5ubxV9YQiV1Yn2kTqyL1If1kTqxPlIn1kXqw/pYY50UZbiQTGghhBBCCCGEECVAkishhBBCCCGEKAGSXAkhhBBCCCFECZAxV3dJURRyc3MxGAyleh29Xo+NjQ1ZWVmlfi1RNBWhTnQ6HTY2NrIUgRBCCCFEMUhydRdycnKIi4sjIyOj1K+lKAp+fn6cP39ePuhaiYpSJ05OTvj7+2NnZ2fpUIQQQggh7guSXBWT0WgkJiYGnU5HQEAAdnZ2pfoB22g0kpaWhouLyx0XLRNlo7zXiaIo5OTkcPnyZWJiYggPDy+Xj1MIIYQQoqRJclVMOTk5GI1GgoKCcHJyKvXrGY1GcnJycHBwkA+4VqIi1ImjoyO2tracPXvW9FiFEEIIIcTtlc9PhmWgvH6oFiKPvMaFEEIIIYpHPj0JIYQQQgghRAmQ5EoIIYQQQghhNQxGhV0xSey9omFXTBIGo2LpkIpMkisLMRgVdp5K5H/7L7LzVOJ99aLJU61aNWbMmFHk/Tdv3oxGo+HatWulFpMQQgghhLh/RR2Oo9VHf/Ds3H9YcFLHs3P/odVHfxB1OM7SoRWJTGhhAVGH45i06ihxyVmmMn93ByZ0r0Pnev4lfr07zWY4YcIEJk6cWOzz7tmzB2dn5yLv37JlS+Li4nB3dy/2te5WrVq1iImJ4ezZs/j5+ZXZdYUQQgghRPFEHY5j6MJ93NrkcCk5i6EL9zHr2QdK5bNySbJ4y9VXX31FtWrVcHBwICIigt27dxe6r16vZ/LkyYSGhuLg4EDDhg2Jiooy22fixIloNBqzrVatWqX9MIos70Vzc2IFN140pZGVx8XFmbYZM2bg5uZmVjZixAjTvnmLIxeFt7d3sWZMtLOzw8/Pr8zWhvrrr7/IzMzkySefZP78+WVyzdvR6/WWDkEIIYQQwioZjAqTVh3Nl1gBprJJq45afW8viyZXixcvZvjw4UyYMIF9+/bRsGFDIiMjSUhIKHD/sWPH8s033/DFF19w9OhRXn75ZXr27Mm///5rtl/dunXNkoe//vqrVB+Hoihk5OTecUvN0jNh5ZHbvmgmrjxKapbe7LjMHEOB51OUor24/Pz8TJu7uzsajcZ0+9ixY7i6urJ27VqaNGmCvb09f/31F6dOneLxxx/H19cXFxcXmjVrxsaNG83Oe2u3QI1Gw/fff0/Pnj1xcnIiPDyclStXmu6/tVvgDz/8QKVKlVi3bh21a9fGxcWFzp07Exd3I8HMzc3ltddeo1KlSnh6ejJq1CgGDhxIjx497vi458yZw9NPP03//v2ZO3duvvsvXLhAv379qFy5Ms7OzjRt2pRdu3aZ7l+1ahXNmjXDwcEBLy8vevbsabrPw8ODFStWmJ2vUqVK/PDDDwCcOXMGjUbD4sWLadu2LQ4ODvz0008kJibSr18/AgMDcXJyon79+vz8889m5zEajXz88ceEhYVhb29P1apVef/99wF4+OGHGTZsmNn+ly9fxs7Ojk2bNt3xORFCCCGEsAZZegPnkzLYd+4qUYcv8f7qo/kaH26mAHHJWeyOSSq7IO+CRbsFTp8+nSFDhvDcc88BMHv2bFavXs3cuXN555138u3/448/MmbMGLp27QrA0KFD2bhxI59++ikLFy407WdjY1OmXcAy9QbqjF93z+dRgEspWdSfuL5I+x+dHImTXclU4TvvvMMnn3xC9erV8fDw4Pz583Tt2pX3338fe3t7FixYQPfu3Tl+/DhVq1Yt9DyTJk3i448/Ztq0aXzxxRc888wznD17lsqVKxe4f0ZGBp988gk//vgjWq2WZ599lhEjRvDTTz8B8NFHH/HTTz8xb948ateuzeeff86KFSto3779bR9PamoqS5YsYdeuXdSqVYvk5GS2bdtG69atAUhLS6Nt27YEBgaycuVK/Pz82LdvH0ajEYDVq1fTs2dPxowZw4IFC8jJyWHNmjV39bx++umnNG7cGAcHB7KysmjSpAmjRo3Czc2N1atX079/f0JDQ2nevDkAo0eP5rvvvuOzzz6jVatWxMXFcezYMQBeeOEFhg0bxqeffoq9vT0ACxcuJDAwkIcffrjY8QkhhBBClBRFUUjJyuVyahYJqdlcTs0mISWby2nZJKTcVJaaTXLm3fXoSUgtPAGzBhZLrnJycti7dy+jR482lWm1Wjp06MDOnTsLPCY7OzvfYqaOjo75WqZOnjxJQEAADg4OtGjRgqlTp942IcjOziY7O9t0OyUlBVC7cd3alUuv16MoCkaj0fRBPO9nWbs5huIcU9DPiRMn8sgjj5j2q1SpEvXr1zfdnjRpEsuXL+d///sfr7zyiqk877nIM3DgQPr06QPAlClTmDlzJn///TedO3c2u2beptfr+frrrwkNDQXglVde4b333jPt+8UXX/DOO+/w+OOPAzBz5kzWrFmT77q3WrRoEeHh4dSuXRuAPn368P333/PQQw8BakJy+fJldu3aZUr8qlevborv/fffp0+fPkyYMMF0zvr162M0Gs1aDG+N4ebHBvD666/na2UbPny46fdXXnmFqKgoFi9eTNOmTUlNTeXzzz9n5syZ9O/fH4CQkBBatmyJ0WikR48eDBs2jOXLl9O7d29AbQEcOHAgiqIUuTWzKPIeq16vR6fTldh5S0Pe+1S6XloHqQ/rI3VifaROrIvUx+0ZjAqJ6TmmxOhKWjYJqerty2lqwqT+nkN2btE/m9rqNPi42uPlYo+NVsPec9fueIynk02Z11Nxrmex5OrKlSsYDAZ8fX3Nyn19fU3f0t8qMjKS6dOn06ZNG0JDQ9m0aRPLli3DYDCY9omIiOCHH36gZs2axMXFMWnSJFq3bs3hw4dxdXUt8LxTp05l0qRJ+crXr1+fb0xRXqtYWloaOTk5gJpg7Bz+4B0f877zybyy5L877vfVU7V5IOjOkz7oM9NJySre+KWsrCz1W4XrCWRGRgYANWvWNJWB2rLz0UcfsX79ei5duoTBYCAzM5OTJ0+a9jMajWRlZZkdFxYWZnbb1dWVc+fOkZKSYrpWamoqWq2WrKwsnJyc8Pb2Nh3j7u5OQkICKSkpJCcnEx8fT506dczO2aBBA3Jzc83KbjVnzhx69epl2qdHjx5069aNKVOm4Orqyp49e6hfvz42NjYFnmf//v0888wzt71GZmam2f2Kopiej7S0NECdUOPmfQwGA9OnT2f58uXExcWh1+vJzs7Gzs6OlJQU9u7dS3Z2NhEREYVeu3fv3nz//fd07tyZAwcOcPjwYX788cfbxno3cnJyyMzMZOvWrUUeh2dpGzZssHQI4iZSH9ZH6sT6SJ1Yl4pWHzkGSNFDSg6k6DW3/ISUHA0pekjTg0LRP3M66hTc7MDVVsHNFtzswM1WLXOzA3dbBVdbcLIBjUYPpGFU4NQlHddygAKvpVDJDi4f/Zs1d/44XaLyPsMWxX01W+Dnn3/OkCFDqFWrFhqNhtDQUJ577jmz8TRdunQx/d6gQQMiIiIIDg7m119/ZfDgwQWed/To0WatCSkpKQQFBdGpUyfc3NzM9s3KyuL8+fO4uLiYtaIVZf67TpUr4bcuhviUrALHXWkAP3cHOjUMRqdVX1SKopCamoqrq2uJTATh4OCARqMxPa685NHPz8/ssY4aNYqNGzeaxv44OjrSu3dvs2O1Wi0ODg5mx7m5uZnd1mq12NnZ4ebmZrqWq6srbm5uODg4YGtra7a/k5MTiqLg5uZmaoVxdnY228fGxgaj0ZivbvIcPXqUPXv2sHfvXrNZEA0GA2vWrGHIkCG4u7tjY2NT6DkcHR3zPbY8iqKg0Wjy3Z+bm2sqc3FxAcDHx8dsn48++ohvvvmG6dOnU79+fZydnXnzzTdNj8fLywsAFxeXQmMbOnQoDzzwACkpKSxZsoT27dtTr169Ave9F1lZWTg6OtKmTZt8LcbWRq/Xs2HDBjp27Iitra2lw6nwpD6sj9SJ9ZE6sS7lqT4UReFapp4rqTkkpGXf1NqUY+qWl9fylJZd9C9PtRrwdLbD29Uebxd79aernanlyef6bS9nexzt7q7Hi221eF795YD6OG4q11z/d8oTDYms61vAkaWrOF9gWyy58vLyQqfTER8fb1YeHx9f6Hgpb29vVqxYQVZWFomJiQQEBPDOO++YunMVpFKlStSoUYPo6OhC97G3tzeNX7mZra1tvjeYwWBAo9Gg1WrRaos3H4hWCxMfq8PQhfvQUNCLBiZ0r4OtzY0XZF73srxr3qu8cxT08+bz79ixg0GDBtGrVy9Abck6c+YM7dq1M9vv1rgKel7yym691q0x3BqPh4cHvr6+7N27l3bt2gHq8//vv//SqFGjQp+PefPm0aZNG7766qt85fPmzeOll16iYcOGzJkzh2vXrhU4HqxBgwb8+eefBSbkRqMRLy8vLl26ZIrh5MmTZGRkFPpYb35eH3/8cQYMGGA618mTJ6lTpw5arZaaNWvi6OjIn3/+aeoqeauGDRvStGlT5syZw88//8yXX35ZIq+NW2m1WjQaTYHvA2t1P8VaEUh9WB+pE+sjdWJdrLk+9AajmhSl3EiYElKzbvo9myvXk6ccQ9G75tnbaPFxs8fH1QFvF/vrv1+/7aomUT5u9ng625u+/C8t3RpVwcZGl2/JIr9SXLKoKIrzmrBYcmVnZ0eTJk3YtGmTaUyK0Whk06ZN+WZDu5WDgwOBgYHo9Xp+++0309iTgqSlpXHq1CnT+BVL61zPn1nPPmB1L5pbhYeHs2zZMrp3745Go2HcuHEWGVv26quvMnXqVMLCwqhVqxZffPEFV69eLbQVT6/X8+OPPzJ58uR8rTkvvPAC06dP58iRI/Tr148PPviAHj16MHXqVPz9/fn3338JCAigRYsWTJgwgUceeYTQ0FD69u1Lbm4ua9asYdSoUQC0bt2ar776ipYtW2IwGBg1alSR3njh4eEsXbqUHTt24OHhwfTp001dH0F9bY8aNYqRI0diZ2fHQw89xOXLlzly5IhZopc3sYWzs7PZLIZCCCGEKB6DUWFXTBJ7r2jwjEmiRZhPqScRN0vPzlWTo5Ss6xM/mCdOeclTUnpOsc7r7mirJkluNxIlH1f7GwmTqwM+bva42tuU2TI5RdG5nj8d6/ixMzqB9dt20al1RJnXyb2waLfA4cOHM3DgQJo2bUrz5s2ZMWMG6enpptkDBwwYQGBgIFOnTgVg165dXLx4kUaNGnHx4kUmTpyI0Whk5MiRpnOOGDGC7t27ExwcTGxsLBMmTECn09GvXz+LPMaC5L1odsckkZCahY+rA81DKlvVi2b69Ok8//zztGzZEi8vL0aNGlXiY3qKYtSoUVy6dIkBAwag0+l48cUXiYyMLHSChZUrV5KYmFhgwlG7dm1q167NnDlzmD59OuvXr+ett96ia9eu5ObmUqdOHVNrV7t27ViyZAnvvfceH374IW5ubrRp08Z0rilTpvD666/TunVrAgIC+Pzzz9m7d+8dH8/YsWM5ffo0kZGRODk58eKLL9KjRw+Sk5NN+4wbNw4bGxvGjx9PbGws/v7+vPzyy2bn6devH2+88Qb9+vWz+i57QgghhLWKOhx30xfeOhac/Af/EvjC22hUSMq4MQHEzYnTjYRJnT0vI8dw5xNep9NqTF3y8hInbxd7vN0cTIlT3k97G+uejOp2dFoNESGVSfxPIcLKPiPfiUYpyenF7sKXX37JtGnTuHTpEo0aNWLmzJlEREQA6gfcatWqmdYO2rJlC0OHDuX06dO4uLjQtWtXPvzwQwICAkzn69u3L1u3biUxMRFvb29atWrF+++/X2gXq4KkpKTg7u5OcnJygWOuYmJiCAkJKZMPtUajkZSUFNzc3Eql69f9xmg0Urt2bXr37s17771nsRgsXSdnzpwhNDSUPXv28MADD5TKNcr6tX4v9Ho9a9asoWvXrlbbnaMikfqwPlIn1kfqxPKiDscxdOG+fOPg8z7Gz3r2gXwJVnauwaw1KW8M0+XULFOLU96YptxiLHbrZKe7KTm6qTueqz0+bje661V2skN7HyUa98Ka3iO3yw1uZfEJLYYNG1ZoN8DNmzeb3W7bti1Hjx697fl++eWXkgpNWIGzZ8+yfv162rZtS3Z2Nl9++SUxMTE8/fTTlg7NIvR6PYmJiYwdO5YHH3yw1BIrIYQQojwzGBUmrTpa4ARjeWVvLTlA1OFLN1qc0rK5llG8KcArO9vl74p3a+Lkao+LvcU/kosSIjUprJpWq+WHH35gxIgRKIpCvXr12Lhxo2n9qopm+/bttG/fnho1arB06VJLhyOEEELcV9Kzc4lOSGPt4UtmY98L3tfAiv2x+cptdZoCu+LdPK4pbwIIOxvpdVTRSHIlrFpQUBDbt2+3dBhWo127diW6ULAQQghRHiVn6Im+nMrJ+DSiE9I4maD+vHgts1jneaxhAO1qepu1OlVysrWqCSCEdZHkSgghhBBC3JcS07I5eT15OpWQxskENaFKSM0u9BgvF3Wtpv/iUu94/n7Nq9Ii1LMkQxblnCRXQgghhBDCaimKQkJq9vVWqFRTMhWdkHbb6cn93R0I83EhzMeFcB9Xwn1dCPN2wcPZDoNRodVHf3ApOavAcVca1GVymofkXwtTiNuR5EoIIYQQQlic0agQm5x5oxUq/npLVEIaqVm5hR4XVNmRMG8Xwn1drydSLoT6uODmUPgMczqthgnd6zB04T40YJZg5XX4m9C9zn01BbiwDpJcCSGEEEKIMmMwKpxPyjC1Pp1MSCX6+u+Frfmk1UA1T2dCrydP4b5qa1R1b2ec7O7u42znev7MevaBm9a5UvmVwDpXouKS5EoIIYQQQpQ4vcHI2cR0NYGKT7sxNupyGjm5xgKPsdVpCPFyJtzH1SyRqubpjINtyS+K27mePx3r+LEzOoH123bRqXUELcJ8pMVK3DVJroQQQgghxF3L0huIuZJ+06x86qQSMVfSC11I195GS6i3i6kbX7ivC2E+rgR7OmGrK9vpy3VaDREhlUn8TyEipLIkVuKeSHJlKUYDnN0BafHg4gvBLUFb8t/IlKR27drRqFEjZsyYAUC1atV44403eOONNwo9RqPRsHz5cnr06HFP1y6p8wghhBDi7mTk5HL6crppRr68bn1nE9MpJIfCyU5nGgMV7uNqSqSqeDhJEiPKJUmuLOHoSogaBSk3LUznFgCdP4I6j5X45bp3745erycqKirffdu2baNNmzYcOHCABg0aFOu8e/bswdnZuaTCBGDixImsWLGC/fv3m5XHxcXh4eFRotcqTGZmJoGBgWi1Wi5evIi9vX2ZXFcIIYSwBqlZerO1oU7Gq5NKXLha+BpRrg421PBVkyfTDH2+rvi7OaCVJEpUIJJclbWjK+HXAXDrxJ8pcWp57wUlnmANHjyYXr16ceHCBapUqWJ237x582jatGmxEysAb2/vkgrxjvz8/MrsWr/99ht169ZFURRWrFhBnz59yuzat1IUBYPBgI2NvFWFEEKUrKvpOURfvjErX97YqEspWYUe4+lsdz1xcjHN0Bfu44K3q70srCsEULadWssrRYGc9DtvWSmwdiT5Eiv1JOqPqFHqfjcfp88o+HxKIW3wt+jWrRve3t788MMPZuVpaWksWbKEwYMHk5iYSL9+/QgMDMTJyYn69evz888/3/a81apVM3URBDh58iRt2rTBwcGBOnXqsGHDhnzHjBo1iho1auDk5ET16tUZN24cer0egB9++IFJkyZx4MABNBoNGo3GFLNGo2HFihWm8xw6dIiHH34YR0dHPD09efHFF0lLSzPdP2jQIHr06MEnn3yCv78/np6evPLKK6Zr3c6cOXN49tlnefbZZ5kzZ06++48cOUKfPn2oVKkSrq6utG7dmlOnTpnunzt3LnXr1sXe3h5/f3+GDRsGwJkzZ9BoNGatcteuXUOj0bB582YANm/ejEajYe3atTRp0gR7e3v++usvTp06xeOPP46vry8uLi40a9aMjRs3msWVnZ3NqFGjCAoKwt7enrCwMObMmYOiKISFhfHJJ5+Y7b9//340Gg3R0dF3fE6EEELcnxRF4XJqNjtOXWHBzjOMW3GYvt/upOmUDTR+bwNPzd7Ju8sPMW/7GbadvGJKrHzd7GkV5sWgltV4v2c9Fr/4IHvHdmDvuI4sfqkFU3rUZ9BDITwU5oWPm4MkVkJcJ1+HlwR9BnwQUAInUtSugh8GmUq0QKXCdn83Fuzu3C3PxsaGAQMG8MMPPzBmzBjTf4BLlizBYDDQr18/0tLSaNKkCaNGjcLNzY3Vq1fTv39/QkNDad68+R2vYTQaeeKJJ/D19WXXrl0kJycXOBbL1dWVH374gYCAAA4dOsSQIUNwdXVl5MiR9OnTh8OHDxMVFWVKHNzd3fOdIz09ncjISFq0aMGePXtISEjghRdeYNiwYWYJ5J9//om/vz9//vkn0dHR9OnTh0aNGjFkyJBCH8epU6fYuXMny5YtQ1EU3nzzTc6ePUtwcDAAFy9epF27djz00ENs3LiRSpUqsX37dnJz1fU3Zs2axfDhw/nwww/p0qULycnJbN++/Y7P363eeecdPvnkE6pXr46Hhwfnz5+na9euvP/++9jb27NgwQK6d+/O8ePHqVq1KgADBgxg586dzJw5k4YNGxITE8OVK1fQaDQ8//zzzJs3jxEjRpiuMW/ePNq0aUNYWFix4xNCCHH3DEaFXTFJ7L2iwTMmqURmp1MUhUspWTeNhboxLio5s/AvFgMrOV6f1tzFNENfmI8L7o6FrxElhCicJFcVxPPPP8+0adPYsmUL7dq1A9QP17169cLd3R13d3ezD96vvvoq69at49dffy1ScrVx40aOHTvGunXrCAhQE80PPviALl26mO03duxY0+/VqlVjxIgR/PLLL4wcORJHR0dcXFywsbG5bTfARYsWkZWVxYIFC0xjvr788ku6d+/ORx99hK+vLwAeHh58+eWX6HQ6atWqxaOPPsqmTZtum1zNnTuXLl26mMZ3RUZGMm/ePCZOnAjAV199hbu7O3PmzMHT0xOtVkuNGjVMx0+ZMoW33nqL119/3VTWrFmzOz5/t5o8eTIdO3Y03a5cuTINGzY03X7vvfdYvnw5K1euZNiwYZw4cYJff/2VDRs20KFDBwCqV69u2n/QoEGMHz+e3bt307x5c/R6PYsWLcrXmiWEEKJ0RR2Ou2ldJR0LTv6DfzHWVTIaFS5eyzSbVCJv0d207IIX2tVqoGplJ8J8XG/qzudCqLcLzvbyUVCIkiTvqJJg66S2It3J2R3w05N33u+ZpersgagtQimpqbi5uqLV3tKL09apyCHWqlWLli1bMnfuXNq1a0d0dDTbtm1j8uTJABgMBj744AN+/fVXLl68SE5ODtnZ2Tg5Fe0a//33H0FBQabECqBFixb59lu8eDEzZ87k1KlTpKWlkZubi5ubW5EfR961GjZsaDaZxkMPPYTRaOT48eOm5Kpu3brodDdmYPT39+fQoUOFntdgMDB//nw+//xzU9mzzz7LiBEjGD9+PFqtlv3799OqVStsbfN/o5eQkEBsbCyPPPJIsR5PQZo2bWp2Oy0tjYkTJ7J69Wri4uLIzc0lMzOTc+fOAWoXP51OR9u2bQs8X0BAAI8++ihz586lefPmrFq1iuzsbJ566ql7jlUIIUTRRB2OY+jCffkGB1xKzmLown3MevYBU4KVazBy7uaFdq9PKnHqchpZ+oLXiLLRaqjm5Xy9FcqFMF9XwrxdqO5dOmtECSHyk+SqJGg0ReqeR+jD6qyAKXEUPO5Ko94f+vCNadmNRrA1qOe/NbkqpsGDB/Pqq6/y1VdfMW/ePEJDQ00fxqdNm8bnn3/OjBkzqF+/Ps7Ozrzxxhvk5OTc0zVvtnPnTp555hkmTZpEZGQk7u7u/PLLL3z66acldo2b3ZoAaTQajMaC/yABrFu3josXL+abwMJgMLBp0yY6duyIo6Njocff7j7AlBwrN42VK2wM2K2zMI4YMYINGzbwySefEBYWhqOjI08++aSpfu50bYAXXniB/v3789lnnzFv3jz69OlT5ORZCCHEvTEYFSatOnq7UdeMWHKQlQdiOZWQTsyVdHIMBf/NsrPRUt3L2TSZRN4MfcGeztjZyHB6ISxJkquypNWp063/OgDQYJ5gXe9r3fnDUlvvqnfv3rz++ussWrSIBQsWMHToUNP4q+3bt/P444/z7LPPAmqL2YkTJ6hTp06Rzl27dm3Onz9PXFwc/v7qt25///232T47duwgODiYMWPGmMrOnj1rto+dnR0Gg+GO1/rhhx9IT083JSHbt29Hq9VSs2bNIsVbkDlz5tC3b1+z+ADef/995syZQ8eOHWnQoAHz588vMClydXWlWrVqbNq0ifbt2+e7P292xbi4OBo3bgyQb8r5wmzfvp1BgwbRs2dPQG3JOnPmjOn++vXrYzQa2bJli6lb4K26du2Ks7Mzs2bNIioqiq1btxbp2kIIIe7d7pik610BC5eWncuaQ5dMtx1tdaZFdsN81TFRYT4uBHk4YlPGC+0KIYpGkquyVucxdbr1Ate5+rBU1rnK4+LiQp8+fRg9ejQpKSkMGjTIdF94eDhLly5lx44deHh4MH36dOLj44ucXHXo0IEaNWowcOBApk2bRkpKSr4kJTw8nHPnzvHLL7/QrFkzVq9ezfLly832qVatGjExMezfv58qVarg6uqab52pZ555hgkTJjBw4EAmTpzI5cuXefXVV+nfv7+pS2BxXb58mVWrVrFy5Urq1atndt+AAQPo2bMnSUlJDBs2jC+++ILBgwczduxYPDw8+Pvvv2nevDk1a9Zk4sSJvPzyy/j4+NClSxdSU1PZvn07r776Ko6Ojjz44IN8+OGHhISEkJCQYDYG7XbCw8NZtmwZ3bt3R6PRMG7cOLNWuGrVqjFw4ECef/5504QWZ8+eJSEhgd69ewOg0+kYNGgQo0ePJjw8vMBum0IIIUpeSpae3w8WYfgA0KNRAI83DiTcx4UAd0dZI0qI+4x87WEJdR6DNw7DwN+h1xz15xuHSjWxyjN48GCuXr1KZGSk2fiosWPH8sADDxAZGUm7du3w8/OjR48eRT6vVqtl+fLlZGZm0rx5c1544QXef/99s30ee+wx3nzzTYYNG0ajRo3YsWMH48aNM9unV69edO7cmfbt2+Pt7V3gdPBOTk6sW7eOpKQkmjVrxpNPPskjjzzCl19+Wbwn4yZ5k2MUNF7qkUcewdHRkYULF+Lp6cnGjRtJT0+nffv2NGnShO+++87UBXHgwIHMmDGDr7/+mrp169KtWzdOnjxpOtfcuXPJzc2lSZMmvPHGG0yZMqVI8U2fPh0PDw9atmxJ9+7diYyM5IEHHjDbZ9asWTz55JP83//9H7Vq1WLIkCGkp6eb7TN48GBycnJ47rnnivsUCSGEKIb07FxWHojlxQX/0HTKRn7ada5Ix/VpVpX2NX2o4uEkiZUQ9yGNohRxsaQKJCUlBXd3d5KTk/NNtpCVlUVMTAwhISE4ODiUeixGo5GUlBTc3NzyT2ghLOJ+rpNt27bxyCOPcP78+Tu28pX1a/1e6PV61qxZQ9euXQucbESULakP6yN1Ujay9Ab+PJbA7wfj2HQs3mziiepeTiSk5hQ6o58G8HN34K9RD9/ztOyi+OQ9Yn2sqU5ulxvcSroFClEBZGdnc/nyZSZOnMhTTz11190nhRBCmMvONbD1xBV+PxjLxqPxpOfcGDcc7OlEtwb+dGsQQC0/V9YducTQhfuAAkddM6F7HUmshLjPSXIlRAXw888/M3jwYBo1asSCBQssHY4QQtzX9AYj26OvsOpAHOuPXiI160ZrVGAlR1NCVS/QzTRxFEDnev7MevaBm9a5UvkVY50rIYR1k+RKiApg0KBBZhOYCCGEKB6DUeHv04n8fjCWqMOXuJpxY9ZYXzd7Hq0fQLeG/jQOqmSWUN2qcz1/OtbxY2d0Auu37aJT6whahPlIi5UQ5YQkV0IIIYQQBTAaFfacSeL3g3GsPRzHlbQbaz96udjRpZ4/3Rr406xa5WJNPqHTaogIqUzifwoRIZUlsRKiHJHk6i7JPCCivJPXuBCiIlIUhX/PX2PVgVjWHIojPiXbdF8lJ1u61POjW4MAIkIqy1pTQoh8JLkqprzZSjIyMnB0dLRwNEKUnoyMDACLz9AjhBClTVEUDl9M4feDsfx+MI6L1zJN97k62BBZ149uDfx5KMwLW0mohBC3IclVMel0OipVqkRCQgKgrrl0u77V98poNJKTk0NWVtZ9N+13eVXe60RRFDIyMkhISKBSpUrodDpLhySEECVOURSOXUo1JVRnEzNM9znb6ehQx5fuDQJoXcMLexv5f1AIUTSSXN0FPz8/AFOCVZoURSEzMxNHR8dSTeJE0VWUOqlUqZLptS6EEOVFdEIqqw7E8fvBWE5dvrHQuoOtlkdq+dKtgT/ta/ngYCsJlRCi+CS5ugsajQZ/f398fHzQ6/V3PuAe6PV6tm7dSps2baR7lpWoCHVia2srLVZCiHLjbGI6vx+MY9WBWI5dSjWV29loaVfDm24NA3iklg/O9vKxSAhxb+R/kXug0+lK/QOoTqcjNzcXBweHcvtB/n4jdSKEENbvwtUMVh+M4/eDcRy6mGwqt9FqaFPDm24N/OlYxxdXB/l/XAhRciS5EkIIIUS5cCk5i9WH1C5//567ZirXaTW0DPWkWwN/Iuv6UcnJznJBCiHKNUmuhBBCCHHfupyaTdThOFYdiGPP2STyVpHQaCAipDLdGgTQpZ4fni72lg1UCFEhSHIlhBBCiPvK1fQcoo5c4veDsew8lYjxpmX5mgZ70K2BP13r++Pj5mC5IIUQFZIkV0IIIYSwesmZetYfucTvB+PYHn2F3JsyqoZV3OnWIIBHG/gTUEnWoBRCWI4kV0IIIYSwSmnZuWz6L55VB2LZeuIKOQaj6b46/m50a+hPt/oBVPV0smCUQghxgyRXQgghhLAamTkG/jiWwO8HY/njWALZuTcSqhq+LnRrEEC3Bv5U93axYJRCCFEwSa6EEEIIYVFZegNbTlzm94NxbPovnowcg+m+EC9nujXwp1uDAGr6uVowSiGEuDOtpQP46quvqFatGg4ODkRERLB79+5C99Xr9UyePJnQ0FAcHBxo2LAhUVFR93ROIYQQQpS9nFwjfxyLZ/ji/TSbspGXftzLqgOxZOQYqOLhyMttQ/n91Vb88VZb3upUUxIrIcR9waItV4sXL2b48OHMnj2biIgIZsyYQWRkJMePH8fHxyff/mPHjmXhwoV899131KpVi3Xr1tGzZ0927NhB48aN7+qcQgghhCgbuQYjO08n8vuBOKKOXCI5U2+6z9/dgUfr+9OtYQANq7ij0WgsGKkQQtwdiyZX06dPZ8iQITz33HMAzJ49m9WrVzN37lzeeeedfPv/+OOPjBkzhq5duwIwdOhQNm7cyKeffsrChQvv6pxCCCGEKD0Go8LumCR+PxhL1OFLJKbnmO7zcrHn0fp+dGsYQJOqHmi1klAJIe5vFkuucnJy2Lt3L6NHjzaVabVaOnTowM6dOws8Jjs7GwcH8zUrHB0d+euvv+76nHnnzc7ONt1OSUkB1G6Ier2+sMPKRN71LR2HuEHqxPpInVgXqQ/rU9Z1YjQq/Hv+GqsPx7PuSDwJqTf+xno42RJZ15dH6/nRrJoHuusJlcGQi8FQ2BnLH3mfWBepD+tjTXVSnBgsllxduXIFg8GAr6+vWbmvry/Hjh0r8JjIyEimT59OmzZtCA0NZdOmTSxbtgzD9f+N7+acAFOnTmXSpEn5ytevX4+Tk3VM77phwwZLhyBuIXVifaROrIvUh/UpzTpRFDiXDv9e0fJvooZrOTdaoRx1Cg0qKzzgpRDunotOc4akY2dYV/if5gpD3ifWRerD+lhDnWRkZBR53/tqtsDPP/+cIUOGUKtWLTQaDaGhoTz33HPMnTv3ns47evRohg8fbrqdkpJCUFAQnTp1ws3N7V7Dvid6vZ4NGzbQsWNHbG1tLRqLUEmdWB+pE+si9WF9SqtOFEXhv0uprDkUz5rDlzh/NdN0n7O9jg61fOha349WoZ7Y2Vh8Di2rIu8T6yL1YX2sqU7yerUVhcWSKy8vL3Q6HfHx8Wbl8fHx+Pn5FXiMt7c3K1asICsri8TERAICAnjnnXeoXr36XZ8TwN7eHnt7+3zltra2Fq/MPNYUi1BJnVgfqRPrIvVhfUqqTk7Ep/L7gVh+PxjH6SvppnJHWx2P1PahW4MA2tX0xsFWd8/XKu/kfWJdpD6sjzXUSXGub7Hkys7OjiZNmrBp0yZ69OgBgNFoZNOmTQwbNuy2xzo4OBAYGIher+e3336jd+/e93xOIYQQQhQu5ko6vx+IZdXBWE7Ep5nK7Wy0PFzTh24N/Xm4lg9OdvdVpxghhChRFv0fcPjw4QwcOJCmTZvSvHlzZsyYQXp6ummmvwEDBhAYGMjUqVMB2LVrFxcvXqRRo0ZcvHiRiRMnYjQaGTlyZJHPKYQQQoiiOZ+Uwe8H4/j9YCxHYm90i7HVaWhbw5tuDQLoUMcXF3tJqIQQAiycXPXp04fLly8zfvx4Ll26RKNGjYiKijJNSHHu3Dm02ht9tLOyshg7diynT5/GxcWFrl278uOPP1KpUqUin1MIIYQQhYtLzmT1wThWHYzjwPlrpnKdVsNDYV50a+BPZB0/3J2k65QQQtzK4l81DRs2rNAue5s3bza73bZtW44ePXpP5xRCCCEqEoNRYVdMEnuvaPCMSaJFmI9p+vM8CalZrDkYx+8H4/jn7FVTuVYDD1b3pFuDADrX86Oys11Zhy+EEPcViydXQgghhCgdUYfjmLTqKHHJWYCOBSf/wd/dgQnd69A8xJO1h+NYdSCWXTFJKMqN45pXq0y3hv50rueHj6tDoecXQghhTpIrIYQQohyKOhzH0IX7UG4pj0vO4uWF+9BqwHjTnY2CKtGtgT+PNvDH392xTGMVQojyQpIrIYQQopwxGBUmrTqaL7G6mVGBugGudG8YyKP1/Qmq7FRm8QkhRHklyZUQQghRzuyOSbreFfD2xj5alxahnmUQkRBCVAyyXLoQQghRjhiNClGH44q0b0LqnRMwIYQQRSctV0IIIUQ5YDQqRB25xIyNJ8wW+b0dmaxCCCFKliRXQgghxH1MURTWHYlnxsYTHLuUCoCrvQ4FDenZuQWOu9IAfu4ONA+pXKaxCiFEeSfJlRBCCHEfUhSFjf8l8NmGExyNSwHA1d6G51uF8HyrEHaeusLQhfvQgFmClbfC1YTudfKtdyWEEOLeSHIlhBBC3EcUReHP4wl8tuEkhy4mA+Bsp+P5ViG80Ko67k62AHSu58+sZx+4aZ0rld/1da461/O3SPxCCFGeSXIlhBBC3AcURWHLict8tvEkB85fA8DJTsegltUY0ro6Hs52+Y7pXM+fjnX82BmdwPptu+jUOoIWYT7SYiWEEKVEkishhBDCiimKwvboRKZvOM6+c9cAcLTVMaBlMC+2ro6ni/1tj9dpNUSEVCbxP4WIkMqSWAkhRCmS5EoIIYSwUjtOXeGzDSfYc+YqAPY2Wvo/GMxLbUPxdr19UiWEEKLsSXIlhBBCWJldpxP5bOMJ/j6dBICdjZZnIqoytG0oPm4yfboQQlgrSa6EEEIIK/HPmSQ+23iC7dGJANjptPRrHsTQdmH4uUtSJYQQ1k6SKyGEEMLC9p27ymcbTrDt5BUAbHUaejcN4pX2YQRUcrRwdEIIIYpKkishhBDCQg6cv8ZnG0+w+fhlAGy0Gp5qWoVX2odRxcPJwtEJIYQoLkmuhBBCiDJ2+GIyMzaeYON/CYA6o1+vBwJ59eFwgipLUiWEEPcrSa6EEEKIMnI0NoUZG0+w/mg8AFoN9GxchVcfDqOal7OFoxNCCHGvJLkSQgghStnxS6nM2HiCtYcvAaDRQI9Ggbz6cBjVvV0sHJ0QQoiSIsmVEEIIUUpOxqcyY9NJ1hyKQ1HUpKpbgwBefySMMB9XS4cnhBCihElyJYQQQpSwU5fTmLnpJCsPxKIoatmj9f15vUM4NXwlqRJCiPJKkishhBCihMRcSeeLTSdZsf8ixutJVee6frzeIZza/m6WDU4IIUSpk+RKCCGEuEfnEjOY+cdJlv97EcP1rKpjHV/e6BBO3QB3C0cnhBCirEhyJYQQQtyl80kZfPlHNEv3XTAlVQ/X8uHNDjWoX0WSKiGEqGgkuRJCCCGK6eK1TL78I5ol/5wn93pS1baGN292rEGjoEqWDU4IIYTFSHIlhBBCFFFcciZf/3mKX/acQ29Qk6rW4V680aEGTYI9LBydEEIIS5PkSgghhLiD+JQsZm0+xaLd58jJNQLQMtSTNzvWoFm1yhaOTgghhLWQ5EoIIYQoREJqFrM3n+anXWfJvp5UNQ+pzJsdatAi1NPC0QkhhLA2klwJIYQQt7iSls03W07x499nydKrSVXTYA+Gd1STKo1GY+EIhRBCWCNJroQQQojrktJz+HbraebvOEOm3gBA46qVeLNDDVqHe0lSJYQQ4rYkuRJCCFHhXcvI4bttp/lh+xnSc9SkqmEVd97oWIN2NbwlqRJCCFEkklwJIYSosJIz9Mz56zRzt58hLTsXgLoBbgzvWIOHa/lIUiWEEKJYJLkSQghR4aRk6Zn7Vwxz/oohNUtNqmr7u/Fmh3A61vGVpEoIIcRdkeRKCCFEhZGWncsP22P4dutpUq4nVTV9XXmzYzid6vih1UpSJYQQ4u5JciWEEKLcS8/OZf7OM3y79TTXMvQAhPu48HqHcLrW85ekSgghRImQ5EoIIUS5lZGTy487z/LN1tMkpecAUN3bmdcfCadbgwB0klQJIYQoQVpLB/DVV19RrVo1HBwciIiIYPfu3bfdf8aMGdSsWRNHR0eCgoJ48803ycrKMt0/ceJENBqN2VarVq3SfhhCCCGsSGaOge+3nabNx38yde0xktJzqObpxGd9GrLhzbY83ihQEishhBAlzqItV4sXL2b48OHMnj2biIgIZsyYQWRkJMePH8fHxyff/osWLeKdd95h7ty5tGzZkhMnTjBo0CA0Gg3Tp0837Ve3bl02btxoum1jIw10QghREWTpDSzadY5ZW05xOTUbgKqVnXjtkXB6NArARmfx7xSFEEKUYxbNOqZPn86QIUN47rnnAJg9ezarV69m7ty5vPPOO/n237FjBw899BBPP/00ANWqVaNfv37s2rXLbD8bGxv8/PxK/wEIIYSwCtm5BhbvOc9Xf0YTn6ImVVU8HHnt4XB6PhCIrSRVQgghyoDFkqucnBz27t3L6NGjTWVarZYOHTqwc+fOAo9p2bIlCxcuZPfu3TRv3pzTp0+zZs0a+vfvb7bfyZMnCQgIwMHBgRYtWjB16lSqVq1aaCzZ2dlkZ2ebbqekpACg1+vR6/X38jDvWd71LR2HuEHqxPpInViXsqyPnFwjS/ddZNaW01y6nlT5uzvwf22r80TjAOxstGA0oDcaSj0WaybvEesjdWJdpD6sjzXVSXFi0CiKopRiLIWKjY0lMDCQHTt20KJFC1P5yJEj2bJlS77WqDwzZ85kxIgRKIpCbm4uL7/8MrNmzTLdv3btWtLS0qhZsyZxcXFMmjSJixcvcvjwYVxdXQs858SJE5k0aVK+8kWLFuHk5HSPj1QIIURJMxhh92UN6y5ouZqjjp1yt1PoFGjkQR8FG2moEkIIUUIyMjJ4+umnSU5Oxs3N7bb73leDkTZv3swHH3zA119/TUREBNHR0bz++uu89957jBs3DoAuXbqY9m/QoAEREREEBwfz66+/Mnjw4ALPO3r0aIYPH266nZKSQlBQEJ06dbrjE1ja9Ho9GzZsoGPHjtja2lo0FqGSOrE+UifWpTTrI9dgZMWBOL7afJoLVzMB8HG156U2IfRpEoi9ra5Er1deyHvE+kidWBepD+tjTXWS16utKCyWXHl5eaHT6YiPjzcrj4+PL3S81Lhx4+jfvz8vvPACAPXr1yc9PZ0XX3yRMWPGoNXm/6qyUqVK1KhRg+jo6EJjsbe3x97ePl+5ra2txSszjzXFIlRSJ9ZH6sS6lGR95BqM/G9/LDP/OMnZxAwAvFzsGdoulGciquIgSVWRyHvE+kidWBepD+tjDXVSnOtbrOOEnZ0dTZo0YdOmTaYyo9HIpk2bzLoJ3iwjIyNfAqXTqX9QC+vdmJaWxqlTp/D39y+hyIUQQpQVg1Fhxb8X6fjZVt5acoCziRl4Otsxpmttto1sz+BWIZJYCSGEsBoW7RY4fPhwBg4cSNOmTWnevDkzZswgPT3dNHvggAEDCAwMZOrUqQB0796d6dOn07hxY1O3wHHjxtG9e3dTkjVixAi6d+9OcHAwsbGxTJgwAZ1OR79+/Sz2OIUQQhSP0ajw+6E4Pt94glOX0wHwcLLlpbahDGgRjJPdfdWrXQghRAVh0b9Offr04fLly4wfP55Lly7RqFEjoqKi8PX1BeDcuXNmLVVjx45Fo9EwduxYLl68iLe3N927d+f999837XPhwgX69etHYmIi3t7etGrVir///htvb+8yf3xCCCGKx2hUiDpyiRkbT3AiPg0Ad0dbXmxTnYEtq+FiL0mVEEII62Xxv1LDhg1j2LBhBd63efNms9s2NjZMmDCBCRMmFHq+X375pSTDE0IIUQYURWHdkXhmbDzBsUupALg52PBC6+o891A1XB1kDIQQQgjrZ/HkSgghRMWlKAob/0vgsw0nOBqnzsbkam/D861CeL5VCO6OklQJIYS4f0hyJYQQoswpisKfxxP4bMNJDl1MBsDZTsfzrUIY3CqESk52Fo5QCCGEKD5JroQQQpQZRVHYcuIyn208yYHz1wBwstMxqGU1hrSujoezJFVCCCHuX5JcCSGEKDEGo8KumCT2XtHgGZNEizAfdFoNiqKwPTqR6RuOs+/cNQAcbXUMaBnMi62r4+mSf61BIYQQ4n4jyZUQQogSEXU4jkmrjhKXnAXoWHDyH/zdHejTLIjt0VfYc+YqAPY2Wvo/GMxLbUPxdpWkSgghRPkhyZUQQoh7FnU4jqEL93Hrcu5xyVnM2HgSADsbLc9EVGVo21B83BzKPkghhBCilElyJYQQ4p4YjAqTVh3Nl1jdzMlOx4Y32xLo4VhmcQkhhBBlTXvnXYQQQojC7Y5Jut4VsHAZOQbOJWWUUURCCCGEZUhyJYQQ4p4kpN4+sSrufkIIIcT9SpIrIYQQdy0jJ5c1h+KKtK+Pq4yzEkIIUb7JmCshhBB3ZcepK4z67SDnkzJvu58G8HN3oHlI5bIJTAghhLAQabkSQghRLGnZuYxZfoinv9vF+aRMAis58urDYWhQE6mb5d2e0L0OOu2t9wohhBDli7RcCSGEKLItJy7z7rJDXLymtlY9E1GVd7rUwtXBlroBbjetc6Xyc3dgQvc6dK7nb6mQhRBCiDIjyZUQQog7Ss7U8/7qo/z6zwUAgio78tETDWgZ5mXap3M9fzrW8WNndALrt+2iU+sIWoT5SIuVEEKICkOSKyGEELe16b943l1+iPiUbDQaGNiiGm9H1sTZPv+fEJ1WQ0RIZRL/U4gIqSyJlRBCiApFkishhBAFupaRw6RVR1n+70UAQryc+fjJBjSrJhNTCCGEEAWR5EoIIUQ+UYfjGLviCFfSstFq4IXW1XmzQw0c7XSWDk0IIYSwWpJcCSGEMElMy2b8yiOsPqiuXRXm48K0JxvQuKqHhSMTQgghrJ8kV0IIIVAUhd8PxjFh5RGS0nPQaTW83LY6rz4cjoOttFYJIYQQRSHJlRBCVHAJqVmMW3GYdUfiAajl58q0JxtSv4q7hSMTQggh7i+SXAkhRAWlKArL/73IpFVHSc7UY6PV8Er7MF5pH4adjawxL4QQQhSXJFdCCFEBxSVnMmb5Yf44lgBA3QA3pj3ZkDoBbhaOTAghhLh/SXIlhBAViKIo/PrPeab8/h+p2bnY6bS83iGcF9tUx1YnrVVCCCHEvZDkSgghKogLVzMYvewQ205eAaBhUCWmPdmAGr6uFo5MCCGEKB8kuRJCiHLOaFT4afc5PlzzH+k5BuxttLzVqQaDW1VHp9VYOjwhhBCi3JDkSgghyrGziemM+u0gf59OAqBpsAcfP9mA6t4uFo5MCCGEKH8kuRJCiHLIaFT4YccZpq07TqbegKOtjpGdazKwRTW00lolhBBClApJroQQopw5fTmNkUsP8s/ZqwA8WL0yH/VqQLCns4UjE0IIIcq3YiVXRqORLVu2sG3bNs6ePUtGRgbe3t40btyYDh06EBQUVFpxCiGEuAODUeH7baeZvuEE2blGnO10jO5am6ebV5XWKiGEEKIMFGne3czMTKZMmUJQUBBdu3Zl7dq1XLt2DZ1OR3R0NBMmTCAkJISuXbvy999/l3bMQgghbnEyPpUnZu1g6tpjZOcaaR3uxbo32/Dsg8GSWAkhhBBlpEgtVzVq1KBFixZ89913dOzYEVtb23z7nD17lkWLFtG3b1/GjBnDkCFDSjxYIYQQ5vQGI99uPc3nG0+SYzDi6mDDuEfr8FTTKmg0klQJIYQQZalIydX69eupXbv2bfcJDg5m9OjRjBgxgnPnzpVIcEIIIQp3NDaFt5ce4EhsCgAP1/Lhg5718XN3sHBkQgghRMVUpOTqTonVzWxtbQkNDb3rgIQQQtxeTq6Rr/6M5qs/o8k1Krg72jKhex16Ng6U1iohhBDCgu56tsDc3Fy++eYbNm/ejMFg4KGHHuKVV17BwUG+MRVCiNJy6EIyby89wLFLqQBE1vXlvR718HGV/3uFEEIIS7vr5Oq1117jxIkTPPHEE+j1ehYsWMA///zDzz//XJLxCSGEALL0BmZuOsk3W09jMCpUdrZj0mN16dbAX1qrhBBCCCtR5ORq+fLl9OzZ03R7/fr1HD9+HJ1OB0BkZCQPPvhgyUcohBAV3L5zVxm59CDRCWkAdGvgz6TH6uLpYm/hyIQQQghxsyInV3PnzmX+/Pl8/fXXBAQE8MADD/Dyyy/Tq1cv9Ho93333Hc2aNSvNWIUQokLJ0hv4dP1x5vwVg1EBLxd7pvSoR+d6fpYOTQghhBAFKNI6VwCrVq2iX79+tGvXji+++IJvv/0WNzc3xowZw7hx4wgKCmLRokXFDuCrr76iWrVqODg4EBERwe7du2+7/4wZM6hZsyaOjo4EBQXx5ptvkpWVdU/nFEIIa7M7Jokun2/ju21qYvVE40A2vNlGEishhBDCihVrzFWfPn2IjIxk5MiRREZGMnv2bD799NO7vvjixYsZPnw4s2fPJiIighkzZhAZGcnx48fx8fHJt/+iRYt45513mDt3Li1btuTEiRMMGjQIjUbD9OnT7+qcQghhTTJycvk46jjzd55BUcDXzZ4Petbnkdq+lg5NCCGEEHdQ5JarPJUqVeLbb79l2rRpDBgwgLfffjtfy1FRTZ8+nSFDhvDcc89Rp04dZs+ejZOTE3Pnzi1w/x07dvDQQw/x9NNPU61aNTp16kS/fv3MWqaKe04hhLAWO6KvEDljKz/sUBOr3k2rsP7NtpJYCSGEEPeJIrdcnTt3jhEjRvDff//RoEEDPvnkE/bu3cv7779Pw4YNmTFjBl26dCnyhXNycti7dy+jR482lWm1Wjp06MDOnTsLPKZly5YsXLiQ3bt307x5c06fPs2aNWvo37//XZ8TIDs7m+zsbNPtlBR1QU69Xo9ery/yYyoNede3dBziBqkT63O/10lqVi4frz/BL3suABDg7sCUHnVoHeYF3H+P636vj/JI6sT6SJ1YF6kP62NNdVKcGDSKoihF2bFdu3b4+fkxaNAg1q1bx6lTp1i5ciUA//33Hy+99BJ+fn78+uuvRbpwbGwsgYGB7NixgxYtWpjKR44cyZYtW9i1a1eBx82cOZMRI0agKAq5ubm8/PLLzJo1657OOXHiRCZNmpSvfNGiRTg5ORXp8QghxN3475qGX05puZajTqf+kK+Rx6oacbjrhTKEEEIIUZIyMjJ4+umnSU5Oxs3N7bb7FvnP9z///MOBAwcIDQ0lMjKSkJAQ0321a9dm69atfPvtt3cfdRFs3ryZDz74gK+//pqIiAiio6N5/fXXee+99xg3btxdn3f06NEMHz7cdDslJYWgoCA6dep0xyewtOn1ejZs2EDHjh2xtbW1aCxCJXVife7HOknJ1DM16gRL/7sIQBUPRz7oUYcW1T0tHNm9ux/ro7yTOrE+UifWRerD+lhTneT1aiuKIidXTZo0Yfz48QwcOJCNGzdSv379fPu8+OKLRb6wl5cXOp2O+Ph4s/L4+Hj8/AqeDWvcuHH079+fF154AYD69euTnp7Oiy++yJgxY+7qnAD29vbY2+dfL8bW1tbilZnHmmIRKqkT63O/1Mmm/+J5d/kh4lOy0WhgYItqjOxcEye78tVcdb/UR0UidWJ9pE6si9SH9bGGOinO9Ys8ocWCBQvIzs7mzTff5OLFi3zzzTd3FVweOzs7mjRpwqZNm0xlRqORTZs2mXXpu1lGRgZarXnIeYsYK4pyV+cUQoiycjU9hzcX72fw/H+IT8kmxMuZX19qwcTH6pa7xEoIIYSoiIr81zw4OJilS5eW6MWHDx/OwIEDadq0Kc2bN2fGjBmkp6fz3HPPATBgwAACAwOZOnUqAN27d2f69Ok0btzY1C1w3LhxdO/e3ZRk3emcQghhCVGH4xi74ghX0rLRauCF1tUZ3rEGDrY6S4cmhBBCiBJSpOQqPT0dZ2fnIp+0qPv36dOHy5cvM378eC5dukSjRo2IiorC11eddvjcuXNmLVVjx45Fo9EwduxYLl68iLe3N927d+f9998v8jmFEKIsXUnLZsLKI6w+GAdAmI8L055sQOOqHhaOTAghhBAlrUjJVVhYGK+//joDBw7E39+/wH0URWHjxo1Mnz6dNm3amE2HfjvDhg1j2LBhBd63efNm82BtbJgwYQITJky463MKIURZUBSFVQfjmLjyCEnpOei0Gl5uW51XHw6X1iohhBCinCpScrV582beffddJk6cSMOGDWnatCkBAQE4ODhw9epVjh49ys6dO7GxsWH06NG89NJLpR23EEJYrYTULMYuP8z6o+rkOrX8XJn2ZEPqV3G3cGRCCCGEKE1FSq5q1qzJb7/9xrlz51iyZAnbtm1jx44dZGZm4uXlRePGjfnuu+/o0qWLaeyTEEJUNIqisGzfRSb/fpTkTD02Wg3DHg7j/9qFYWdT5PmDhBBCCHGfKtb0VFWrVuWtt97irbfeKq14hBDivhSXnMm7yw7x5/HLANQLdGPakw2p7W/ZtfKEEEIIUXZk7l8hhLgHiqLw6z/nmfL7f6Rm52Kn0/J6h3BebFMdW520VgkhhBAViSRXQghxly5czWD0skNsO3kFgEZBlZj2ZAPCfV0tHJkQQgghLEGSKyGEKCajUeGn3ef4cM1/pOcYsLfR8lanGgxuVR2dVmPp8IQQQghhIZJcCSFEMZxNTGfUbwf5+3QSAE2DPfj4yQZU93axcGRCCCGEsDRJroQQoggMRoX5O84wbd1xMvUGHG11jOxck4EtqqGV1iohhBBCcBfJVbVq1Xj++ecZNGgQVatWLY2YhBDCqpy6nMbIpQfZe/YqAA9Wr8xHvRoQ7Ols4ciEEEIIYU2KPZXVG2+8wbJly6hevTodO3bkl19+ITs7uzRiE0IIizIYFb7Zcoqun29j79mrONvpmNKjHoteeFASKyGEEELkc1fJ1f79+9m9eze1a9fm1Vdfxd/fn2HDhrFv377SiFEIIcrcifhUnpi1g6lrj5Gda6R1uBfr3mzDsw8GSzdAIYQQQhTorhdheeCBB5g5cyaxsbFMmDCB77//nmbNmtGoUSPmzp2LoiglGacQQpQJvcHIl3+cpNvMvzhw/hquDjZ83KsBC55vThUPJ0uHJ4QQQggrdtcTWuj1epYvX868efPYsGEDDz74IIMHD+bChQu8++67bNy4kUWLFpVkrEIIUaqOxqbw9tIDHIlNAeDhWj580LM+fu4OFo5MCCGEEPeDYidX+/btY968efz8889otVoGDBjAZ599Rq1atUz79OzZk2bNmpVooEIIUVpyco18+Wc0X/8ZTa5Rwd3Rlgnd69CzcSAajXQBFEIIIUTRFDu5atasGR07dmTWrFn06NEDW1vbfPuEhITQt2/fEglQCCFK06ELyby99ADHLqUCEFnXl/d61MPHVVqrhBBCCFE8xU6uTp8+TXBw8G33cXZ2Zt68eXcdlBBClLYsvYHPN53k262nMRgVKjvbMemxunRr4C+tVUIIIYS4K8VOrhISErh06RIRERFm5bt27UKn09G0adMSC04IIUrDvnNXGbn0INEJaQB0a+DPpMfq4ulib+HIhBBCCHE/K/Zsga+88grnz5/PV37x4kVeeeWVEglKCCFKQ2aOgfdXH+XJWTuITkjDy8We2c824cunH5DESgghhBD3rNgtV0ePHuWBBx7IV964cWOOHj1aIkEJIURJ2x2TxMilBziTmAHAE40DGd+9DpWc7CwcmRBCCCHKi2InV/b29sTHx1O9enWz8ri4OGxs7npmdyGEKBXp2blMW3ec+TvPoCjg62bPBz3r80htX0uHJoQQQohyptjZUKdOnRg9ejT/+9//cHd3B+DatWu8++67dOzYscQDFEKIu7Uj+gqjlh3kfFImAH2aBvHuo7Vxd8w/y6kQQgghxL0qdnL1ySef0KZNG4KDg2ncuDEA+/fvx9fXlx9//LHEAxRCiNsxGBV2xSSx94oGz5gkWoT5kJGTy9S1x1i06xwAgZUcmfpEfdrU8LZwtEIIIYQoz4qdXAUGBnLw4EF++uknDhw4gKOjI8899xz9+vUrcM0rIYQoLVGH45i06ihxyVmAjgUn/8HDyQ5QuJqhB+DZB6syqnMtXB3k/ychhBBClK67GiTl7OzMiy++WNKxCCFEkUUdjmPown0ot5RfzcgBwNPFji/6NaZlqFfZByeEEEKICumuZ6A4evQo586dIycnx6z8scceu+eghBDidgxGhUmrjuZLrG5mq9USEeJZZjEJIYQQQhQ7uTp9+jQ9e/bk0KFDaDQaFEX9eKPRaAAwGAwlG6EQQtxid0zS9a6AhbuUksXumCRahEqCJYQQQoiyUexFhF9//XVCQkJISEjAycmJI0eOsHXrVpo2bcrmzZtLIUQhhDB3JDa5SPslpN4+ARNCCCGEKEnFbrnauXMnf/zxB15eXmi1WrRaLa1atWLq1Km89tpr/Pvvv6URpxBCcCk5i883nWTxnnNF2t/H1aGUIxJCCCGEuKHYyZXBYMDV1RUALy8vYmNjqVmzJsHBwRw/frzEAxRCiKvpOczacor5O86QnWsEwN5Ga/r9VhrAz92B5iGVyzBKIYQQQlR0xU6u6tWrx4EDBwgJCSEiIoKPP/4YOzs7vv32W6pXr14aMQohKqj07Fzm/hXDt1tPk5qdC0Czah68HVmLpPRshi7cB2A2sYXm+s8J3eug02oQQgghhCgrxU6uxo4dS3p6OgCTJ0+mW7dutG7dGk9PTxYvXlziAQohKp7sXAOLdp3jqz+juZKmzkha29+NkZE1aVfT2zSBzqxnH7hpnSuVn7sDE7rXoXM9f4vELoQQQoiKq9jJVWRkpOn3sLAwjh07RlJSEh4eHqYPPEIIcTcMRoVl+y4wY+NJLl7LBCDY04nhHWvQvUEA2ltaojrX86djHT92RiewftsuOrWOoEWYj7RYCSGEEMIiipVc6fV6HB0d2b9/P/Xq1TOVV64s4xqEEHdPURTWHbnEJ+tPEJ2QBoCvmz2vPRJO76ZB2OoKn9hUp9UQEVKZxP8UIkIqS2IlhBBCCIspVnJla2tL1apVZS0rIUSJ2R59hY+jjnHggjq9urujLf/XLpSBLavhYKuzcHRCCCGEEEVX7G6BY8aM4d133+XHH3+UFishxF3bf/4a09YdY3t0IgBOdjoGtwphSJvquDnYWjg6IYQQQojiK3Zy9eWXXxIdHU1AQADBwcE4Ozub3b9v374SC04IUf6ciE/lk3XHWX80HgBbnYZnIoJ5pX0Y3q72Fo5OCCGEEOLuFTu56tGjR4kH8dVXXzFt2jQuXbpEw4YN+eKLL2jevHmB+7Zr144tW7bkK+/atSurV68GYNCgQcyfP9/s/sjISKKioko8diFE0ZxPymDGxpMs//cCRgW0GujZuApvdAgnqLKTpcMTQgghhLhnxU6uJkyYUKIBLF68mOHDhzN79mwiIiKYMWMGkZGRHD9+HB8fn3z7L1u2jJycHNPtxMREGjZsyFNPPWW2X+fOnZk3b57ptr29fCMuhCVcTs3mqz+j+WnXWfQGdUWqyLq+jOhUk3BfVwtHJ4QQQghRcoqdXJW06dOnM2TIEJ577jkAZs+ezerVq5k7dy7vvPNOvv1vHef1yy+/4OTklC+5sre3x8/Pr/QCF0LcVkqWnm+3nGbu9hgyctRJcB4K8+TtyFo0Cqpk2eCEEEIIIUpBsZMrrVZ72/WsijOTYE5ODnv37mX06NFm5+/QoQM7d+4s0jnmzJlD375984392rx5Mz4+Pnh4ePDwww8zZcoUPD09CzxHdnY22dnZptspKSmAOvW8Xq8v8uMpDXnXt3Qc4gapk9vLzDHw465zfLsthuTMXAAaBLrxVsdwWoaq78GSfu6kTqyL1If1kTqxPlIn1kXqw/pYU50UJwaNoihKcU7+v//9L9/F/v33X+bPn8+kSZMYPHhwkc8VGxtLYGAgO3bsoEWLFqbykSNHsmXLFnbt2nXb43fv3k1ERAS7du0yG6OV15oVEhLCqVOnePfdd3FxcWHnzp3odPmndp44cSKTJk3KV75o0SKcnGQsiBBFYTDC35c1rDuvJVmvfgHj66jwaJCRBpUVZI1xIYQQQtyPMjIyePrpp0lOTsbNze22+xY7uSrMokWLWLx4cb7k63buNbl66aWX2LlzJwcPHrztfqdPnyY0NJSNGzfyyCOP5Lu/oJaroKAgrly5cscnsLTp9Xo2bNhAx44dsbWV6amtgdSJOaNRYfXhS3y+6RRnkzIACHB34PVHQnm8YUCZLOordWJdpD6sj9SJ9ZE6sS5SH9bHmuokJSUFLy+vIiVXJTbm6sEHH+TFF18s1jFeXl7odDri4+PNyuPj4+84Xio9PZ1ffvmFyZMn3/E61atXx8vLi+jo6AKTK3t7+wInvLC1tbV4ZeaxpliEqqLXiaIo/Hk8gWnrTvBfnNqV1tPZjmEPh/F0RFXsbcp+AeCKXifWRurD+kidWB+pE+si9WF9rKFOinP9EkmuMjMzmTlzJoGBgcU6zs7OjiZNmrBp0ybTFO9Go5FNmzYxbNiw2x67ZMkSsrOzefbZZ+94nQsXLpCYmIi/v3+x4hNCFGx3TBLT1h1jz5mrALja2/Bim+o83yoEZ3uLz5MjhBBCCGERxf4U5OHhYTahhaIopKam4uTkxMKFC4sdwPDhwxk4cCBNmzalefPmzJgxg/T0dNPsgQMGDCAwMJCpU6eaHTdnzhx69OiRb5KKtLQ0Jk2aRK9evfDz8+PUqVOMHDmSsLAwIiMjix2fEOKGI7HJfLLuOH8evwyAvY2WQS2r8XLbUDyc7SwcnRBCCCGEZRU7ufrss8/MkiutVou3tzcRERF4eHgUO4A+ffpw+fJlxo8fz6VLl2jUqBFRUVH4+voCcO7cObRardkxx48f56+//mL9+vX5zqfT6Th48CDz58/n2rVrBAQE0KlTJ9577z1Z60qIuxRzJZ3pG06w6kAsADqthj7Ngnjt4XD83B0sHJ0QQgghhHUodnI1aNCgEg9i2LBhhXYD3Lx5c76ymjVrUtg8HI6Ojqxbt64kwxOiwrqUnMXnm07y6z/nMRjV91z3hgEM71iDEC/nOxwthBBCCFGxFDu5mjdvHi4uLvkW7V2yZAkZGRkMHDiwxIITQljG1fQcZm05xfwdZ8jONQLQvqY3IyJrUjfA3cLRCSGEEEJYp2InV1OnTuWbb77JV+7j48OLL74oyZUQ97H07Fzm/hXDt1tPk5qtLgDcrJoHb0fWonlIZQtHJ4QQQghh3YqdXJ07d46QkJB85cHBwZw7d65EghJClK3sXAOLdp3jqz+juZKWA0BtfzdGRtakXU1vs3GWQgghhBCiYMVOrnx8fDh48CDVqlUzKz9w4EC+mfuEENbNYFRYtu8CMzae5OK1TACCPZ0Y3rEG3RsEoC2DBYCFEEIIIcqLYidX/fr147XXXsPV1ZU2bdoAsGXLFl5//XX69u1b4gEKIUqeoiisO3KJT9afIDohDQBfN3teeySc3k2DsNVp73AGIYQQQghxq2InV++99x5nzpzhkUcewcZGPdxoNDJgwAA++OCDEg9QCFGytkdf4eOoYxy4kAyAu6Mt/9culIEtq+Fgq7NwdEIIIYQQ969iJ1d2dnYsXryYKVOmsH//fhwdHalfvz7BwcGlEZ8QooTsP3+NaeuOsT06EQAnOx2DW4UwpE113BxsLRydEEIIIcT9r9jJVZ7w8HDCw8NLMhYhRCk4GZ/KJ+uPs+5IPAC2Og3PRATzSvswvF1lYW0hhBBCiJJS7OSqV69eNG/enFGjRpmVf/zxx+zZs4clS5aUWHBCiLt3PimDGRtPsvzfCxgV0GqgZ+MqvNEhnKDKTpYOTwghKi6jAc3ZvwhM2onmrBtUbwNa6ZYtRHlQ7ORq69atTJw4MV95ly5d+PTTT0siJiHEPbicms1Xf0bz066z6A0KAJF1fRnRqSbhvq4Wjk4IISq4oyshahQ2KbE0BTg7C9wCoPNHUOcxS0cnhLhHxU6u0tLSsLOzy1dua2tLSkpKiQQlhCi+lCw93245zdztMWTkGAB4KMyTtyNr0SiokmWDE0IIoSZWvw4AFPPylDi1vPcCSbCEuM8VO7mqX78+ixcvZvz48Wblv/zyC3Xq1CmxwIQQRZOZY2D+zjPM2nyK5Ew9AA2ruPN2ZC1ahXtZODohhBAAGA0QNYp8iRXcKFs9HCpXB3tXsHMGWyewdQRZyF2I+0axk6tx48bxxBNPcOrUKR5++GEANm3axM8//yzjrYQoQ3qDkV//Oc/MTSeJT8kGIMzHhRGdahBZ1w+N/DEWQoiypyiQkQiJ0XDlpPozMRriDkBK7O2PTb8Msx+6pVCjJll2Ttd/Ot902/kO5Xe630nGeoGMgRMlqtjJVffu3VmxYgUffPABS5cuxdHRkQYNGrBx40batm1bGjEKIW5iNCqsOhjLZxtOcCYxA4DASo680SGcJx6ogk4rSZUQQpS6nAxIOg2J1xOoK9eTqMSTkJV89+e1dQbFALlZ1wsU0KerW2mwcbiLpC2v3PH2+9rkH0ZidWQMnChhdzUV+6OPPsqjjz6ar/zw4cPUq1fvnoMSQuSnKAp/Hk9g2roT/Benjm/0dLZj2MNhPB1RFXsb+ZZNCCFKlNEA185B4qkbiVNeIpVy4TYHasA9CDxDwSscPMNAnwkbJ9z5mk8vhpDW6rX1GWoSp0+//jMDctILKL/T/beWZ2DqipibpW6ZSSXxjJnT2tySfBWWuN1Fa5yNw713l5QxcKIU3PU6V3lSU1P5+eef+f7779m7dy8Gg6Ek4hJC3GR3TBLT1h1jz5mrALja2zCkTXWebxWCi/09v42FEKLiurkb361d+ZJOgyGn8GMdKl1PnsLNE6nK1dVWnZsZDbD7G/WDe4HjrjRqi0lwS/WmVqeOvbIvhVleFUVN9u45Wcu8pez6Psbc6485F7KT1a2kabTXx6QVt7Xt+u82DvD7GxQ+Bk4DUe9ArUeli6Aolrv+VLZ161a+//57li1bRkBAAE888QRfffVVScYmRIV3JDaZT9Yd58/jlwGwt9EysGU1hrYNxcP5PuhuIYQQ1uLWbnyJp24kUlnXCj9OZ68mTp6hauLkeT2B8goHp8pFv75Wp3Y1+3UAoMH8Q/31FpjOH5bNB3mNRk047JzAuRQmPsrNKYXWtuvlBnWMMYoRctLUrVR6TCqQchHO7lBbEoUoomIlV5cuXeKHH35gzpw5pKSk0Lt3b7Kzs1mxYoXMFChECYq5ks70DSdYdUAd/KzTaujdNIjXHwnHz93BwtEJIYSVMhog+bz5+Ke8RCr5/G0OLKAbn2eomki5Vym5hKfOY2pXs6hR5pNbuAWoiVV56YJmY6dujh4lf25DrppwFTcpu/X+5AtwNebO1/v9Taj3BFRrDVWaga38DRa3V+Tkqnv37mzdupVHH32UGTNm0LlzZ3Q6HbNnzy7N+ISoUC4lZ/H5ppP8+s95DEb1W83uDQMY3rEGIV7OFo5OCCGsQFl14ystdR6DWo+Se3or+7eto1HrSGxkdrqi09mAzg0c3O7tPDHbYH63O++XeBK2fKRuNg4QFAEhbSCkLQQ0VuMR4iZFfkWsXbuW1157jaFDhxIeHl6aMQlR4VxNz2HWllPM33GG7FwjAO1qejOiU03qBbpbODohhLAAUze+vBaoYnTjq1wdvMLMu/F5hoGzZ5mFf1taHUpwKy4eSaFhcCtJrCwhuKXaYni7MXAuPtBuNJz5C2K2QnoCxGxRN94DOxf1PCFt1M23Pmi1ZfxAhLUpcnL1119/MWfOHJo0aULt2rXp378/ffv2Lc3YhCj30rNzmftXDN9uPU1qtjoAuGmwByM716J5SDH68gshxP0oXze+mxKponbjyxv/VBrd+ET5VZQxcF0/UVsamz6ntphePg5ntl1PsLapSf7J9eoGajfIaq2g2vVky7umLABdARU5uXrwwQd58MEHmTFjBosXL2bu3LkMHz4co9HIhg0bCAoKwtW1FGa0EaIcys41sGjXOb76M5oraWoXltr+boyMrEm7mt6yALAQovxQFMhIumn8U15XvlPXu/FlF37srd348hKpsuzGJ8qv4oyB02jAp5a6NR8CRiPEH1JbtGK2qhNfZF6F/1apG4Czz41WrZA24FFNkq0KoNgdRZ2dnXn++ed5/vnnOX78OHPmzOHDDz/knXfeoWPHjqxcubI04hSiXDAYFZbtu8CMjSe5eC0TgGBPJ4Z3rEH3BgFoZQFgIcT9Sp95y3pQp24kUvd7Nz5Rft3tGDitFvwbqlvLV8Ggh9j911u1tsL5XWo3wsNL1Q3Avao682BesuUWUOoPT5S9exqFV7NmTT7++GOmTp3KqlWrmDt3bknFJUS5oigK645c4pP1J4hOSAPA182e1x4Jp3fTIGx10kf7rhkNaM7+RWDSTjRn3UAGhgthriTfI3nd+PIW0pVufKI8KIkxcDpbCGqmbm1GgD4LLuy53o1wq/p78jnY/5O6gfpeyEu0qrUunWnxRZkrkSlOdDodPXr0oEePHiVxOiHKle3RV/g46hgHLqiLKLo72vJ/7UIZ2LIaDrbygeKeHF0JUaOwSYmlKcDZWde7c3xUfqY0FuJe3M17xNSN76apzIvdjS/sxibd+ERFZOtwvZWqNbR/F7LT4PzfN7oRxh240U32n+uNEz51byRbwS3BsZJFH4K4OzJ/pBClZP/5a0xbd4zt0YkAONnpGNwqhCFtquPmYGvh6MqBoyuvD0S+ZZanlDi1vPcCSbBExXan90iv78G71k0TSUTfZTe+m7rySTc+IQpm7wJhHdQNIPOaOk4rL9lKOHJj2zULNFrwb3SjG2HVFmAnS7LcDyS5EqKEnYxP5ZP1x1l3JB4AW52GZyKCeaV9GN6u9haOrpww5MLakRQ8fa4CaCDqHaj1qHQ3EhWT0aAO0i/0PQL8Nvj253APMm99yuvS5x4k7ysh7pVjJajVVd0A0i7f6EJ4Zpv6RUfsPnXb/jlobaFK0xstW1WagY18prBGklwJUQwGo8KumCT2XtHgGZNEizAfdNcnoTiflMGMjSdZ/u8FjApoNdCzcRXe6BBOUGUnC0d+H9BnQvoVSL9808/LBdy+AmnxoBhuczIFUi6q3wqGtC6zhyCE1Ti73Xz2s8LYOoNvHenGJ4SluXhDvSfUDSD54o1kK2arOqbx3E51kwWNrZrUghBFFHU4jkmrjhKXnAXoWHDyH/zdHXijQw3+i0vhp11n0RvUb4Qj6/oyolNNwn0r8PIEhlzITCokQboM6Ynm9+WklnwM0RvVrhTyB0dUFImn4NDSG2M47uSxmVD/ydKNSQhRfO6B0LCvuikKXI1R19bKS7byLWjsen1B49ayoLGFyScOIYog6nAcQxfuy9fBJi45i1G/HTTdfijMk7cja9EoqFKZxlcmFAWykiEj8fatSnm/ZyRRcJek29DZgbO3OmOSs3cBv1+/nRQDS5+78/m2z4B/F6rfBNbvrXapkDVGRHmTGg9HlsOhX+Hi3uId6+JbOjEJIUqORqO2JleuDk0G3ljQOGYrnNl604LG69QNbixoHNJWTba8asjfvzIiyZUQd2AwKkxadfS2aYKtTsOcAc1oU9O7zOIqEcXpipd+GYz6Yl5AA06VC0mUCkia7N2K9p+/XwN1xrOUOApN4Oyc1YH3GVdg97fq5lEN6j+lbt41i/lYhLAiWSlw7Hc4tARObwbFqJZrtFC9vdoatWkypF6i4PeIRn0PBbcsw6CFECXi5gWNI15Ux1jGH779gsYuvup077KgcamT5EqIO9gdk3S9KyBoMdJcewwfrpFAJXYba2FEi96gYGtjBc3vZdEVz871NknSLQmTU+XSGfiu1alTSf86ANBg/uHx+h+LHrOhZhf1g+fBX+HYarh6BrZOUze/BtcTrSdlIUdxf8jNVru6HvwVTkRBbtaN+wKbQoPeULcnuPioZXYut3+PdP5QJqYQojzQ6u68oHFafAELGl9PtEJay9/BEiTJlRB3kJCqfoCJ1O5mgu0CAjRJpvtilcpM0g9gnbG5ab8SldcVL/2K2gJTFl3xnLwKblXK+2ktg9zrPKZOtx41ynzgvluA+qExbxr28I7qlpMOx9eq3/RHb4RLB9Vtw3i160T9p9RjHD0s83iEKIjRqE5OcWgJHP2f+RTpnuFqQlX/SbW70K2K+h4RQpQvhS1onNeydfGf6wsaL1Q3kAWNS5AkV0LcgY+rA5Ha3cyynZHvPj+SmGU7g6H6N/BxfbBoJ7xfu+JZozqPQa1HyT29lf3b1tGodSQ21dsU/G28nbP6IbT+k2oL3tEV6gfWczvVGZnObIM1IyC8k7pPjc7Wk0iKikVR4NIhdQzVod8g9abEyNUf6vVSvwzwb3jn925x3iNCiPLp5gWNGVO0BY19693oRigLGheLJFdC3EHzYHdCbBcA6vTqN9NqwKjAe3YL8HR4Ak4fKiRhulJKXfFu/ekNjpUr1ux4Wh1KcCsuHkmhYXCron1odPaEZoPV7do5dXa1Q0sg4ag6juXY7+pzXrs7NHhKHRAsH0ZFaUuKUbvsHFwCV47fKLd3V5Ok+k+prazFfS3ezXtECFF+5VvQ+Or1BY233VjQOP6wupktaHy9C6EsaHxbFegTmBB3QVHYuWYBrW7qCngrrQZ8SILv2hT9vFrborUqWVtXvPKoUlVoPVzd4o+o41kO/6auKXJgkbo5+6itBQ2egoAH7t+WPmF90i5fn+lvCVzYfaNcZw81ItVuf2Ed1W+ehRCiNDh6QK1H1Q3MFzSO2QpJp25a0HiGLGh8B5JcCWE0qt1ukk6r3xwnnVa3qzEYrpymVW560c5j5wLuVQpOmpxuue3gLh/QrZFvXeg4CR6ZoHaZOLRE/eCbnqB+e7drFlQOvTHjoFeYpSMW96PsNHWClUNL4NQfNxbE1mjVDyr1n1JbTR3cLRunEKJiut2Cxqe3QMqF/AsaV33w+nitNhV+QWOreORfffUV06ZN49KlSzRs2JAvvviC5s2bF7hvu3bt2LJlS77yrl27snr1agAURWHChAl89913XLt2jYceeohZs2YRHh5eqo9DWDGDXm2JMCVQNydRZ8CQXeBhOtRuf7d2ByxQv1+u92cW9z2tVu1jHtxSnZXw1B/q+Jdja9Rv8LZ8qG4BjdX1s+o9Aa5+lo5aWLPcHPPXUW7mjfvkdSSEsGYFLmi89UY3wvQEdWbe05vV/U0LGl9v2fKtV6EWNLZ4crV48WKGDx/O7NmziYiIYMaMGURGRnL8+HF8fHzy7b9s2TJycnJMtxMTE2nYsCFPPfWUqezjjz9m5syZzJ8/n5CQEMaNG0dkZCRHjx7FwUG6VpRb+iw1Ubp6U+KUl0RdO3fj2+GCaG3U7mHXF+nLdAli8o5sdie74xVQnV9yXkWTWtiaSrJeTLlmYwc1O6vbrS0Osf+q2/ox0uIg8jMab2oBXaEuk5CncnU1oZIWUCHE/cRsQeNB5gsax2yBM3+VzILGRgOas38RmLQTzVk3uI8m4rF4cjV9+nSGDBnCc889B8Ds2bNZvXo1c+fO5Z133sm3f+XKlc1u//LLLzg5OZmSK0VRmDFjBmPHjuXxxx8HYMGCBfj6+rJixQr69u1byo9IlKrs1BsJkymJut4SlXKR205DbuMAHiFQOeT6fwwh129XB/cgUxN2Tq6R5+fuZue1RAIrOfLloIfQnL/DmkqyXkzFYO8CDfuoW9pldcbBg7+qY2XyvrX7fbiMlano4o+oCdWhpWqLeR4ZuyeEKG/udkHjvCnfC1rQ+OhKiBqFTUosTQHOzrq+hMRH98USEhZNrnJycti7dy+jR482lWm1Wjp06MDOnTuLdI45c+bQt29fnJ3VWUtiYmK4dOkSHTp0MO3j7u5OREQEO3fuLDC5ys7OJjv7RrewlJQUAPR6PXp9cafBLll517d0HGVGUSDzKpqrZ+DqaTRXY67/HqP+nn759ofbuYBHCErl6ige1VA81ARK8QhRu9toCmmWNipg1KMoCmP+d5SdpxNxttPxzTONqOSgRR/eBU2veejWv4vmpmmRFbcADB3fRwnvAhWljqyQRd4n9pWg8SB1u3oG7ZFlaI8sRXPlBPy3Ev5biWLvhlKrO8Z6vVCqPlRhEvAK9/8WQPL566+B39AkHDUVK3Yu6mugbi+Uaq3UVnKA3NwyDa9C1omVkzqxLlIfJcirjro1exkMejRx+9Gc2Ybm7DY0F/agSYu//gXUEgAU9yCU4NYYq7UCfTa6tcMBhZu/flJS4uDXARh6zUOp1a3MH1JxXhcaRVGKueJoyYmNjSUwMJAdO3bQokULU/nIkSPZsmULu3btuu3xu3fvJiIigl27dpnGaO3YsYOHHnqI2NhY/P39Tfv27t0bjUbD4sWL851n4sSJTJo0KV/5okWLcHJyutuHJwqjKNjnJuOcHY9zdsL1n/E456i/2xkybnt4to0r6XY+pNv7km5/0087X3JsXO/p2+A/YjX876wODQpDahmp63HL20Mx4pl2HAf9NbJsK5HoUrPwhE1UPIqCW+Y5qlzdQZWrf+Oov2q6K9PWg4uVIrhQuSXJjsHSalEO2OWmEnBtD1WSduCZfsJUbtDYkODWkPMeLYh3b4RRa2fBKIUQwnpojTl4pJ/CO+0oXqlH8Ug/jRbzYRsKUNBfSAXItK3MhrrTy/yzV0ZGBk8//TTJycm4ubnddl+Ldwu8F3PmzKF+/fqFTn5RVKNHj2b48OGm2ykpKQQFBdGpU6c7PoGlTa/Xs2HDBjp27Iitra1FYykWowFSY9EkxaC5qk4aobne+sTVM2j0t0+gFBc/lMohUCkEpbLa8qR4VAOPELQO7rgCriUc8qb/Elj5934A3u1ai0EtggvcT6+PvD/rpByzvvfJUFCM5J7bgfbwb2iOrcQx6yphl6MIuxyF4hmGse6TGOv1UrumljPWVx8lKCcdzckotV5P/4HGqLZAKWhQgh9SW6hqdcfLsRJeFg71ZuW6Tu5TUifWRerDMgw5aRjP70Jz9i80x9egTTpVYGIFasLlpE/i0XqVUIJblWWYpl5tRWHR5MrLywudTkd8fLxZeXx8PH5+t58xKT09nV9++YXJkyeblecdFx8fb9ZyFR8fT6NGjQo8l729Pfb2+efnt7W1tZo3mDXFYpKbo04UUeAEEmfBkFP4sRqtOm153qDIvLFPlauDRzU0dk6FvrlKw5HYZIYvPYSiwDMRVXmhdSiaO7QsWGWdVHBWVydh7dUt91OI3qiOzzoRhSYxGt3WD9Ft/RACm6qTGtR7AlzyT+JzP7O6+rhbBr06nu7gr+qEJvqblmfwawANeqOp+wQa90CsvR273NRJOSJ1Yl2kPsqYrQfU6qxuAY3gt8F3PMQmMxHKuI6K85qwaHJlZ2dHkyZN2LRpEz169ADAaDSyadMmhg0bdttjlyxZQnZ2Ns8++6xZeUhICH5+fmzatMmUTKWkpLBr1y6GDh1aGg+jfMvJUGfgM5tA4noSlXweFGPhx2pt1UGKeZNH3JxIVaqqzsJmBeJTshj8wz9k5BhoHe7FxMfq3jGxEqJYbOxvLNCYlQLHflf7mp/eDBf/Ubd1o6F6O3UGuVqPgoNlW80rPEWB87tvrHWWceXGfR7Vbqx15l3TYiEKIUS54uJbsvtZiMW7BQ4fPpyBAwfStGlTmjdvzowZM0hPTzfNHjhgwAACAwOZOnWq2XFz5syhR48eeHp6mpVrNBreeOMNpkyZQnh4uGkq9oCAAFMCd98oq2kos5ILmYHvNKTG3f5YW6f8M/DlJVFugVY/gD8zx8CQBf9wKSWLUG9nvnz6AWx11v7ds7ivObhBo6fVLTVe/eB+6Fe4uFed3v3UH+rMljW7qB/ewzpazRcRFULCMbU+Di1RW+bzOHmprYv1e0OVpjJmTgghSlpwS3VWwJT7e+kbiydXffr04fLly4wfP55Lly7RqFEjoqKi8PVVs9Jz586hvWXhsePHj/PXX3+xfv36As85cuRI0tPTefHFF7l27RqtWrUiKirq/lrjqiSnoVQUyEg0T5puboXKSLz98fbu4HlL1728JMrF9779kGE0Kry1ZD8HLyTj4WTL3EHNcHeUrgCiDLn6woMvq1viKXXq7kO/QmK0mnQdWQ4OlaDO4+rU7lVbVqiFGMtM8gU4/BscXALxh26U27lArW5qklu9nWm5BiGEEKVAq1M/597nS99YxV+KYcOGFdoNcPPmzfnKatasye0mOdRoNEyePDnfeKz7xtGV119YtzzG69NQ0ntB/gTLaIS0S+YJVN529Qxk32EgnrN3weOfKoeoi7/dpwnU7UzfcII1hy5hq9PwTf+mBHs6WzokUZF5hkK7UdB2JMTtVz/oH/5NfV/vm69uboHX10nqra54Xw7fl2UmIwmO/k9NaM9ux/T/rdZGbS1s8BTU6AJ2MmOsEEKUmTqPqZ9zo0ZByo2lb9QGhg9lnStxF4wG9QVVYHPo9ckpf39T/cB19exNLVFnIDfz9ud2q3K9xSnkliQqBOxLeu4967Zs3wW+/DMagKlPNKB5SOU7HCFEGdFoIKCxunV6D85sU7uoHV2pLpS9Y6a6ede6Pu7nSXUMkLgzfSYcX6smVCfXg/GmdUuqtlQTqjo9wEn+PxBCCIup8xjUepTc01vZv20djVpHYlNaQ2NKgSRX1ubsDvNMPR9FHVi95u38d2l06kQRBU0g4REMto6lFvb9ZM+ZJN75Te3683/tQnmySRULRyREIbQ6tTta9XbQ9VM1ITj0K5xYB5ePwR/vqVtQhJpo1e0JztY0AbgVMORCzBY1Qf1vFeSk3bjPt56anNZ7EioFWS5GIYQQ5rQ6lOBWXDySQsPgVvdNYgWSXFmftPg77wPg1xCqtTJviXIPAp2MGbqds4npvLjgH3IMRrrU82NEJ5npS9wnbB3Ub/PqPAaZ19RE4dCvELMNzu9St7WjIPRhtdtgza5g72LpqC1DUdQJQg4tgcPLID3hxn3uVdWEqv5T4FvHcjEKIYQolyS5sjZFnV4y8n0IaV26sZQzyZl6Bs//h6sZeuoHujO9dyO0WhmzIu5DjpXggf7qlhKnjs06tEQdqxW9Qd1sndQEq/5TEPZIxfji5cpJdS2qQ0vUSXvyOFZWW/Ua9IYqzWVSECGEEKVGkitrU06mobQ2uQYjwxbtIzohDT83B74f2BRHu/uniVmIQrn5Q8th6nb5BBxeqiYYV2PU3w8vvZFc1H9K7UJYnpKLW5PLPHnJZYPeamteRUguhRBCWJwkV9amnExDaU0URWHiqiNsO3kFR1sd3w9siq/bfTQtvxBF5V0D2r8L7UbDxX1qt8HDv0H6Zfhnjrq5V4X6vdT1mu7XbnG3dovM+39So1Nb6eo/VbG7RQohhLAYSa6sUTmYhtKa/LDjDAv/PodGAzP6NqJeoLulQxKidGk0UKWJunV6//qEDkvVhCT5HPz1mbr51FVnyLsfJnTQZ8HJdWoL1Yn1YMi+cZ9M6CGEEMJKSHJlre7zaSitxZ/HEnjv96MAvNO5FpF1/SwckRBlTGejtuaEPQLdpsOJKHUNrZPrIeEIbDwCGyeqU5HXf1JNUKxlKnKjQZ2K/uAS+G+l+Xp9MhW9EEIIKyTJlTW7j6ehtAbHLqXw6s//YlSgT9MgXmxT3dIhCWFZto5q8lS3J2RevbGI7pm/4NwOdVs7EsI63OhaV9aL6CoKxP6rxpW3iHIet8CbZvqTRZSFEEJYH0muRLl0OTWbwT/8Q1p2Lg9Wr8x7PeqhkQ9iQtzg6AFNBqlb8oUbk0JcOqS2bp2IAltnqN1NTWaqt1dbwUpL4in1+oeWQGL0jXKHSlC3hzpGrGqL8jUZhxBCiHJHkitR7mTpDbz44z9cvJZJiJczs59tgp2NfCATolDuVeCh19Ut4diNJOfaWTi4WN2cvKDeE2qiVaVZ4a1GRgOas38RmLQTzVk3uF135tR4OLJMnd0wdt+NchuHm6aR7wA2diX/mIUQQohSIMmVKFcURWHk0oP8e+4a7o62zBnYlEpO8sFMiCLzqQWPjIOHx8KFPWric2QZZFyB3d+qW6VgNfFp0Bu8b1qI++hKiBqFTUosTQHOzro+Ec9HNybiyUq5PtPfEnWiDcWolmu0autY/afU1jJ717J+5EIIIcQ9k+RKlCufbzrJygOx2Gg1zPr/9u49Lso6//v4exjOiHgAdEAUNUUl8YRriFZuklK3ZdlBJUOz2hQNc+tHeq+id6nt1s+1+7eFaXnYzKzcNNc8pO6qlZmgabIe8ZxnReVgAjJz/0FS3HhAHLkumNfz8eDxaK65ZuY9ftB4c13zvZ7qqGZBLMUMVIrFIoX9ruSr9xRp/9qSQrRzackRra/fKvlq2LbklD2v2tLSUSp3fb6c4yWXlohNls4dLDnd8PKlX+8PjS4paZGPSLWCq+79AQBwG1CuUGN8sfWopq3eK0l6ve+d6tqcJZkBp7B6SC3iSr4K86Xdy0uKVtbqks9ondh+nQf/Ura+nfbrpvotSgpV28ekeiw0AwCoOShXqBG2HD6nVxb+KEl6/u5m6v+7xgYnAmooT79fVux7TMo/K+1YLKW/L53acePHtukrdXtJsrVjpT8AQI3Ep/xR7f107qKe/3uGCi/b1bN1A6X0bmV0JMA1+NWXOg+Vuv+xYvu37iOFtKdYAQBqLMoVqrXcS0UaOidDZ/IK1dpWW2/3by+rGz+4AVWqVgPn7gcAQDVFuUK1dbnYrhc//kG7T+Yq2N9LHyRGy8+LM12BKteka8mqgLrWLzYsJRcAbtK1KlMBAFDlKFeotiYt26l/7z4tbw83vZ8YrZA6PkZHAlyTm7VkuXVJ5QvWL7d7v3Ht610BAFBDUK5QLX248ZBmf3tQkjT1ifaKalTH0DyAy2vzkPTE36XatrLba4eUbL9ynSsAAGowzqFCtfP13tOasOQ/kqRXekXogba2GzwCQJVo85DU6kFd3r9eW79eqfbde8m92d0csQIAuAyOXKFayTqVq+EfbVGx3aFHO4Rq+L3NjY4E4LfcrHI06aaj9WLkaNKNYgUAcCmUK1Qb2fmFemZOhnIvXVZ0k7qa0q+tLCzpDAAAAJOgXKFaKLhcrD98mKHD2RfVuJ6v3hvUSV7u/EYcAAAA5kG5guk5HA6N+Xy70g+ek7+3u2YNjlb9Wl5GxwIAAADKoFzB9N5du0+fbzkqq5tF7wzsqDuC/Y2OBAAAAJRDuYKpLd9+XG+u3C1JmvBQpO5uGWRwIgAAAODqKFcwrR9/Oq+XPt0qSRrcNVyD7mpibCAAAADgOihXMKXjF37Ws3MzdKnIrnsjgvSnB1sbHQkAAAC4LsoVTCe/4LKGzsnQqdwCRTTw1/8M6CB3K9+qAAAAMDd+YoWpFNsdSl6wVTuO5yiwlqfeT4yWv7eH0bEAAACAG6JcwVT+smKXVu88KU93N703KFph9XyNjgQAAABUCOUKpvFJ+mG9t36/JOnNx6LUqUldgxMBAAAAFUe5gils2HdG/3tRpiQp+b4Werh9qMGJAAAAgJtDuYLh9p/O07B5W3TZ7tBD7UI0qmcLoyMBAAAAN41yBUOdv1iooXMzdOHnInVoXEd/eSxKFovF6FgAAADATTO8XL3zzjsKDw+Xt7e3unTpok2bNl13//PnzyspKUk2m01eXl5q2bKlli1bVnr/hAkTZLFYyny1atXqdr8NVELhZbuGzduiA2fyFVrHRzMGRcvbw2p0LAAAAKBS3I188U8++USjR4/W9OnT1aVLF02bNk29evXS7t27FRwcXG7/wsJCxcXFKTg4WAsXLlRoaKgOHTqkOnXqlNkvMjJSq1evLr3t7m7o28RVOBwOjVucqe/2n5Wfp1UfDI5WkL+X0bEAAACASjO0dUydOlXPPfechgwZIkmaPn26vvzyS82aNUuvvvpquf1nzZql7OxsbdiwQR4eJdc+Cg8PL7efu7u7GjZseFuz49a8//UBfZJxRG4W6W8DO6pVw9pGRwIAAABuiWHlqrCwUJs3b9aYMWNKt7m5ualnz5767rvvrvqYJUuWKCYmRklJSfriiy8UFBSkgQMHKiUlRVbrr6eT7d27VyEhIfL29lZMTIymTJmixo0bXzNLQUGBCgoKSm/n5ORIkoqKilRUVHSrb/WWXHl9o3M40+qdpzR5+U5J0tj4CHVrXrdavb+aOJPqjpmYC/MwH2ZiPszEXJiH+ZhpJjeTweJwOBy3Mcs1HTt2TKGhodqwYYNiYmJKt//Xf/2X1q1bp++//77cY1q1aqWDBw8qISFBw4cPV1ZWloYPH64XX3xRqampkqTly5crLy9PEREROn78uCZOnKijR48qMzNT/v7+V80yYcIETZw4sdz2+fPny9eXi9g600/50tuZVhXaLYptYNfjTe1i/QoAAACY1cWLFzVw4EBduHBBtWtf/2yralWuWrZsqUuXLunAgQOlR6qmTp2qN998U8ePH7/q65w/f15NmjTR1KlTNXTo0Kvuc7UjV2FhYTpz5swN/wBvt6KiIq1atUpxcXGlp0JWVydzLqnfe9/rZE6BYpvX18xBHeRhNXxNlZtWk2ZSUzATc2Ee5sNMzIeZmAvzMB8zzSQnJ0eBgYEVKleGnRYYGBgoq9WqkydPltl+8uTJa35eymazycPDo8wpgK1bt9aJEydUWFgoT0/Pco+pU6eOWrZsqaysrGtm8fLykpdX+cUUPDw8DB/mFWbKUhk/FxZr+MfbdDKnQM2D/PTuU53k6119349U/WdSEzETc2Ee5sNMzIeZmAvzMB8zzORmXt+wwwaenp7q1KmT1qxZU7rNbrdrzZo1ZY5k/VZsbKyysrJkt9tLt+3Zs0c2m+2qxUqS8vLytG/fPtlsNue+AVSY3e7QHz/bqh9/uqC6vh6aNbizAnz4hwsAAAA1i6HnZI0ePVozZ87U3LlztXPnTg0bNkz5+fmlqwc+/fTTZRa8GDZsmLKzs5WcnKw9e/boyy+/1OTJk5WUlFS6z8svv6x169bp4MGD2rBhgx555BFZrVYNGDCgyt8fSvz3qt1atv2EPK1umvF0tJrU9zM6EgAAAOB0hi7F/uSTT+r06dMaP368Tpw4ofbt22vFihVq0KCBJOnw4cNyc/u1/4WFhWnlypV66aWXFBUVpdDQUCUnJyslJaV0n59++kkDBgzQ2bNnFRQUpG7dumnjxo0KCgqq8vcH6R+bf9I7/94nSZryaFt1Dq9ncCIAAADg9jD86rojRozQiBEjrnrf2rVry22LiYnRxo0br/l8CxYscFY03KJNB7L16uc/SpKSejRXv06NDE4EAAAA3D7Vb6k2VAuHzubrDx9mqKjYofg7G+qPcRFGRwIAAABuK8oVnO7Cz0UaOjdD5y4WqW1ogKY+0V5ublzMCgAAADUb5QpOVVRs14j5W5R1Kk+2AG+9nxgtH0/rjR8IAAAAVHOUKziNw+HQhCX/0dd7z8jX06r3E6PVoLa30bEAAACAKkG5gtPM2XBQH31/WBaLNO3J9ooMCTA6EgAAAFBlKFdwin/vOqXXlu6QJI2Jb6X7IxsanAgAAACoWpQr3LJdJ3I08uMfZHdIT0aH6bnuzYyOBAAAAFQ5yhVuyencAg2dk6G8gsuKaVZfr/W9UxYLKwMCAADA9VCuUGmXior1/IcZOnr+ZzUN9FPaUx3l6c63FAAAAFwTPwmjUhwOh15Z+KN+OHxeAT4e+iAxWnV8PY2OBQAAABiGcoVKeXvNXv1z2zG5u1mU9lRHNQuqZXQkAAAAwFCUK9y0L7Ye1bTVeyVJr/e9U12bBxqcCAAAADAe5Qo3Zcvhc3pl4Y+SpOfvbqb+v2tscCIAAADAHChXqLAj2Rf1/N8zVHjZrrg2DZTSu5XRkQAAAADToFyhQnIvFenZuRk6k1eoNrbamvZke1ndWHIdAAAAuIJyhRu6XGzXyI9/0O6TuQr299IHg6Pl5+VudCwAAADAVChXuKFJy3Zq7e7T8vZw0/uJ0bIF+BgdCQAAADAdyhWu68ONhzT724OSpKlPtFdUozqG5gEAAADMinKFa/p672lNWPIfSdIrvSL0QFubwYkAAAAA86Jc4aqyTuVq+EdbVGx36NGOoRp+b3OjIwEAAACmRrlCOWfzCjRkTrpyL11W5/C6mvJoW1ksrAwIAAAAXA/lCmUUXC7WC/M260j2z2pcz1fvDYqWl7vV6FgAAACA6VGuUMrhcGjM59uVfvCc/L3dNWtwtOr5eRodCwAAAKgWKFco9e7affp8y1FZ3Sx6N6Gj7gj2NzoSAAAAUG1QriBJWrb9uN5cuVuSNPGhSHVvEWRwIgAAAKB6oVxB246c1+hPt0qShsSG66m7mhgbCAAAAKiGKFcu7tj5n/Xs3zN0qciuHhFB+tODbYyOBAAAAFRLlCsXll9wWc/OzdDp3AJFNPDX/x3QQVY3llwHAAAAKoNy5aKK7Q4lL9iqHcdzFFjLUx8Mjpa/t4fRsQAAAIBqi3Llov68YpdW7zwpT3c3zXg6Wo3q+hodCQAAAKjWKFcuaMGmw5qxfr8k6a3H26lj47oGJwIAAACqP8qVi9mw74z+tDhTkjSqZws91C7E4EQAAABAzUC5ciH7T+dp2Lwtumx36KF2IUq+r4XRkQAAAIAag3LlIs5fLNTQuRm68HOROjSuo788FiWLhZUBAQAAAGehXLmAwst2DZu3RQfO5Cu0jo9mDIqWt4fV6FgAAABAjUK5quEcDofGLc7Ud/vPqpaXu2YN7qwgfy+jYwEAAAA1DuWqhpv59X59knFEbhbpfwZ0UERDf6MjAQAAADWS4eXqnXfeUXh4uLy9vdWlSxdt2rTpuvufP39eSUlJstls8vLyUsuWLbVs2bJbes6a6qv/nNCU5bskSeP+Vxv1aBVscCIAAACg5jK0XH3yyScaPXq0UlNTtWXLFrVr1069evXSqVOnrrp/YWGh4uLidPDgQS1cuFC7d+/WzJkzFRoaWunnrKkyj15Q8oKtcjikp+5qrMFdw42OBAAAANRohparqVOn6rnnntOQIUPUpk0bTZ8+Xb6+vpo1a9ZV9581a5ays7O1ePFixcbGKjw8XPfcc4/atWtX6eesiU7mXNKzczP0c1GxurcI1IQ+kawMCAAAANxm7ka9cGFhoTZv3qwxY8aUbnNzc1PPnj313XffXfUxS5YsUUxMjJKSkvTFF18oKChIAwcOVEpKiqxWa6WeU5IKCgpUUFBQejsnJ0eSVFRUpKKiolt9q7fkyutXNMfPhcV6dm66TuRcUvMgP017vK0c9mIV2YtvZ0yXcrMzwe3HTMyFeZgPMzEfZmIuzMN8zDSTm8lgWLk6c+aMiouL1aBBgzLbGzRooF27dl31Mfv379e//vUvJSQkaNmyZcrKytLw4cNVVFSk1NTUSj2nJE2ZMkUTJ04st/2rr76Sr69vJd6d861ateqG+9gd0pw9btqe7SY/d4cGNrqgb/5948ehcioyE1QtZmIuzMN8mIn5MBNzYR7mY4aZXLx4scL7GlauKsNutys4OFgzZsyQ1WpVp06ddPToUb355ptKTU2t9POOGTNGo0ePLr2dk5OjsLAw3X///apdu7YzoldaUVGRVq1apbi4OHl4eFx336mr9mpb9gF5WC16f3BnRTepW0UpXcvNzARVg5mYC/MwH2ZiPszEXJiH+ZhpJlfOaqsIw8pVYGCgrFarTp48WWb7yZMn1bBhw6s+xmazycPDQ1brrxfAbd26tU6cOKHCwsJKPackeXl5ycur/LWfPDw8DB/mFTfK8o/NPylt/QFJ0huPRinmDlYGvN3M9P2BEszEXJiH+TAT82Em5sI8zMcMM7mZ1zdsQQtPT0916tRJa9asKd1mt9u1Zs0axcTEXPUxsbGxysrKkt1uL922Z88e2Ww2eXp6Vuo5a4JNB7L16uc/SpJG9LhD/To1MjgRAAAA4HoMXS1w9OjRmjlzpubOnaudO3dq2LBhys/P15AhQyRJTz/9dJnFKYYNG6bs7GwlJydrz549+vLLLzV58mQlJSVV+DlrmkNn8/WHDzNUVOzQA20banRcS6MjAQAAAC7J0M9cPfnkkzp9+rTGjx+vEydOqH379lqxYkXpghSHDx+Wm9uv/S8sLEwrV67USy+9pKioKIWGhio5OVkpKSkVfs6a5MLPRXpmTrrOXSxSVKMA/ffj7eXmxpLrAAAAgBEMX9BixIgRGjFixFXvW7t2bbltMTEx2rhxY6Wfs6YoKrZrxPwt2nc6X7YAb73/dLR8PK03fiAAAACA28LQ0wJROQ6HQxOW/Edf7z0jX0+r3k+MVnBtb6NjAQAAAC6NclUNzdlwUB99f1gWi/R2/w6KDAkwOhIAAADg8ihX1cy/d53Sa0t3SJLGxrdWXJua91kyAAAAoDoy/DNXuLZiu0PfH8jW5jMW1T+Qrbq1vDXy4x9kd0j9O4fp2e5NjY4IAAAA4BeUK5NakXlcE/+5Q8cvXJJk1d/3ZsjNItkdUkyz+vo/D98pi4WVAQEAAACzoFyZ0IrM4xo2b4sc/992+y8bHusUKk93zugEAAAAzISf0E2m2O7QxH/uKFesfuutr/ao2H69PQAAAABUNcqVyWw6kP3LqYDXdvzCJW06kF1FiQAAAABUBOXKZE7lXr9Y3ex+AAAAAKoG5cpkgv0rdjHgiu4HAAAAoGpQrkzmd03ryRbgrWutA2iRZAvw1u+a1qvKWAAAAABugHJlMlY3i1L7tJGkcgXryu3UPm1kdWMZdgAAAMBMKFcm1PtOm9Ke6qiGAWVP/WsY4K20pzqq9502g5IBAAAAuBauc2VSve+0Ka5NQ32XdUpfff297u/eRTF3BHPECgAAADApypWJWd0s6tK0ns7udKhL03oUKwAAAMDEOC0QAAAAAJyAcgUAAAAATkC5AgAAAAAnoFwBAAAAgBNQrgAAAADACShXAAAAAOAElCsAAAAAcALKFQAAAAA4AeUKAAAAAJyAcgUAAAAATuBudAAzcjgckqScnByDk0hFRUW6ePGicnJy5OHhYXQciJmYETMxF+ZhPszEfJiJuTAP8zHTTK50gisd4XooV1eRm5srSQoLCzM4CQAAAAAzyM3NVUBAwHX3sTgqUsFcjN1u17Fjx+Tv7y+LxWJolpycHIWFhenIkSOqXbu2oVlQgpmYDzMxF+ZhPszEfJiJuTAP8zHTTBwOh3JzcxUSEiI3t+t/qoojV1fh5uamRo0aGR2jjNq1axv+jYWymIn5MBNzYR7mw0zMh5mYC/MwH7PM5EZHrK5gQQsAAAAAcALKFQAAAAA4AeXK5Ly8vJSamiovLy+jo+AXzMR8mIm5MA/zYSbmw0zMhXmYT3WdCQtaAAAAAIATcOQKAAAAAJyAcgUAAAAATkC5AgAAAAAnoFwBAAAAgBNQrkxq/fr16tOnj0JCQmSxWLR48WKjI7m8KVOmqHPnzvL391dwcLD69u2r3bt3Gx3LZaWlpSkqKqr04oIxMTFavny50bHwG2+88YYsFotGjRpldBSXNWHCBFksljJfrVq1MjqWSzt69Kieeuop1a9fXz4+Pmrbtq0yMjKMjuWywsPDy/0dsVgsSkpKMjqaSyouLta4cePUtGlT+fj4qHnz5nrttddUndbfczc6AK4uPz9f7dq10zPPPKNHH33U6DiQtG7dOiUlJalz5866fPmyxo4dq/vvv187duyQn5+f0fFcTqNGjfTGG2+oRYsWcjgcmjt3rh5++GH98MMPioyMNDqey0tPT9d7772nqKgoo6O4vMjISK1evbr0trs7/+s3yrlz5xQbG6sePXpo+fLlCgoK0t69e1W3bl2jo7ms9PR0FRcXl97OzMxUXFycHn/8cQNTua4///nPSktL09y5cxUZGamMjAwNGTJEAQEBevHFF42OVyH8C2tS8fHxio+PNzoGfmPFihVlbs+ZM0fBwcHavHmz7r77boNSua4+ffqUuT1p0iSlpaVp48aNlCuD5eXlKSEhQTNnztTrr79udByX5+7uroYNGxodAyr5wTEsLEyzZ88u3da0aVMDEyEoKKjM7TfeeEPNmzfXPffcY1Ai17ZhwwY9/PDDevDBByWVHFn8+OOPtWnTJoOTVRynBQKVdOHCBUlSvXr1DE6C4uJiLViwQPn5+YqJiTE6jstLSkrSgw8+qJ49exodBZL27t2rkJAQNWvWTAkJCTp8+LDRkVzWkiVLFB0drccff1zBwcHq0KGDZs6caXQs/KKwsFDz5s3TM888I4vFYnQcl9S1a1etWbNGe/bskSRt27ZN33zzTbU64MCRK6AS7Ha7Ro0apdjYWN15551Gx3FZ27dvV0xMjC5duqRatWpp0aJFatOmjdGxXNqCBQu0ZcsWpaenGx0Fkrp06aI5c+YoIiJCx48f18SJE9W9e3dlZmbK39/f6HguZ//+/UpLS9Po0aM1duxYpaen68UXX5Snp6cSExONjufyFi9erPPnz2vw4MFGR3FZr776qnJyctSqVStZrVYVFxdr0qRJSkhIMDpahVGugEpISkpSZmamvvnmG6OjuLSIiAht3bpVFy5c0MKFC5WYmKh169ZRsAxy5MgRJScna9WqVfL29jY6DqQyv+2NiopSly5d1KRJE3366acaOnSogclck91uV3R0tCZPnixJ6tChgzIzMzV9+nTKlQl88MEHio+PV0hIiNFRXNann36qjz76SPPnz1dkZKS2bt2qUaNGKSQkpNr8HaFcATdpxIgRWrp0qdavX69GjRoZHceleXp66o477pAkderUSenp6Xr77bf13nvvGZzMNW3evFmnTp1Sx44dS7cVFxdr/fr1+tvf/qaCggJZrVYDE6JOnTpq2bKlsrKyjI7ikmw2W7lf/rRu3Vr/+Mc/DEqEKw4dOqTVq1fr888/NzqKS3vllVf06quvqn///pKktm3b6tChQ5oyZQrlCqhpHA6HRo4cqUWLFmnt2rV8CNmE7Ha7CgoKjI7hsu677z5t3769zLYhQ4aoVatWSklJoViZQF5envbt26dBgwYZHcUlxcbGlruEx549e9SkSRODEuGK2bNnKzg4uHQhBRjj4sWLcnMruySE1WqV3W43KNHNo1yZVF5eXpnfLB44cEBbt25VvXr11LhxYwOTua6kpCTNnz9fX3zxhfz9/XXixAlJUkBAgHx8fAxO53rGjBmj+Ph4NW7cWLm5uZo/f77Wrl2rlStXGh3NZfn7+5f7DKKfn5/q16/PZxMN8vLLL6tPnz5q0qSJjh07ptTUVFmtVg0YMMDoaC7ppZdeUteuXTV58mQ98cQT2rRpk2bMmKEZM2YYHc2l2e12zZ49W4mJiVyqwGB9+vTRpEmT1LhxY0VGRuqHH37Q1KlT9cwzzxgdrcIsjup0VS4XsnbtWvXo0aPc9sTERM2ZM6fqA+GaKwfNnj2bD78aYOjQoVqzZo2OHz+ugIAARUVFKSUlRXFxcUZHw2/ce++9at++vaZNm2Z0FJfUv39/rV+/XmfPnlVQUJC6deumSZMmqXnz5kZHc1lLly7VmDFjtHfvXjVt2lSjR4/Wc889Z3Qsl/bVV1+pV69e2r17t1q2bGl0HJeWm5urcePGadGiRTp16pRCQkI0YMAAjR8/Xp6enkbHqxDKFQAAAAA4Ade5AgAAAAAnoFwBAAAAgBNQrgAAAADACShXAAAAAOAElCsAAAAAcALKFQAAAAA4AeUKAAAAAJyAcgUAAAAATkC5AgDgFlksFi1evNjoGAAAg1GuAADV2uDBg2WxWMp99e7d2+hoAAAX4250AAAAblXv3r01e/bsMtu8vLwMSgMAcFUcuQIAVHteXl5q2LBhma+6detKKjllLy0tTfHx8fLx8VGzZs20cOHCMo/fvn27fv/738vHx0f169fX888/r7y8vDL7zJo1S5GRkfLy8pLNZtOIESPK3H/mzBk98sgj8vX1VYsWLbRkyZLS+86dO6eEhAQFBQXJx8dHLVq0KFcGAQDVH+UKAFDjjRs3Tv369dO2bduUkJCg/v37a+fOnZKk/Px89erVS3Xr1lV6ero+++wzrV69ukx5SktLU1JSkp5//nlt375dS5Ys0R133FHmNSZOnKgnnnhCP/74ox544AElJCQoOzu79PV37Nih5cuXa+fOnUpLS1NgYGDV/QEAAKqExeFwOIwOAQBAZQ0ePFjz5s2Tt7d3me1jx47V2LFjZbFY9MILLygtLa30vrvuuksdO3bUu+++q5kzZyolJUVHjhyRn5+fJGnZsmXq06ePjh07pgYNGig0NFRDhgzR66+/ftUMFotFf/rTn/Taa69JKilstWrV0vLly9W7d2899NBDCgwM1KxZs27TnwIAwAz4zBUAoNrr0aNHmfIkSfXq1Sv975iYmDL3xcTEaOvWrZKknTt3ql27dqXFSpJiY2Nlt9u1e/duWSwWHTt2TPfdd991M0RFRZX+t5+fn2rXrq1Tp05JkoYNG6Z+/fppy5Ytuv/++9W3b1917dq1Uu8VAGBelCsAQLXn5+dX7jQ9Z/Hx8anQfh4eHmVuWywW2e12SVJ8fLwOHTqkZcuWadWqVbrvvvuUlJSkt956y+l5AQDG4TNXAIAab+PGjeVut27dWpLUunVrbdu2Tfn5+aX3f/vtt3Jzc1NERIT8/f0VHh6uNWvW3FKGoKAgJSYmat68eZo2bZpmzJhxS88HADAfjlwBAKq9goICnThxosw2d3f30kUjPvvsM0VHR6tbt2766KOPtGnTJn3wwQeSpISEBKWmpioxMVETJkzQ6dOnNXLkSA0aNEgNGjSQJE2YMEEvvPCCgoODFR8fr9zcXH377bcaOXJkhfKNHz9enTp1UmRkpAoKCrR06dLScgcAqDkoVwCAam/FihWy2WxltkVERGjXrl2SSlbyW7BggYYPHy6bzaaPP/5Ybdq0kST5+vpq5cqVSk5OVufOneXr66t+/fpp6tSppc+VmJioS5cu6a9//atefvllBQYG6rHHHqtwPk9PT40ZM0YHDx6Uj4+PunfvrgULFjjhnQMAzITVAgEANZrFYtGiRYvUt29fo6MAAGo4PnMFAAAAAE5AuQIAAAAAJ+AzVwCAGo2z3wEAVYUjVwAAAADgBJQrAAAAAHACyhUAAAAAOAHlCgAAAACcgHIFAAAAAE5AuQIAAAAAJ6BcAQAAAIATUK4AAAAAwAn+H/YmqyHgQn9uAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Example correction: Ensure all arrays have the same length\n",
        "num_epochs = range(1, len(epoch_train_loss) + 1)  # Adjust to match the length of training/validation data\n",
        "\n",
        "# Plotting loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(num_epochs, epoch_train_loss, label=\"Training Loss\", marker='o')\n",
        "plt.plot(num_epochs, epoch_val_loss, label=\"Validation Loss\", marker='o')\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Plotting accuracy\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(num_epochs, epoch_train_accuracy, label=\"Training Accuracy\", marker='o')\n",
        "plt.plot(num_epochs, epoch_val_accuracy, label=\"Validation Accuracy\", marker='o')\n",
        "plt.title(\"Training and Validation Accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4X8I7W3jahFR"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SUb9Y33zUU7m",
        "outputId": "eaad7189-1669-4c9b-af87-091fbc222133"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "HierarchicalBERT(\n",
              "  (sentence_encoder): SentenceEncodingLayer(\n",
              "    (context_encoder): BertModel(\n",
              "      (embeddings): BertEmbeddings(\n",
              "        (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "        (position_embeddings): Embedding(512, 768)\n",
              "        (token_type_embeddings): Embedding(2, 768)\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (encoder): BertEncoder(\n",
              "        (layer): ModuleList(\n",
              "          (0-11): 12 x BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSdpaSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (pooler): BertPooler(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (activation): Tanh()\n",
              "      )\n",
              "    )\n",
              "    (response_encoder): BertModel(\n",
              "      (embeddings): BertEmbeddings(\n",
              "        (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "        (position_embeddings): Embedding(512, 768)\n",
              "        (token_type_embeddings): Embedding(2, 768)\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (encoder): BertEncoder(\n",
              "        (layer): ModuleList(\n",
              "          (0-11): 12 x BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSdpaSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (pooler): BertPooler(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (activation): Tanh()\n",
              "      )\n",
              "    )\n",
              "    (lstm): LSTM(768, 300, batch_first=True, bidirectional=True)\n",
              "  )\n",
              "  (context_summarizer): ContextSummarizationLayer(\n",
              "    (conv2D): Conv2d(768, 128, kernel_size=(2, 2), stride=(1, 1))\n",
              "  )\n",
              "  (context_encoder): ContextEncoderLayer(\n",
              "    (bilstm): LSTM(12672, 300, batch_first=True, bidirectional=True)\n",
              "  )\n",
              "  (syntactic_feature_fc): Linear(in_features=4, out_features=600, bias=True)\n",
              "  (cnn_layer): CNNLayer(\n",
              "    (convs): ModuleList(\n",
              "      (0): Conv2d(1, 128, kernel_size=(2, 2), stride=(1, 1))\n",
              "      (1): Conv2d(1, 128, kernel_size=(2, 3), stride=(1, 1))\n",
              "      (2): Conv2d(1, 128, kernel_size=(2, 5), stride=(1, 1))\n",
              "    )\n",
              "    (maxpool): AdaptiveMaxPool2d(output_size=(1, 1))\n",
              "  )\n",
              "  (fc_layer): FullyConnectedLayer(\n",
              "    (fc): Linear(in_features=1368, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#try:\n",
        "model = HierarchicalBERT()\n",
        "# Load the saved model weights\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/FALL2024/comp550/final_project/models/best_model_2024-12-16_03-02-41.pth\"\n",
        "))\n",
        "\n",
        "# Move the model to the appropriate device (e.g., GPU if available)\n",
        "model.to(device)\n",
        "# except:\n",
        "#     disconnect_runtime()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "uKtidHDYA_0I"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "def test_model(model, test_loader, device):\n",
        "    \"\"\"\n",
        "    Test the model on the test dataset and return the loss and accuracy.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    epoch_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    all_true_labels = []\n",
        "    all_predicted_labels = []\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculations for testing\n",
        "        for batch in tqdm(test_loader, desc=\"Testing Progress\"):\n",
        "            # Move data to the device\n",
        "            context = batch['context']\n",
        "            response = batch['response']\n",
        "            syntactic_features = batch['syntactic_features'].to(device)\n",
        "            labels = batch['label'].unsqueeze(1).to(device).float()\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(context, response, syntactic_features)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            # Compute predictions\n",
        "            predictions = (torch.sigmoid(logits) > 0.5).long()\n",
        "\n",
        "            # Update metrics\n",
        "            epoch_loss += loss.item()\n",
        "            total_predictions += labels.size(0)\n",
        "            correct_predictions += (predictions == labels.long()).sum().item()\n",
        "\n",
        "            # Collect true and predicted labels\n",
        "            all_true_labels.extend(labels.cpu().numpy().flatten())\n",
        "            all_predicted_labels.extend(predictions.cpu().numpy().flatten())\n",
        "\n",
        "    # Calculate average loss and accuracy\n",
        "    avg_loss = epoch_loss / len(test_loader)\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(all_true_labels, all_predicted_labels)\n",
        "\n",
        "    # Display the confusion matrix\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "\n",
        "    # Visualize the confusion matrix\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1], yticklabels=[0, 1])\n",
        "    plt.xlabel('Predicted Labels')\n",
        "    plt.ylabel('True Labels')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lrx7zWTyBQnh",
        "outputId": "0e8db9c3-f857-4c45-ce5c-08ca09b52b68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recevied a csv file\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing Progress:   0%|          | 1/225 [00:01<04:39,  1.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:   1%|          | 2/225 [00:02<04:25,  1.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:   1%|         | 3/225 [00:03<04:21,  1.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:   2%|         | 4/225 [00:04<04:18,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:   2%|         | 5/225 [00:05<04:17,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:   3%|         | 6/225 [00:07<04:16,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:   3%|         | 7/225 [00:08<04:14,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:   4%|         | 8/225 [00:09<04:13,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:   4%|         | 9/225 [00:10<04:11,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:   4%|         | 10/225 [00:11<04:11,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:   5%|         | 11/225 [00:12<04:10,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:   5%|         | 12/225 [00:14<04:10,  1.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:   6%|         | 13/225 [00:15<04:09,  1.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:   6%|         | 14/225 [00:16<04:08,  1.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:   7%|         | 15/225 [00:17<04:08,  1.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:   7%|         | 16/225 [00:18<04:07,  1.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:   8%|         | 17/225 [00:20<04:06,  1.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:   8%|         | 18/225 [00:21<04:05,  1.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:   8%|         | 19/225 [00:22<04:05,  1.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:   9%|         | 20/225 [00:23<04:03,  1.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:   9%|         | 21/225 [00:24<04:02,  1.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  10%|         | 22/225 [00:26<04:04,  1.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  10%|         | 23/225 [00:27<04:05,  1.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  11%|         | 24/225 [00:28<04:02,  1.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  11%|         | 25/225 [00:29<04:00,  1.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  12%|        | 26/225 [00:30<03:57,  1.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  12%|        | 27/225 [00:32<03:55,  1.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  12%|        | 28/225 [00:33<03:54,  1.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  13%|        | 29/225 [00:34<03:52,  1.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  13%|        | 30/225 [00:35<03:51,  1.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  14%|        | 31/225 [00:36<03:49,  1.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  14%|        | 32/225 [00:37<03:47,  1.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  15%|        | 33/225 [00:39<03:46,  1.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  15%|        | 34/225 [00:40<03:44,  1.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  16%|        | 35/225 [00:41<03:44,  1.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  16%|        | 36/225 [00:42<03:43,  1.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  16%|        | 37/225 [00:43<03:42,  1.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  17%|        | 38/225 [00:44<03:40,  1.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  17%|        | 39/225 [00:46<03:38,  1.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  18%|        | 40/225 [00:47<03:37,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  18%|        | 41/225 [00:48<03:35,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  19%|        | 42/225 [00:49<03:33,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  19%|        | 43/225 [00:50<03:31,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  20%|        | 44/225 [00:51<03:29,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  20%|        | 45/225 [00:53<03:28,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  20%|        | 46/225 [00:54<03:28,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  21%|        | 47/225 [00:55<03:27,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  21%|       | 48/225 [00:56<03:25,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  22%|       | 49/225 [00:57<03:23,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  22%|       | 50/225 [00:58<03:22,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  23%|       | 51/225 [01:00<03:21,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  23%|       | 52/225 [01:01<03:19,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  24%|       | 53/225 [01:02<03:18,  1.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  24%|       | 54/225 [01:03<03:16,  1.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  24%|       | 55/225 [01:04<03:15,  1.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  25%|       | 56/225 [01:05<03:15,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  25%|       | 57/225 [01:06<03:14,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  26%|       | 58/225 [01:08<03:12,  1.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  26%|       | 59/225 [01:09<03:11,  1.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  27%|       | 60/225 [01:10<03:10,  1.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  27%|       | 61/225 [01:11<03:08,  1.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  28%|       | 62/225 [01:12<03:07,  1.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  28%|       | 63/225 [01:13<03:07,  1.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  28%|       | 64/225 [01:15<03:05,  1.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  29%|       | 65/225 [01:16<03:04,  1.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  29%|       | 66/225 [01:17<03:03,  1.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  30%|       | 67/225 [01:18<03:03,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  30%|       | 68/225 [01:19<03:02,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  31%|       | 69/225 [01:20<03:00,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  31%|       | 70/225 [01:21<02:59,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  32%|      | 71/225 [01:23<02:57,  1.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  32%|      | 72/225 [01:24<02:56,  1.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  32%|      | 73/225 [01:25<02:55,  1.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  33%|      | 74/225 [01:26<02:54,  1.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  33%|      | 75/225 [01:27<02:53,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  34%|      | 76/225 [01:28<02:52,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  34%|      | 77/225 [01:30<02:52,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  35%|      | 78/225 [01:31<02:51,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  35%|      | 79/225 [01:32<02:50,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  36%|      | 80/225 [01:33<02:49,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  36%|      | 81/225 [01:34<02:48,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  36%|      | 82/225 [01:35<02:47,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  37%|      | 83/225 [01:37<02:45,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  37%|      | 84/225 [01:38<02:47,  1.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  38%|      | 85/225 [01:39<02:45,  1.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  38%|      | 86/225 [01:40<02:44,  1.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  39%|      | 87/225 [01:41<02:42,  1.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  39%|      | 88/225 [01:43<02:41,  1.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  40%|      | 89/225 [01:44<02:40,  1.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  40%|      | 90/225 [01:45<02:38,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  40%|      | 91/225 [01:46<02:36,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  41%|      | 92/225 [01:47<02:35,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  41%|     | 93/225 [01:48<02:34,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  42%|     | 94/225 [01:50<02:32,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  42%|     | 95/225 [01:51<02:31,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  43%|     | 96/225 [01:52<02:30,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  43%|     | 97/225 [01:53<02:29,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  44%|     | 98/225 [01:54<02:29,  1.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  44%|     | 99/225 [01:55<02:28,  1.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  44%|     | 100/225 [01:57<02:26,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  45%|     | 101/225 [01:58<02:24,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  45%|     | 102/225 [01:59<02:23,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  46%|     | 103/225 [02:00<02:22,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  46%|     | 104/225 [02:01<02:21,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  47%|     | 105/225 [02:02<02:19,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  47%|     | 106/225 [02:04<02:18,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  48%|     | 107/225 [02:05<02:17,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  48%|     | 108/225 [02:06<02:16,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  48%|     | 109/225 [02:07<02:15,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  49%|     | 110/225 [02:08<02:14,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  49%|     | 111/225 [02:09<02:12,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  50%|     | 112/225 [02:11<02:11,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  50%|     | 113/225 [02:12<02:10,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  51%|     | 114/225 [02:13<02:09,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  51%|     | 115/225 [02:14<02:07,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  52%|    | 116/225 [02:15<02:06,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  52%|    | 117/225 [02:16<02:05,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  52%|    | 118/225 [02:18<02:04,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  53%|    | 119/225 [02:19<02:03,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  53%|    | 120/225 [02:20<02:02,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  54%|    | 121/225 [02:21<02:01,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  54%|    | 122/225 [02:22<01:59,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  55%|    | 123/225 [02:23<01:58,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  55%|    | 124/225 [02:25<01:57,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  56%|    | 125/225 [02:26<01:55,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  56%|    | 126/225 [02:27<01:54,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  56%|    | 127/225 [02:28<01:53,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  57%|    | 128/225 [02:29<01:52,  1.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  57%|    | 129/225 [02:30<01:51,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  58%|    | 130/225 [02:31<01:50,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  58%|    | 131/225 [02:33<01:49,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  59%|    | 132/225 [02:34<01:47,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  59%|    | 133/225 [02:35<01:46,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  60%|    | 134/225 [02:36<01:45,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  60%|    | 135/225 [02:37<01:44,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  60%|    | 136/225 [02:38<01:43,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  61%|    | 137/225 [02:40<01:42,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  61%|   | 138/225 [02:41<01:40,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  62%|   | 139/225 [02:42<01:39,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  62%|   | 140/225 [02:43<01:38,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  63%|   | 141/225 [02:44<01:37,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  63%|   | 142/225 [02:45<01:36,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  64%|   | 143/225 [02:47<01:35,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  64%|   | 144/225 [02:48<01:33,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  64%|   | 145/225 [02:49<01:32,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  65%|   | 146/225 [02:50<01:31,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  65%|   | 147/225 [02:51<01:30,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  66%|   | 148/225 [02:52<01:29,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  66%|   | 149/225 [02:54<01:28,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  67%|   | 150/225 [02:55<01:26,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  67%|   | 151/225 [02:56<01:26,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  68%|   | 152/225 [02:57<01:25,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  68%|   | 153/225 [02:58<01:23,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  68%|   | 154/225 [02:59<01:22,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  69%|   | 155/225 [03:01<01:21,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  69%|   | 156/225 [03:02<01:20,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  70%|   | 157/225 [03:03<01:19,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  70%|   | 158/225 [03:04<01:18,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  71%|   | 159/225 [03:05<01:17,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  71%|   | 160/225 [03:06<01:15,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  72%|  | 161/225 [03:08<01:14,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  72%|  | 162/225 [03:09<01:13,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  72%|  | 163/225 [03:10<01:12,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  73%|  | 164/225 [03:11<01:11,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  73%|  | 165/225 [03:12<01:09,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  74%|  | 166/225 [03:13<01:08,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  74%|  | 167/225 [03:15<01:07,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  75%|  | 168/225 [03:16<01:06,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  75%|  | 169/225 [03:17<01:05,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  76%|  | 170/225 [03:18<01:03,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  76%|  | 171/225 [03:19<01:02,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  76%|  | 172/225 [03:20<01:01,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  77%|  | 173/225 [03:22<01:01,  1.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  77%|  | 174/225 [03:23<00:59,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  78%|  | 175/225 [03:24<00:58,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  78%|  | 176/225 [03:25<00:57,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  79%|  | 177/225 [03:26<00:55,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  79%|  | 178/225 [03:27<00:54,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  80%|  | 179/225 [03:29<00:53,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  80%|  | 180/225 [03:30<00:52,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  80%|  | 181/225 [03:31<00:51,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  81%|  | 182/225 [03:32<00:50,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  81%| | 183/225 [03:33<00:49,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  82%| | 184/225 [03:34<00:48,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  82%| | 185/225 [03:36<00:46,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  83%| | 186/225 [03:37<00:45,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  83%| | 187/225 [03:38<00:44,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  84%| | 188/225 [03:39<00:43,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  84%| | 189/225 [03:40<00:41,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  84%| | 190/225 [03:41<00:40,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  85%| | 191/225 [03:43<00:39,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  85%| | 192/225 [03:44<00:38,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  86%| | 193/225 [03:45<00:37,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  86%| | 194/225 [03:46<00:36,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  87%| | 195/225 [03:47<00:35,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  87%| | 196/225 [03:48<00:33,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  88%| | 197/225 [03:50<00:32,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  88%| | 198/225 [03:51<00:31,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  88%| | 199/225 [03:52<00:30,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  89%| | 200/225 [03:53<00:29,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  89%| | 201/225 [03:54<00:27,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  90%| | 202/225 [03:55<00:26,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  90%| | 203/225 [03:57<00:25,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  91%| | 204/225 [03:58<00:24,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  91%| | 205/225 [03:59<00:23,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  92%|| 206/225 [04:00<00:22,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  92%|| 207/225 [04:01<00:20,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  92%|| 208/225 [04:02<00:19,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  93%|| 209/225 [04:03<00:18,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  93%|| 210/225 [04:05<00:17,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  94%|| 211/225 [04:06<00:16,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  94%|| 212/225 [04:07<00:15,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  95%|| 213/225 [04:08<00:13,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  95%|| 214/225 [04:09<00:12,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  96%|| 215/225 [04:10<00:11,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  96%|| 216/225 [04:12<00:10,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  96%|| 217/225 [04:13<00:09,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  97%|| 218/225 [04:14<00:08,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  97%|| 219/225 [04:15<00:06,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  98%|| 220/225 [04:16<00:05,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  98%|| 221/225 [04:17<00:04,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  99%|| 222/225 [04:19<00:03,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress:  99%|| 223/225 [04:20<00:02,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTesting Progress: 100%|| 224/225 [04:21<00:01,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing Progress: 100%|| 225/225 [04:22<00:00,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Context Output Shape: torch.Size([8, 384])\n",
            "CNN Response Output Shape: torch.Size([8, 384])\n",
            "Syntactic Feature Embedding Shape: torch.Size([8, 600])\n",
            "Flattened CNN Context Shape: torch.Size([8, 384])\n",
            "Flattened CNN Response Shape: torch.Size([8, 384])\n",
            "Confusion Matrix:\n",
            "[[454 446]\n",
            " [202 698]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Test] Loss: 0.6807, Accuracy: 64.00%\n"
          ]
        }
      ],
      "source": [
        "# Test the model\n",
        "from tqdm import tqdm\n",
        "reddit_test_set  = RedditDataset(REDDIT_TESTING_PATH_DATAFRAME, SYNTACTIC_TESTING_OUTPUT, device)\n",
        "reddit_test_data_loader = DataLoader(reddit_test_set, batch_size=8, shuffle=False, collate_fn=custom_collate_fn, generator=torch.Generator(device=device))\n",
        "test_loss, test_accuracy = test_model(model, test_loader=reddit_test_data_loader, device=device)\n",
        "\n",
        "print(f\"[Test] Loss: {test_loss:.4f}, Accuracy: {test_accuracy * 100:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "002c24e70d3d42dc8225ac57c03de12a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01c1390329b543fe94327f9d8365a7b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2555660ccaf346e5bacb1a75b9c1b1a2",
            "placeholder": "",
            "style": "IPY_MODEL_657dbb637136492c9253f078fb7def4b",
            "value": "213k/213k[00:00&lt;00:00,8.34MB/s]"
          }
        },
        "07eaede52e414518859b7ef8e3decdf9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13f2e1cb405748b3ad2106ef1a2eeb4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07eaede52e414518859b7ef8e3decdf9",
            "max": 49,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_814fcc4b662c4b4b92686cf5c42c4002",
            "value": 49
          }
        },
        "145560b4b5624c2596ae4545c8b53229": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2104d2ae2a9d4effae258fd8a3d59e85": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_002c24e70d3d42dc8225ac57c03de12a",
            "placeholder": "",
            "style": "IPY_MODEL_7f46abc0f4ae40b6b19e94eca97549f2",
            "value": "vocab.txt:100%"
          }
        },
        "2255e4aed13a46d8807d024e42b3d81d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69ff5149b99846e7bde0d944b64e2224",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5ff6d61576a44b93878d53c7d2a2aa7a",
            "value": 570
          }
        },
        "24eb164519124866bec36b49adb61d31": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2555660ccaf346e5bacb1a75b9c1b1a2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27107c15410f4d1aaf5024808ae7b10b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2af0cd20c29d4d99801f50eb38563a24": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d036f48c7d742a695f7fceeb64eb4bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5845280e32834a48b283988e2baab942",
              "IPY_MODEL_6143a79c4b4240a28fff3ca77fe4775a",
              "IPY_MODEL_d056a81931d4400fb6431c5644e6ad63"
            ],
            "layout": "IPY_MODEL_49af1c865fdd44f385cbdc0960d04760"
          }
        },
        "39782951d4cb4bac94d4a1455c6bf586": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e7d6c8ff12340f781c3dddcfd092d42": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "47d9c86c963946c985b7fad02c352071": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49af1c865fdd44f385cbdc0960d04760": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a3414073c4a42d98edad65d57e64e1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9bfe7282584641b9bc54abac95e6793e",
            "placeholder": "",
            "style": "IPY_MODEL_77471b6cb886453d840addeb3f31a726",
            "value": "tokenizer.json:100%"
          }
        },
        "4fa6bd0598f84cf4ab51c193cd5060b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_145560b4b5624c2596ae4545c8b53229",
            "max": 435797,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_904e8ec70b254b2ca83f2e2d9ff36be7",
            "value": 435797
          }
        },
        "5173af63a09f4f42bdadf01c5cfe68dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "53b947439596427182fb9fdc397f5729": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5845280e32834a48b283988e2baab942": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e1a65de386543988032506e2e4a41aa",
            "placeholder": "",
            "style": "IPY_MODEL_6fc8fd792d494845a999fcb92b882cc9",
            "value": "model.safetensors:100%"
          }
        },
        "5ff6d61576a44b93878d53c7d2a2aa7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6143a79c4b4240a28fff3ca77fe4775a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2af0cd20c29d4d99801f50eb38563a24",
            "max": 435755784,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_53b947439596427182fb9fdc397f5729",
            "value": 435755784
          }
        },
        "657dbb637136492c9253f078fb7def4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "69ff5149b99846e7bde0d944b64e2224": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e3ec84fbb8c4139943a6bff7d2f334d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2104d2ae2a9d4effae258fd8a3d59e85",
              "IPY_MODEL_d5ad2cfb2dfd4ae5a2e9a36be08872e2",
              "IPY_MODEL_01c1390329b543fe94327f9d8365a7b6"
            ],
            "layout": "IPY_MODEL_47d9c86c963946c985b7fad02c352071"
          }
        },
        "6fc8fd792d494845a999fcb92b882cc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6fcc712426df4335ad1965932422f542": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b6f47643b84c40589ba1c55687bee93b",
              "IPY_MODEL_2255e4aed13a46d8807d024e42b3d81d",
              "IPY_MODEL_abaca4e1ee7f40fba3ca5fabe39324d8"
            ],
            "layout": "IPY_MODEL_b6e527bcc8fb421281e64a8e85062ccb"
          }
        },
        "77471b6cb886453d840addeb3f31a726": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ab224df975d4fadb7ea2c8e6fb8112f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f46abc0f4ae40b6b19e94eca97549f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "814fcc4b662c4b4b92686cf5c42c4002": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "85b14e9a686e4d3a859255a5f6abe98c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b0103837fc64e3b85cde418e64139eb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b5feec9d48c47149ac18c053e87d86d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e1a65de386543988032506e2e4a41aa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e90fb6ebe984fe9ad8f455063ab8049": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85b14e9a686e4d3a859255a5f6abe98c",
            "placeholder": "",
            "style": "IPY_MODEL_27107c15410f4d1aaf5024808ae7b10b",
            "value": "tokenizer_config.json:100%"
          }
        },
        "904e8ec70b254b2ca83f2e2d9ff36be7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9925ba69ee5a404d9c4277639661c157": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf1f93140177405e96d5ed5fad9145da",
            "placeholder": "",
            "style": "IPY_MODEL_f679b9049a6f42bab43cc0fe71b76215",
            "value": "436k/436k[00:00&lt;00:00,30.5MB/s]"
          }
        },
        "9bfe7282584641b9bc54abac95e6793e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a92286d8461d449db35a076b391a39c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4a3414073c4a42d98edad65d57e64e1c",
              "IPY_MODEL_4fa6bd0598f84cf4ab51c193cd5060b0",
              "IPY_MODEL_9925ba69ee5a404d9c4277639661c157"
            ],
            "layout": "IPY_MODEL_aa84539315ce44bf9f0ef96f8415f8a9"
          }
        },
        "aa84539315ce44bf9f0ef96f8415f8a9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abaca4e1ee7f40fba3ca5fabe39324d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b5feec9d48c47149ac18c053e87d86d",
            "placeholder": "",
            "style": "IPY_MODEL_39782951d4cb4bac94d4a1455c6bf586",
            "value": "570/570[00:00&lt;00:00,42.5kB/s]"
          }
        },
        "b31d627e784f45df815c9abfeb28a827": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6e527bcc8fb421281e64a8e85062ccb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6f47643b84c40589ba1c55687bee93b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c37a136098704fbcb5d986704e83506e",
            "placeholder": "",
            "style": "IPY_MODEL_3e7d6c8ff12340f781c3dddcfd092d42",
            "value": "config.json:100%"
          }
        },
        "b848c6cdf0004e198521e812ce943fd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c37a136098704fbcb5d986704e83506e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf1f93140177405e96d5ed5fad9145da": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d056a81931d4400fb6431c5644e6ad63": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ab224df975d4fadb7ea2c8e6fb8112f",
            "placeholder": "",
            "style": "IPY_MODEL_e7e844efbdb74e56b328341c957227c0",
            "value": "436M/436M[00:01&lt;00:00,252MB/s]"
          }
        },
        "d2dbc7f69bca4ed8aa0fab11d6c23264": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8e90fb6ebe984fe9ad8f455063ab8049",
              "IPY_MODEL_13f2e1cb405748b3ad2106ef1a2eeb4c",
              "IPY_MODEL_e73c11b4cff94f6a91bda2bfc163ffa9"
            ],
            "layout": "IPY_MODEL_8b0103837fc64e3b85cde418e64139eb"
          }
        },
        "d5ad2cfb2dfd4ae5a2e9a36be08872e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b31d627e784f45df815c9abfeb28a827",
            "max": 213450,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b848c6cdf0004e198521e812ce943fd8",
            "value": 213450
          }
        },
        "e73c11b4cff94f6a91bda2bfc163ffa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24eb164519124866bec36b49adb61d31",
            "placeholder": "",
            "style": "IPY_MODEL_5173af63a09f4f42bdadf01c5cfe68dc",
            "value": "49.0/49.0[00:00&lt;00:00,4.02kB/s]"
          }
        },
        "e7e844efbdb74e56b328341c957227c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f679b9049a6f42bab43cc0fe71b76215": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
